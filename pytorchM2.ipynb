{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorchM2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristynpantoja/math689project/blob/master/pytorchM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Yzom68CRUCUt",
        "colab_type": "code",
        "outputId": "1c0040de-5aca-4ae3-fe5f-243f973f5ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 26kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x59e42000 @  0x7fce1832c2a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L382anKLUwuE",
        "colab_type": "code",
        "outputId": "01f181ce-aa8f-4a07-8c49-1b6cd051e5a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pJ2ybE0_FuhI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model Parameters"
      ]
    },
    {
      "metadata": {
        "id": "Y96bktYniWeB",
        "colab_type": "code",
        "outputId": "62b1d660-a46e-4464-fcfc-9fc9754ebeda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.6MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 11.7MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/7e/fe8faa29e771a09c528ee70e1fd9b317006021c48311ecccc78c22ebe739/boto3-1.9.37-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 29.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.2MB/s \n",
            "\u001b[?25hCollecting botocore<1.13.0,>=1.12.37 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/35/1461771f778b67984a75a9853a2b3ed25e65a4345e669a81b50c67c930ab/botocore-1.12.37-py2.py3-none-any.whl (4.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.7MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.37->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting docutils>=0.10 (from botocore<1.13.0,>=1.12.37->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 21.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.37 botocore-1.12.37 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f5gixJ2PhDZy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from types import SimpleNamespace\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-D3sQR-gF0Vk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data: 20newsgroups\n",
        "We get the document-term matrix"
      ]
    },
    {
      "metadata": {
        "id": "TNUmE8UMGRMh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a5fe6b99-a29d-4a52-a1ac-93bf865e1a56"
      },
      "cell_type": "code",
      "source": [
        "# categories = ['talk.politics.guns', 'sci.space', 'soc.religion.christian',\n",
        "#               'misc.forsale', 'rec.sport.baseball', 'comp.sys.mac.hardware']\n",
        "categories = ['talk.politics.guns', 'sci.space']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "vectorizer = CountVectorizer(stop_words = 'english', min_df=.01, max_df=0.9, \n",
        "                             token_pattern = u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
        "count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
        "doc_term_matrix = count_vecs.toarray()\n",
        "doc_term_matrix.shape # number of documents, number of words (in vocab)\n",
        "tokenizer = vectorizer.build_tokenizer()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XtIb9CFS59LP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9dcc4db2-9104-47d9-d847-a5db76d6e947"
      },
      "cell_type": "code",
      "source": [
        "doc_term_matrix.shape[1] # vocab size"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2441"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "ilQe6iCsZ9VO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "203150db-3e28-412e-fc97-75413aeaa04e"
      },
      "cell_type": "code",
      "source": [
        "len(vectorizer.get_feature_names())"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2441"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "metadata": {
        "id": "oft5ScQFZyEw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dict_word_freq = dict(zip(vectorizer.get_feature_names(), list(doc_term_matrix.sum(0))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ECE01K5OF9r0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ragged array of words in each document (by index in vocabulary)"
      ]
    },
    {
      "metadata": {
        "id": "NKDN2_x45V44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def countsToInput(row):\n",
        "#   return np.repeat(np.arange(doc_term_matrix.shape[1]),row)\n",
        "  \n",
        "# def numWords(row):\n",
        "#   return row.sum()\n",
        "\n",
        "# N_train = np.apply_along_axis(numWords, axis=1, arr=doc_term_matrix)\n",
        "# data_train = []\n",
        "# for d in range(doc_term_matrix.shape[0]):\n",
        "#   data_train.append(torch.from_numpy(countsToInput(doc_term_matrix[d])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fN8idSgxlxiR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cseFKDH3nfkW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FrATqONaiDBo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word2vec"
      ]
    },
    {
      "metadata": {
        "id": "aLXYF7P7xSEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65a9c53f-d20c-4ae0-d127-3fbd1be8c07a"
      },
      "cell_type": "code",
      "source": [
        "# ideally, we would do some preprocessing\n",
        "newsgroups_train_preproc = []\n",
        "for document in newsgroups_train.data:\n",
        "    newsgroups_train_preproc.append(document.split())\n",
        "\n",
        "# make the model\n",
        "w2v = Word2Vec(sg=1, negative=5, size=100, window=10, min_count=1, max_vocab_size=None, max_final_vocab=None)\n",
        "# w2v.build_vocab(newsgroups_train_preproc)\n",
        "w2v.build_vocab_from_freq(word_freq = dict_word_freq)\n",
        "# train the model\n",
        "w2v.train(sentences=newsgroups_train_preproc,epochs=10, total_examples=doc_term_matrix.shape[1])\n",
        "# save the model\n",
        "# w2v.save(\"sg_1_M2\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(625063, 3791210)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "-nNkaKgu7Dsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3184052e-0eeb-41b6-d79e-853c1df70455"
      },
      "cell_type": "code",
      "source": [
        "len(set([item for sublist in newsgroups_train_preproc for item in sublist]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51319"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "2EBD-baJ70bC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "17e1c072-bdfd-4610-8296-32aeb822c6f8"
      },
      "cell_type": "code",
      "source": [
        "torch.tensor(w2v.syn1neg).shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn1neg` (Attribute will be removed in 4.0.0, use self.trainables.syn1neg instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2441, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "eTmRNeXU6Od7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa103a76-3519-4864-bf60-6bef1ebd6d18"
      },
      "cell_type": "code",
      "source": [
        "len(w2v.wv.vocab)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2441"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "LcpyWmcDGrjR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "445946f5-1cb8-48ea-ce23-8a60b1e3db7e"
      },
      "cell_type": "code",
      "source": [
        "doc_term_matrix.shape[1] == len(w2v.wv.vocab)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "DTX4Rsqf4IVr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "50ac98d1-01c6-471e-fcc6-531549ea9686"
      },
      "cell_type": "code",
      "source": [
        "w2v.syn1neg"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn1neg` (Attribute will be removed in 4.0.0, use self.trainables.syn1neg instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.26420075,  0.00585905,  0.22831912, ...,  0.64760387,\n",
              "         0.18856551,  0.65869856],\n",
              "       [-0.23667611,  0.00330805,  0.20415191, ...,  0.57213134,\n",
              "         0.16721836,  0.58668435],\n",
              "       [ 0.07048804,  0.048697  ,  0.09646006, ...,  0.23505957,\n",
              "        -0.1924888 ,  0.49675873],\n",
              "       ...,\n",
              "       [-0.12941656,  0.08567844, -0.01250011, ...,  0.07879993,\n",
              "         0.07050155,  0.20635433],\n",
              "       [ 0.02957424, -0.04086953,  0.00365244, ...,  0.15197736,\n",
              "         0.03688794,  0.1449362 ],\n",
              "       [-0.1434233 , -0.00083163,  0.13793549, ...,  0.35631266,\n",
              "         0.10791391,  0.36240584]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "Qe0oJvkE21H3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "092f7c8c-e7ee-45ef-f6b1-e5fbfd38a675"
      },
      "cell_type": "code",
      "source": [
        "w2v.wv.most_similar(\"university\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('students', 0.9315541982650757),\n",
              " ('standard', 0.9139469861984253),\n",
              " ('education', 0.9031931161880493),\n",
              " ('college', 0.8969163298606873),\n",
              " ('electronic', 0.8959286212921143),\n",
              " ('books', 0.888676643371582),\n",
              " ('greatly', 0.8844950199127197),\n",
              " ('papers', 0.8828896284103394),\n",
              " ('americans', 0.8791347742080688),\n",
              " ('mac', 0.8752748966217041)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "8ASbgnVqGDvU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup for Model"
      ]
    },
    {
      "metadata": {
        "id": "HoGhPn8n4Gok",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from types import SimpleNamespace\n",
        "# args_dict = {\"batch_size\" : 50, \"epochs\" : 50, \"no_cuda\" : False, \"seed\" : 1, \"log_interval\" : 10}\n",
        "# args = SimpleNamespace(**args_dict)\n",
        "# args.epochs\n",
        "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "args_dict = {\"batch_size\" : 50, \"epochs\" : 30, \"no_cuda\" : False, \"seed\" : 1, \"log_interval\" : 100}\n",
        "args = SimpleNamespace(**args_dict)\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.ToTensor()),\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6E6F6rVgGXnS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define model"
      ]
    },
    {
      "metadata": {
        "id": "vwZfWV16T97w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, vocab_size, num_docs, wordvec_dim, encoder_hidden, rt_normal_dim, K):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        vocab_size = vocab_size\n",
        "        wordvec_dim = wordvec_dim\n",
        "        encoder_hidden = encoder_hidden\n",
        "        rp_normal_dim = rt_normal_dim\n",
        "        K = K\n",
        "        self.num_docs = num_docs\n",
        "        \n",
        "        self.word_embedding = nn.Embedding(vocab_size, wordvec_dim) # decoder\n",
        "#         self.word_embedding = nn.Embedding.from_pretrained(torch.tensor(w2v.syn1neg), freeze=True)\n",
        "        self.topic_embedding = nn.Embedding(K, wordvec_dim) # decoder\n",
        "        self.lin1 = nn.Linear(vocab_size, encoder_hidden) # encoder\n",
        "        self.mean = nn.Linear(encoder_hidden, rp_normal_dim) # encoder\n",
        "        self.logvar = nn.Linear(encoder_hidden, rp_normal_dim) # encoder\n",
        "        self.lin2 = nn.Linear(rp_normal_dim, K) # decoder \n",
        "        #self.topicslayer = nn.Linear(wordvec_dim, K) # decoder\n",
        "        \n",
        "        self.beta = torch.zeros([K, vocab_size], dtype = torch.float32) # decoder\n",
        "        #self.theta = torch.zeros([10, K], dtype = torch.float32)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.lin1(x))\n",
        "        return self.mean(h1), self.logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar) # get sigma\n",
        "        eps = torch.randn_like(std) # get epsilon, generated from N(0, I_k) where k is dimension of std: k x 1\n",
        "        return eps.mul(std).add_(mu) # this gives x ~ N(mu, var)\n",
        "        # note: .mul is element-wise multiplication - this is fine, since sigma is diagonal matrix\n",
        "\n",
        "      \n",
        "    def decode(self, z):\n",
        "        x = self.lin2(z) \n",
        "        theta = F.softmax(x, dim = 1) # to get theta, dim = batch size x K\n",
        "        #word_dot_topic = self.topicslayer(self.word_embedding.weight) # weights corresp to topic vector\n",
        "        self.beta = F.softmax(torch.mm(self.word_embedding.weight, \n",
        "                                       torch.transpose(self.topic_embedding.weight, 0, 1)), dim = 0) # beta, dim = V x K\n",
        "        log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(self.beta, 0, 1))) # dim = batch size x V\n",
        "        #theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "        ####log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "        return log_theta_dot_beta ####log_theta_dot_beta_normalized\n",
        "        \n",
        "    def forward(self, doc):\n",
        "        mu, logvar = self.encode(doc)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar, self.topic_embedding.weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uoGmoqZjaAHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load training data (separate into batches)"
      ]
    },
    {
      "metadata": {
        "id": "yWSJgB41sicr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# mnist_train_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=True, download=True,\n",
        "#                    transform=transforms.ToTensor()),\n",
        "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "# # enumerate(train_loader)\n",
        "\n",
        "train_data = torch.utils.data.TensorDataset(torch.tensor(doc_term_matrix))\n",
        "train_loader = torch.utils.data.DataLoader(train_data,                                            \n",
        "    batch_size = args.batch_size, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dt0_YBz5aQEI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "instantiate model and define functions for training"
      ]
    },
    {
      "metadata": {
        "id": "DO5T6YFkCTIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = VAE(vocab_size = doc_term_matrix.shape[1], \n",
        "            num_docs = doc_term_matrix.shape[0], \n",
        "            wordvec_dim = 100, \n",
        "            encoder_hidden = 256, \n",
        "            rt_normal_dim = 75, \n",
        "            K = 2).to(device) \n",
        "      \n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# optimizer = optim.RMSprop(model.parameters(), lr = 1e-3)\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(data, train_loader, log_theta_dot_beta, x, mu, logvar, t):\n",
        "    vocab_size = doc_term_matrix.shape[1]\n",
        "    rt_normal_dim = 75\n",
        "    # multiply by vocab_size / len(train_loader.dataset) -- this didn't work for any of the 3 possible locations      https://vxlabs.com/2017/12/08/variational-autoencoder-in-pytorch-commented-and-annotated/\n",
        "    #  1.0 / (len(train_loader.dataset) * vocab_size) -- also didn't work\n",
        "    \n",
        "#     BCE = data[0].shape[0] * 1.0 / len(train_loader.dataset) * log_theta_dot_beta.sum() # MC est for expected log lik \n",
        "\n",
        "    KLD = (-0.5) * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # KL divergence; it's fine to sum them all up now, rather than for each sample in mini-batch, because they'll all be summed up anyways\n",
        "    \n",
        "    BCE = log_theta_dot_beta.sum()\n",
        "#     KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "#     K = t.shape[0]\n",
        "#     arccos = []\n",
        "#     for j in range(K):\n",
        "#         for k in range(j, K):\n",
        "#             arccos.append(torch.acos(torch.dot(t[:, j], t[:, k]) /\n",
        "#                                                    (max(t[:, j].norm() * t[:, k].norm(), 1e-5))))\n",
        "#             arccos.append(F.cosine_similarity(t[:, j], t[:, k]))\n",
        "#     arccos = torch.tensor(arccos)\n",
        "#     print(arccos.max())\n",
        "#     zeta = (1 / (K * K)) * arccos.sum()\n",
        "#     nu = torch.zeros(1)\n",
        "#     print(\"zeta: \" + str(zeta) + \"nu: \" + str(nu))\n",
        "#     for a in arccos:\n",
        "#         nu = nu.add((a - zeta).pow(2))\n",
        "#     nu = (1 / (K * K)) * nu\n",
        "\n",
        "    print(\"BCE: \" + \"{:.2f}\".format(float(BCE)))\n",
        "    print(\"KLD: \" + \"{:.2f}\".format(float(KLD)))\n",
        "    print(\"Loss: \" + \"{:.2f}\".format(float(- BCE + KLD)))\n",
        "    \n",
        "    return data[0].shape[0] * 1.0 / len(train_loader.dataset) * (-BCE + KLD)\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "enc_variables = list(model.lin1.parameters()) + list(model.mean.parameters()) + list(model.logvar.parameters())\n",
        "dec_variables = list(model.word_embedding.parameters()) + list(model.lin2.parameters()) + list(model.topic_embedding.parameters())\n",
        "\n",
        "optim_enc = optim.Adam(enc_variables, lr=1e-3)\n",
        "optim_dec = optim.Adam(dec_variables, lr=1e-3)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for switch in range(2):\n",
        "        if switch == 0:\n",
        "            print(\"updating encoder variables\")\n",
        "            optimizer = optim_enc\n",
        "        else:\n",
        "            print(\"updating decoder variables\")\n",
        "            optimizer = optim_dec\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "#         data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        log_theta_beta, mu, logvar, topic_vecs = model(data[0].float())\n",
        "        loss = loss_function(data, train_loader, log_theta_beta, data, mu, logvar, topic_vecs)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * data[0].shape[0], len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "#                 loss.item() / data[0].shape[0]))\n",
        "                loss.item() ))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "    return train_loss / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "# def test(epoch):\n",
        "#     model.eval()\n",
        "#     test_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for i, (data, _) in enumerate(test_loader):\n",
        "#             data = data.to(device)\n",
        "#             recon_batch, mu, logvar = model(data)\n",
        "#             test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "#             if i == 0:\n",
        "#                 n = min(data.size(0), 8)\n",
        "#                 comparison = torch.cat([data[:n],\n",
        "#                                       recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
        "# #                 save_image(comparison.cpu(),\n",
        "# #                          'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "#     test_loss /= len(test_loader.dataset)\n",
        "#     print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     for epoch in range(1, args.epochs + 1):\n",
        "#         train(epoch)\n",
        "#         test(epoch)\n",
        "#         with torch.no_grad():\n",
        "#             sample = torch.randn(64, 20).to(device)\n",
        "#             sample = model.decode(sample).cpu()\n",
        "#             save_image(sample.view(64, 1, 28, 28),\n",
        "#                        'results/sample_' + str(epoch) + '.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zuF9sAfDGjUE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " train the model"
      ]
    },
    {
      "metadata": {
        "id": "XY-GsKzb3b_h",
        "colab_type": "code",
        "outputId": "b13d0fbb-88b3-4114-a199-5aca0acbb3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37247
        }
      },
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    losses.append(train(epoch))\n",
        "#     if epoch > 1:\n",
        "#         if np.abs(losses[epoch-1] - losses[epoch-2]) < 1e-2:\n",
        "#             break\n",
        "\n",
        "\n",
        "\n",
        "# why is KL going to 0 (and has negative sign sometimes, even though it's 0)? - this occurs when switch is off.\n",
        "# when switch is on, KL is small, but never 0.\n",
        "# smaller dimension for normal parameters leads to smaller KL\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -3200579.00\n",
            "KLD: 31.99\n",
            "Loss: 3200611.00\n",
            "Train Epoch: 1 [0/1139 (0%)]\tLoss: 140500.921875\n",
            "BCE: -3175138.75\n",
            "KLD: 39.72\n",
            "Loss: 3175178.50\n",
            "BCE: -3153130.00\n",
            "KLD: 20.02\n",
            "Loss: 3153150.00\n",
            "BCE: -3129754.00\n",
            "KLD: 34.77\n",
            "Loss: 3129788.75\n",
            "BCE: -3101332.75\n",
            "KLD: 269.40\n",
            "Loss: 3101602.25\n",
            "BCE: -3083434.25\n",
            "KLD: 24.72\n",
            "Loss: 3083459.00\n",
            "BCE: -3060193.50\n",
            "KLD: 17.08\n",
            "Loss: 3060210.50\n",
            "BCE: -3032922.75\n",
            "KLD: 40.22\n",
            "Loss: 3032963.00\n",
            "BCE: -3011908.00\n",
            "KLD: 22.91\n",
            "Loss: 3011931.00\n",
            "BCE: -2988769.00\n",
            "KLD: 23.15\n",
            "Loss: 2988792.25\n",
            "BCE: -2966027.00\n",
            "KLD: 20.82\n",
            "Loss: 2966047.75\n",
            "BCE: -2940860.50\n",
            "KLD: 40.97\n",
            "Loss: 2940901.50\n",
            "BCE: -2916207.50\n",
            "KLD: 40.74\n",
            "Loss: 2916248.25\n",
            "BCE: -2898493.00\n",
            "KLD: 24.53\n",
            "Loss: 2898517.50\n",
            "BCE: -2874221.00\n",
            "KLD: 18.73\n",
            "Loss: 2874239.75\n",
            "BCE: -2849180.25\n",
            "KLD: 34.82\n",
            "Loss: 2849215.00\n",
            "BCE: -2824984.50\n",
            "KLD: 24.56\n",
            "Loss: 2825009.00\n",
            "BCE: -2803999.25\n",
            "KLD: 15.41\n",
            "Loss: 2804014.75\n",
            "BCE: -2782209.75\n",
            "KLD: 21.98\n",
            "Loss: 2782231.75\n",
            "BCE: -2762459.50\n",
            "KLD: 32.09\n",
            "Loss: 2762491.50\n",
            "BCE: -2736625.75\n",
            "KLD: 19.77\n",
            "Loss: 2736645.50\n",
            "BCE: -2717595.50\n",
            "KLD: 33.40\n",
            "Loss: 2717629.00\n",
            "BCE: -2100313.75\n",
            "KLD: 46.83\n",
            "Loss: 2100360.50\n",
            "====> Epoch: 1 Average loss: 2568.7228\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -2674526.25\n",
            "KLD: 24.91\n",
            "Loss: 2674551.25\n",
            "Train Epoch: 2 [0/1139 (0%)]\tLoss: 117407.875000\n",
            "BCE: -2652280.75\n",
            "KLD: 24.57\n",
            "Loss: 2652305.25\n",
            "BCE: -2635088.50\n",
            "KLD: 20.24\n",
            "Loss: 2635108.75\n",
            "BCE: -2609760.50\n",
            "KLD: 37.28\n",
            "Loss: 2609797.75\n",
            "BCE: -2590726.00\n",
            "KLD: 19.44\n",
            "Loss: 2590745.50\n",
            "BCE: -2569512.50\n",
            "KLD: 34.15\n",
            "Loss: 2569546.75\n",
            "BCE: -2550762.25\n",
            "KLD: 52.64\n",
            "Loss: 2550815.00\n",
            "BCE: -2530356.50\n",
            "KLD: 32.47\n",
            "Loss: 2530389.00\n",
            "BCE: -2509483.00\n",
            "KLD: 20.38\n",
            "Loss: 2509503.50\n",
            "BCE: -2492259.75\n",
            "KLD: 272.97\n",
            "Loss: 2492532.75\n",
            "BCE: -2468060.75\n",
            "KLD: 25.54\n",
            "Loss: 2468086.25\n",
            "BCE: -2449805.00\n",
            "KLD: 38.28\n",
            "Loss: 2449843.25\n",
            "BCE: -2430117.25\n",
            "KLD: 22.61\n",
            "Loss: 2430139.75\n",
            "BCE: -2408683.75\n",
            "KLD: 50.19\n",
            "Loss: 2408734.00\n",
            "BCE: -2390408.75\n",
            "KLD: 17.18\n",
            "Loss: 2390426.00\n",
            "BCE: -2370067.25\n",
            "KLD: 17.93\n",
            "Loss: 2370085.25\n",
            "BCE: -2350609.50\n",
            "KLD: 20.15\n",
            "Loss: 2350629.75\n",
            "BCE: -2329793.00\n",
            "KLD: 36.34\n",
            "Loss: 2329829.25\n",
            "BCE: -2311602.25\n",
            "KLD: 31.65\n",
            "Loss: 2311634.00\n",
            "BCE: -2293582.75\n",
            "KLD: 17.74\n",
            "Loss: 2293600.50\n",
            "BCE: -2275351.50\n",
            "KLD: 31.86\n",
            "Loss: 2275383.25\n",
            "BCE: -2256529.00\n",
            "KLD: 20.36\n",
            "Loss: 2256549.25\n",
            "BCE: -1746368.00\n",
            "KLD: 29.74\n",
            "Loss: 1746397.75\n",
            "====> Epoch: 2 Average loss: 2139.5024\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -2219023.00\n",
            "KLD: 20.33\n",
            "Loss: 2219043.25\n",
            "Train Epoch: 3 [0/1139 (0%)]\tLoss: 97411.914062\n",
            "BCE: -2200857.50\n",
            "KLD: 40.13\n",
            "Loss: 2200897.75\n",
            "BCE: -2183186.25\n",
            "KLD: 23.95\n",
            "Loss: 2183210.25\n",
            "BCE: -2166282.25\n",
            "KLD: 270.07\n",
            "Loss: 2166552.25\n",
            "BCE: -2152184.75\n",
            "KLD: 24.96\n",
            "Loss: 2152209.75\n",
            "BCE: -2130629.00\n",
            "KLD: 23.32\n",
            "Loss: 2130652.25\n",
            "BCE: -2115816.00\n",
            "KLD: 18.85\n",
            "Loss: 2115834.75\n",
            "BCE: -2097423.50\n",
            "KLD: 22.47\n",
            "Loss: 2097446.00\n",
            "BCE: -2082931.75\n",
            "KLD: 49.00\n",
            "Loss: 2082980.75\n",
            "BCE: -2063829.62\n",
            "KLD: 66.21\n",
            "Loss: 2063895.88\n",
            "BCE: -2047751.50\n",
            "KLD: 28.56\n",
            "Loss: 2047780.00\n",
            "BCE: -2031270.62\n",
            "KLD: 33.62\n",
            "Loss: 2031304.25\n",
            "BCE: -2013970.88\n",
            "KLD: 34.03\n",
            "Loss: 2014004.88\n",
            "BCE: -1997763.38\n",
            "KLD: 20.14\n",
            "Loss: 1997783.50\n",
            "BCE: -1983722.38\n",
            "KLD: 23.51\n",
            "Loss: 1983745.88\n",
            "BCE: -1965527.00\n",
            "KLD: 21.12\n",
            "Loss: 1965548.12\n",
            "BCE: -1950060.38\n",
            "KLD: 38.05\n",
            "Loss: 1950098.38\n",
            "BCE: -1933775.12\n",
            "KLD: 16.69\n",
            "Loss: 1933791.88\n",
            "BCE: -1918914.50\n",
            "KLD: 15.78\n",
            "Loss: 1918930.25\n",
            "BCE: -1903808.50\n",
            "KLD: 18.55\n",
            "Loss: 1903827.00\n",
            "BCE: -1888364.62\n",
            "KLD: 23.14\n",
            "Loss: 1888387.75\n",
            "BCE: -1872360.62\n",
            "KLD: 38.22\n",
            "Loss: 1872398.88\n",
            "BCE: -1450082.25\n",
            "KLD: 27.94\n",
            "Loss: 1450110.25\n",
            "====> Epoch: 3 Average loss: 1774.8657\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -1844076.00\n",
            "KLD: 28.25\n",
            "Loss: 1844104.25\n",
            "Train Epoch: 4 [0/1139 (0%)]\tLoss: 80952.781250\n",
            "BCE: -1829989.38\n",
            "KLD: 29.69\n",
            "Loss: 1830019.12\n",
            "BCE: -1816702.38\n",
            "KLD: 36.79\n",
            "Loss: 1816739.12\n",
            "BCE: -1801640.38\n",
            "KLD: 19.93\n",
            "Loss: 1801660.25\n",
            "BCE: -1787705.00\n",
            "KLD: 29.59\n",
            "Loss: 1787734.62\n",
            "BCE: -1774129.50\n",
            "KLD: 26.04\n",
            "Loss: 1774155.50\n",
            "BCE: -1761142.75\n",
            "KLD: 21.08\n",
            "Loss: 1761163.88\n",
            "BCE: -1747194.25\n",
            "KLD: 15.21\n",
            "Loss: 1747209.50\n",
            "BCE: -1734210.00\n",
            "KLD: 26.16\n",
            "Loss: 1734236.12\n",
            "BCE: -1720191.50\n",
            "KLD: 28.19\n",
            "Loss: 1720219.62\n",
            "BCE: -1706211.62\n",
            "KLD: 24.52\n",
            "Loss: 1706236.12\n",
            "BCE: -1694327.62\n",
            "KLD: 27.57\n",
            "Loss: 1694355.25\n",
            "BCE: -1682095.88\n",
            "KLD: 20.42\n",
            "Loss: 1682116.25\n",
            "BCE: -1668570.25\n",
            "KLD: 39.93\n",
            "Loss: 1668610.12\n",
            "BCE: -1657147.38\n",
            "KLD: 36.79\n",
            "Loss: 1657184.12\n",
            "BCE: -1643281.50\n",
            "KLD: 24.96\n",
            "Loss: 1643306.50\n",
            "BCE: -1631575.50\n",
            "KLD: 24.35\n",
            "Loss: 1631599.88\n",
            "BCE: -1619995.12\n",
            "KLD: 71.42\n",
            "Loss: 1620066.50\n",
            "BCE: -1608149.00\n",
            "KLD: 30.31\n",
            "Loss: 1608179.25\n",
            "BCE: -1596714.00\n",
            "KLD: 22.15\n",
            "Loss: 1596736.12\n",
            "BCE: -1584732.25\n",
            "KLD: 28.60\n",
            "Loss: 1584760.88\n",
            "BCE: -1573994.12\n",
            "KLD: 273.01\n",
            "Loss: 1574267.12\n",
            "BCE: -1218273.00\n",
            "KLD: 13.69\n",
            "Loss: 1218286.62\n",
            "====> Epoch: 4 Average loss: 1481.3190\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -1551338.88\n",
            "KLD: 33.03\n",
            "Loss: 1551371.88\n",
            "Train Epoch: 5 [0/1139 (0%)]\tLoss: 68102.367188\n",
            "BCE: -1541483.38\n",
            "KLD: 33.89\n",
            "Loss: 1541517.25\n",
            "BCE: -1530176.50\n",
            "KLD: 23.19\n",
            "Loss: 1530199.75\n",
            "BCE: -1520106.88\n",
            "KLD: 18.87\n",
            "Loss: 1520125.75\n",
            "BCE: -1510395.00\n",
            "KLD: 28.61\n",
            "Loss: 1510423.62\n",
            "BCE: -1499406.00\n",
            "KLD: 41.70\n",
            "Loss: 1499447.75\n",
            "BCE: -1490628.88\n",
            "KLD: 37.17\n",
            "Loss: 1490666.00\n",
            "BCE: -1480234.75\n",
            "KLD: 69.62\n",
            "Loss: 1480304.38\n",
            "BCE: -1470322.38\n",
            "KLD: 270.38\n",
            "Loss: 1470592.75\n",
            "BCE: -1460861.50\n",
            "KLD: 19.37\n",
            "Loss: 1460880.88\n",
            "BCE: -1451978.88\n",
            "KLD: 19.97\n",
            "Loss: 1451998.88\n",
            "BCE: -1442487.38\n",
            "KLD: 21.99\n",
            "Loss: 1442509.38\n",
            "BCE: -1433342.62\n",
            "KLD: 30.26\n",
            "Loss: 1433372.88\n",
            "BCE: -1424247.00\n",
            "KLD: 20.10\n",
            "Loss: 1424267.12\n",
            "BCE: -1416119.50\n",
            "KLD: 34.15\n",
            "Loss: 1416153.62\n",
            "BCE: -1407136.88\n",
            "KLD: 26.89\n",
            "Loss: 1407163.75\n",
            "BCE: -1397826.62\n",
            "KLD: 40.69\n",
            "Loss: 1397867.38\n",
            "BCE: -1390086.38\n",
            "KLD: 21.01\n",
            "Loss: 1390107.38\n",
            "BCE: -1381418.50\n",
            "KLD: 20.84\n",
            "Loss: 1381439.38\n",
            "BCE: -1373209.00\n",
            "KLD: 27.95\n",
            "Loss: 1373237.00\n",
            "BCE: -1365369.50\n",
            "KLD: 21.41\n",
            "Loss: 1365390.88\n",
            "BCE: -1357777.88\n",
            "KLD: 17.88\n",
            "Loss: 1357795.75\n",
            "BCE: -1052759.38\n",
            "KLD: 19.65\n",
            "Loss: 1052779.00\n",
            "====> Epoch: 5 Average loss: 1260.9833\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -1342209.62\n",
            "KLD: 26.21\n",
            "Loss: 1342235.88\n",
            "Train Epoch: 6 [0/1139 (0%)]\tLoss: 58921.683594\n",
            "BCE: -1335000.00\n",
            "KLD: 17.72\n",
            "Loss: 1335017.75\n",
            "BCE: -1327707.00\n",
            "KLD: 16.61\n",
            "Loss: 1327723.62\n",
            "BCE: -1320896.50\n",
            "KLD: 21.04\n",
            "Loss: 1320917.50\n",
            "BCE: -1313699.25\n",
            "KLD: 33.12\n",
            "Loss: 1313732.38\n",
            "BCE: -1306662.88\n",
            "KLD: 43.47\n",
            "Loss: 1306706.38\n",
            "BCE: -1299858.12\n",
            "KLD: 39.71\n",
            "Loss: 1299897.88\n",
            "BCE: -1293236.00\n",
            "KLD: 33.02\n",
            "Loss: 1293269.00\n",
            "BCE: -1286526.62\n",
            "KLD: 21.82\n",
            "Loss: 1286548.50\n",
            "BCE: -1280077.38\n",
            "KLD: 23.85\n",
            "Loss: 1280101.25\n",
            "BCE: -1273615.75\n",
            "KLD: 36.75\n",
            "Loss: 1273652.50\n",
            "BCE: -1268011.12\n",
            "KLD: 30.59\n",
            "Loss: 1268041.75\n",
            "BCE: -1261784.25\n",
            "KLD: 25.57\n",
            "Loss: 1261809.88\n",
            "BCE: -1255317.62\n",
            "KLD: 21.95\n",
            "Loss: 1255339.62\n",
            "BCE: -1249345.75\n",
            "KLD: 34.72\n",
            "Loss: 1249380.50\n",
            "BCE: -1243498.38\n",
            "KLD: 24.27\n",
            "Loss: 1243522.62\n",
            "BCE: -1237818.00\n",
            "KLD: 24.05\n",
            "Loss: 1237842.00\n",
            "BCE: -1232119.50\n",
            "KLD: 19.23\n",
            "Loss: 1232138.75\n",
            "BCE: -1226250.88\n",
            "KLD: 22.41\n",
            "Loss: 1226273.25\n",
            "BCE: -1221422.00\n",
            "KLD: 66.09\n",
            "Loss: 1221488.12\n",
            "BCE: -1215976.50\n",
            "KLD: 279.17\n",
            "Loss: 1216255.62\n",
            "BCE: -1210468.88\n",
            "KLD: 19.04\n",
            "Loss: 1210487.88\n",
            "BCE: -940309.69\n",
            "KLD: 18.24\n",
            "Loss: 940327.94\n",
            "====> Epoch: 6 Average loss: 1107.5069\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -1200195.12\n",
            "KLD: 21.87\n",
            "Loss: 1200217.00\n",
            "Train Epoch: 7 [0/1139 (0%)]\tLoss: 52687.316406\n",
            "BCE: -1195306.62\n",
            "KLD: 28.78\n",
            "Loss: 1195335.38\n",
            "BCE: -1190780.25\n",
            "KLD: 14.46\n",
            "Loss: 1190794.75\n",
            "BCE: -1185957.62\n",
            "KLD: 53.89\n",
            "Loss: 1186011.50\n",
            "BCE: -1181609.50\n",
            "KLD: 20.17\n",
            "Loss: 1181629.62\n",
            "BCE: -1176631.25\n",
            "KLD: 40.61\n",
            "Loss: 1176671.88\n",
            "BCE: -1172539.62\n",
            "KLD: 31.94\n",
            "Loss: 1172571.62\n",
            "BCE: -1168102.25\n",
            "KLD: 22.22\n",
            "Loss: 1168124.50\n",
            "BCE: -1163986.12\n",
            "KLD: 269.02\n",
            "Loss: 1164255.12\n",
            "BCE: -1159449.38\n",
            "KLD: 28.88\n",
            "Loss: 1159478.25\n",
            "BCE: -1154892.62\n",
            "KLD: 28.37\n",
            "Loss: 1154921.00\n",
            "BCE: -1151000.50\n",
            "KLD: 24.93\n",
            "Loss: 1151025.38\n",
            "BCE: -1146899.38\n",
            "KLD: 55.04\n",
            "Loss: 1146954.38\n",
            "BCE: -1142969.50\n",
            "KLD: 40.59\n",
            "Loss: 1143010.12\n",
            "BCE: -1139078.75\n",
            "KLD: 32.06\n",
            "Loss: 1139110.75\n",
            "BCE: -1135169.88\n",
            "KLD: 21.64\n",
            "Loss: 1135191.50\n",
            "BCE: -1131514.12\n",
            "KLD: 36.75\n",
            "Loss: 1131550.88\n",
            "BCE: -1127560.12\n",
            "KLD: 24.28\n",
            "Loss: 1127584.38\n",
            "BCE: -1124104.88\n",
            "KLD: 31.56\n",
            "Loss: 1124136.50\n",
            "BCE: -1120176.12\n",
            "KLD: 19.65\n",
            "Loss: 1120195.75\n",
            "BCE: -1117032.50\n",
            "KLD: 19.68\n",
            "Loss: 1117052.12\n",
            "BCE: -1113461.25\n",
            "KLD: 16.58\n",
            "Loss: 1113477.88\n",
            "BCE: -865702.62\n",
            "KLD: 15.67\n",
            "Loss: 865718.31\n",
            "====> Epoch: 7 Average loss: 1004.9387\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -1106984.62\n",
            "KLD: 38.89\n",
            "Loss: 1107023.50\n",
            "Train Epoch: 8 [0/1139 (0%)]\tLoss: 48596.292969\n",
            "BCE: -1103787.38\n",
            "KLD: 40.43\n",
            "Loss: 1103827.75\n",
            "BCE: -1100740.88\n",
            "KLD: 28.10\n",
            "Loss: 1100769.00\n",
            "BCE: -1097422.62\n",
            "KLD: 25.44\n",
            "Loss: 1097448.12\n",
            "BCE: -1094761.00\n",
            "KLD: 30.96\n",
            "Loss: 1094792.00\n",
            "BCE: -1091494.25\n",
            "KLD: 19.34\n",
            "Loss: 1091513.62\n",
            "BCE: -1089082.62\n",
            "KLD: 23.86\n",
            "Loss: 1089106.50\n",
            "BCE: -1085880.12\n",
            "KLD: 25.12\n",
            "Loss: 1085905.25\n",
            "BCE: -1083127.00\n",
            "KLD: 35.08\n",
            "Loss: 1083162.12\n",
            "BCE: -1080168.88\n",
            "KLD: 26.35\n",
            "Loss: 1080195.25\n",
            "BCE: -1077657.62\n",
            "KLD: 21.82\n",
            "Loss: 1077679.50\n",
            "BCE: -1074842.88\n",
            "KLD: 16.52\n",
            "Loss: 1074859.38\n",
            "BCE: -1072452.38\n",
            "KLD: 20.47\n",
            "Loss: 1072472.88\n",
            "BCE: -1069847.38\n",
            "KLD: 23.20\n",
            "Loss: 1069870.62\n",
            "BCE: -1067329.62\n",
            "KLD: 25.36\n",
            "Loss: 1067355.00\n",
            "BCE: -1064686.38\n",
            "KLD: 31.05\n",
            "Loss: 1064717.38\n",
            "BCE: -1062418.50\n",
            "KLD: 30.79\n",
            "Loss: 1062449.25\n",
            "BCE: -1060086.12\n",
            "KLD: 23.56\n",
            "Loss: 1060109.62\n",
            "BCE: -1057473.12\n",
            "KLD: 20.00\n",
            "Loss: 1057493.12\n",
            "BCE: -1055385.88\n",
            "KLD: 286.68\n",
            "Loss: 1055672.50\n",
            "BCE: -1053066.25\n",
            "KLD: 45.65\n",
            "Loss: 1053111.88\n",
            "BCE: -1051027.38\n",
            "KLD: 43.24\n",
            "Loss: 1051070.62\n",
            "BCE: -818000.44\n",
            "KLD: 16.74\n",
            "Loss: 818017.19\n",
            "====> Epoch: 8 Average loss: 938.0353\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -1046901.25\n",
            "KLD: 23.23\n",
            "Loss: 1046924.50\n",
            "Train Epoch: 9 [0/1139 (0%)]\tLoss: 45958.058594\n",
            "BCE: -1044741.12\n",
            "KLD: 25.93\n",
            "Loss: 1044767.06\n",
            "BCE: -1042689.12\n",
            "KLD: 20.38\n",
            "Loss: 1042709.50\n",
            "BCE: -1040787.31\n",
            "KLD: 28.48\n",
            "Loss: 1040815.81\n",
            "BCE: -1038939.38\n",
            "KLD: 291.40\n",
            "Loss: 1039230.75\n",
            "BCE: -1036986.38\n",
            "KLD: 15.88\n",
            "Loss: 1037002.25\n",
            "BCE: -1035130.12\n",
            "KLD: 28.77\n",
            "Loss: 1035158.88\n",
            "BCE: -1033240.44\n",
            "KLD: 16.33\n",
            "Loss: 1033256.75\n",
            "BCE: -1031553.44\n",
            "KLD: 22.70\n",
            "Loss: 1031576.12\n",
            "BCE: -1029813.81\n",
            "KLD: 43.35\n",
            "Loss: 1029857.19\n",
            "BCE: -1027917.19\n",
            "KLD: 49.90\n",
            "Loss: 1027967.06\n",
            "BCE: -1026266.81\n",
            "KLD: 26.77\n",
            "Loss: 1026293.56\n",
            "BCE: -1024672.56\n",
            "KLD: 31.37\n",
            "Loss: 1024703.94\n",
            "BCE: -1023068.88\n",
            "KLD: 25.72\n",
            "Loss: 1023094.62\n",
            "BCE: -1021416.31\n",
            "KLD: 20.40\n",
            "Loss: 1021436.69\n",
            "BCE: -1019998.06\n",
            "KLD: 20.84\n",
            "Loss: 1020018.88\n",
            "BCE: -1018259.62\n",
            "KLD: 35.95\n",
            "Loss: 1018295.56\n",
            "BCE: -1016943.81\n",
            "KLD: 34.90\n",
            "Loss: 1016978.69\n",
            "BCE: -1015379.12\n",
            "KLD: 20.41\n",
            "Loss: 1015399.56\n",
            "BCE: -1013991.56\n",
            "KLD: 40.32\n",
            "Loss: 1014031.88\n",
            "BCE: -1012572.12\n",
            "KLD: 20.49\n",
            "Loss: 1012592.62\n",
            "BCE: -1011198.38\n",
            "KLD: 33.10\n",
            "Loss: 1011231.50\n",
            "BCE: -787683.50\n",
            "KLD: 22.01\n",
            "Loss: 787705.50\n",
            "====> Epoch: 9 Average loss: 895.2200\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -1008481.38\n",
            "KLD: 14.27\n",
            "Loss: 1008495.62\n",
            "Train Epoch: 10 [0/1139 (0%)]\tLoss: 44271.101562\n",
            "BCE: -1007388.38\n",
            "KLD: 38.36\n",
            "Loss: 1007426.75\n",
            "BCE: -1006028.88\n",
            "KLD: 38.11\n",
            "Loss: 1006067.00\n",
            "BCE: -1004892.25\n",
            "KLD: 23.61\n",
            "Loss: 1004915.88\n",
            "BCE: -1003684.62\n",
            "KLD: 22.40\n",
            "Loss: 1003707.00\n",
            "BCE: -1002620.94\n",
            "KLD: 33.50\n",
            "Loss: 1002654.44\n",
            "BCE: -1001366.31\n",
            "KLD: 23.84\n",
            "Loss: 1001390.12\n",
            "BCE: -1000166.44\n",
            "KLD: 25.04\n",
            "Loss: 1000191.50\n",
            "BCE: -999267.00\n",
            "KLD: 18.38\n",
            "Loss: 999285.38\n",
            "BCE: -998140.31\n",
            "KLD: 35.98\n",
            "Loss: 998176.31\n",
            "BCE: -997150.44\n",
            "KLD: 49.92\n",
            "Loss: 997200.38\n",
            "BCE: -996118.69\n",
            "KLD: 73.13\n",
            "Loss: 996191.81\n",
            "BCE: -995075.94\n",
            "KLD: 25.78\n",
            "Loss: 995101.75\n",
            "BCE: -994072.12\n",
            "KLD: 15.57\n",
            "Loss: 994087.69\n",
            "BCE: -993111.50\n",
            "KLD: 18.56\n",
            "Loss: 993130.06\n",
            "BCE: -992136.38\n",
            "KLD: 35.26\n",
            "Loss: 992171.62\n",
            "BCE: -991332.25\n",
            "KLD: 21.21\n",
            "Loss: 991353.44\n",
            "BCE: -990359.19\n",
            "KLD: 17.94\n",
            "Loss: 990377.12\n",
            "BCE: -989459.81\n",
            "KLD: 29.09\n",
            "Loss: 989488.88\n",
            "BCE: -988624.44\n",
            "KLD: 26.76\n",
            "Loss: 988651.19\n",
            "BCE: -987799.38\n",
            "KLD: 14.94\n",
            "Loss: 987814.31\n",
            "BCE: -986878.44\n",
            "KLD: 18.28\n",
            "Loss: 986896.69\n",
            "BCE: -769204.94\n",
            "KLD: 278.71\n",
            "Loss: 769483.62\n",
            "====> Epoch: 10 Average loss: 868.5195\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -985368.00\n",
            "KLD: 37.12\n",
            "Loss: 985405.12\n",
            "Train Epoch: 11 [0/1139 (0%)]\tLoss: 43257.468750\n",
            "BCE: -984641.25\n",
            "KLD: 25.30\n",
            "Loss: 984666.56\n",
            "BCE: -983734.75\n",
            "KLD: 28.87\n",
            "Loss: 983763.62\n",
            "BCE: -983095.62\n",
            "KLD: 18.37\n",
            "Loss: 983114.00\n",
            "BCE: -982340.56\n",
            "KLD: 21.78\n",
            "Loss: 982362.31\n",
            "BCE: -981642.69\n",
            "KLD: 26.73\n",
            "Loss: 981669.44\n",
            "BCE: -980958.19\n",
            "KLD: 23.93\n",
            "Loss: 980982.12\n",
            "BCE: -980283.81\n",
            "KLD: 22.84\n",
            "Loss: 980306.62\n",
            "BCE: -979731.81\n",
            "KLD: 28.63\n",
            "Loss: 979760.44\n",
            "BCE: -979010.31\n",
            "KLD: 16.17\n",
            "Loss: 979026.50\n",
            "BCE: -978327.38\n",
            "KLD: 35.47\n",
            "Loss: 978362.88\n",
            "BCE: -977666.69\n",
            "KLD: 24.56\n",
            "Loss: 977691.25\n",
            "BCE: -977173.00\n",
            "KLD: 31.47\n",
            "Loss: 977204.50\n",
            "BCE: -976501.69\n",
            "KLD: 34.33\n",
            "Loss: 976536.00\n",
            "BCE: -975933.50\n",
            "KLD: 275.15\n",
            "Loss: 976208.62\n",
            "BCE: -975357.81\n",
            "KLD: 21.59\n",
            "Loss: 975379.38\n",
            "BCE: -974827.00\n",
            "KLD: 53.08\n",
            "Loss: 974880.06\n",
            "BCE: -974275.38\n",
            "KLD: 19.09\n",
            "Loss: 974294.50\n",
            "BCE: -973739.75\n",
            "KLD: 33.72\n",
            "Loss: 973773.50\n",
            "BCE: -973148.00\n",
            "KLD: 38.65\n",
            "Loss: 973186.62\n",
            "BCE: -972665.56\n",
            "KLD: 28.63\n",
            "Loss: 972694.19\n",
            "BCE: -972093.25\n",
            "KLD: 19.93\n",
            "Loss: 972113.19\n",
            "BCE: -757961.38\n",
            "KLD: 33.22\n",
            "Loss: 757994.56\n",
            "====> Epoch: 11 Average loss: 852.3187\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -971179.56\n",
            "KLD: 17.89\n",
            "Loss: 971197.44\n",
            "Train Epoch: 12 [0/1139 (0%)]\tLoss: 42633.777344\n",
            "BCE: -970743.44\n",
            "KLD: 33.95\n",
            "Loss: 970777.38\n",
            "BCE: -970439.94\n",
            "KLD: 17.35\n",
            "Loss: 970457.31\n",
            "BCE: -969852.56\n",
            "KLD: 28.36\n",
            "Loss: 969880.94\n",
            "BCE: -969383.44\n",
            "KLD: 26.56\n",
            "Loss: 969410.00\n",
            "BCE: -969009.50\n",
            "KLD: 17.95\n",
            "Loss: 969027.44\n",
            "BCE: -968623.56\n",
            "KLD: 25.87\n",
            "Loss: 968649.44\n",
            "BCE: -968215.25\n",
            "KLD: 43.03\n",
            "Loss: 968258.25\n",
            "BCE: -967894.75\n",
            "KLD: 292.86\n",
            "Loss: 968187.62\n",
            "BCE: -967420.88\n",
            "KLD: 33.49\n",
            "Loss: 967454.38\n",
            "BCE: -967079.62\n",
            "KLD: 44.48\n",
            "Loss: 967124.12\n",
            "BCE: -966734.75\n",
            "KLD: 31.94\n",
            "Loss: 966766.69\n",
            "BCE: -966394.81\n",
            "KLD: 25.31\n",
            "Loss: 966420.12\n",
            "BCE: -966016.12\n",
            "KLD: 17.73\n",
            "Loss: 966033.88\n",
            "BCE: -965608.38\n",
            "KLD: 49.54\n",
            "Loss: 965657.94\n",
            "BCE: -965339.62\n",
            "KLD: 22.42\n",
            "Loss: 965362.06\n",
            "BCE: -965045.75\n",
            "KLD: 24.57\n",
            "Loss: 965070.31\n",
            "BCE: -964639.38\n",
            "KLD: 24.15\n",
            "Loss: 964663.50\n",
            "BCE: -964319.69\n",
            "KLD: 19.09\n",
            "Loss: 964338.81\n",
            "BCE: -963991.12\n",
            "KLD: 33.77\n",
            "Loss: 964024.88\n",
            "BCE: -963792.44\n",
            "KLD: 31.96\n",
            "Loss: 963824.38\n",
            "BCE: -963469.00\n",
            "KLD: 23.08\n",
            "Loss: 963492.06\n",
            "BCE: -751286.69\n",
            "KLD: 13.27\n",
            "Loss: 751299.94\n",
            "====> Epoch: 12 Average loss: 842.5861\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -962857.75\n",
            "KLD: 25.26\n",
            "Loss: 962883.00\n",
            "Train Epoch: 13 [0/1139 (0%)]\tLoss: 42268.789062\n",
            "BCE: -962672.88\n",
            "KLD: 58.87\n",
            "Loss: 962731.75\n",
            "BCE: -962382.75\n",
            "KLD: 26.77\n",
            "Loss: 962409.50\n",
            "BCE: -962121.25\n",
            "KLD: 20.84\n",
            "Loss: 962142.06\n",
            "BCE: -961919.94\n",
            "KLD: 20.77\n",
            "Loss: 961940.69\n",
            "BCE: -961690.44\n",
            "KLD: 277.80\n",
            "Loss: 961968.25\n",
            "BCE: -961383.62\n",
            "KLD: 41.38\n",
            "Loss: 961425.00\n",
            "BCE: -961210.50\n",
            "KLD: 18.75\n",
            "Loss: 961229.25\n",
            "BCE: -960922.62\n",
            "KLD: 20.83\n",
            "Loss: 960943.44\n",
            "BCE: -960680.12\n",
            "KLD: 35.15\n",
            "Loss: 960715.25\n",
            "BCE: -960514.56\n",
            "KLD: 50.25\n",
            "Loss: 960564.81\n",
            "BCE: -960301.12\n",
            "KLD: 19.92\n",
            "Loss: 960321.06\n",
            "BCE: -960160.62\n",
            "KLD: 17.88\n",
            "Loss: 960178.50\n",
            "BCE: -959918.56\n",
            "KLD: 21.07\n",
            "Loss: 959939.62\n",
            "BCE: -959791.12\n",
            "KLD: 20.74\n",
            "Loss: 959811.88\n",
            "BCE: -959533.50\n",
            "KLD: 39.63\n",
            "Loss: 959573.12\n",
            "BCE: -959396.75\n",
            "KLD: 17.51\n",
            "Loss: 959414.25\n",
            "BCE: -959160.12\n",
            "KLD: 30.63\n",
            "Loss: 959190.75\n",
            "BCE: -959043.25\n",
            "KLD: 44.39\n",
            "Loss: 959087.62\n",
            "BCE: -958804.00\n",
            "KLD: 18.62\n",
            "Loss: 958822.62\n",
            "BCE: -958635.06\n",
            "KLD: 21.46\n",
            "Loss: 958656.50\n",
            "BCE: -958525.00\n",
            "KLD: 33.17\n",
            "Loss: 958558.19\n",
            "BCE: -747487.69\n",
            "KLD: 16.95\n",
            "Loss: 747504.62\n",
            "====> Epoch: 13 Average loss: 836.9386\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -958199.38\n",
            "KLD: 47.30\n",
            "Loss: 958246.69\n",
            "Train Epoch: 14 [0/1139 (0%)]\tLoss: 42065.265625\n",
            "BCE: -958071.62\n",
            "KLD: 21.51\n",
            "Loss: 958093.12\n",
            "BCE: -957912.31\n",
            "KLD: 21.82\n",
            "Loss: 957934.12\n",
            "BCE: -957770.94\n",
            "KLD: 27.45\n",
            "Loss: 957798.38\n",
            "BCE: -957618.81\n",
            "KLD: 34.73\n",
            "Loss: 957653.56\n",
            "BCE: -957537.31\n",
            "KLD: 24.14\n",
            "Loss: 957561.44\n",
            "BCE: -957449.31\n",
            "KLD: 33.10\n",
            "Loss: 957482.44\n",
            "BCE: -957256.38\n",
            "KLD: 18.03\n",
            "Loss: 957274.38\n",
            "BCE: -957138.44\n",
            "KLD: 15.42\n",
            "Loss: 957153.88\n",
            "BCE: -957046.75\n",
            "KLD: 28.16\n",
            "Loss: 957074.94\n",
            "BCE: -956914.06\n",
            "KLD: 22.85\n",
            "Loss: 956936.94\n",
            "BCE: -956810.44\n",
            "KLD: 17.75\n",
            "Loss: 956828.19\n",
            "BCE: -956698.62\n",
            "KLD: 280.19\n",
            "Loss: 956978.81\n",
            "BCE: -956600.88\n",
            "KLD: 35.88\n",
            "Loss: 956636.75\n",
            "BCE: -956465.88\n",
            "KLD: 17.68\n",
            "Loss: 956483.56\n",
            "BCE: -956380.50\n",
            "KLD: 43.12\n",
            "Loss: 956423.62\n",
            "BCE: -956277.88\n",
            "KLD: 48.31\n",
            "Loss: 956326.19\n",
            "BCE: -956145.94\n",
            "KLD: 28.57\n",
            "Loss: 956174.50\n",
            "BCE: -956100.81\n",
            "KLD: 19.64\n",
            "Loss: 956120.44\n",
            "BCE: -956000.81\n",
            "KLD: 56.76\n",
            "Loss: 956057.56\n",
            "BCE: -955918.69\n",
            "KLD: 16.92\n",
            "Loss: 955935.62\n",
            "BCE: -955818.31\n",
            "KLD: 21.35\n",
            "Loss: 955839.69\n",
            "BCE: -745428.44\n",
            "KLD: 17.94\n",
            "Loss: 745446.38\n",
            "====> Epoch: 14 Average loss: 833.8131\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -955604.75\n",
            "KLD: 53.79\n",
            "Loss: 955658.56\n",
            "Train Epoch: 15 [0/1139 (0%)]\tLoss: 41951.648438\n",
            "BCE: -955558.75\n",
            "KLD: 36.47\n",
            "Loss: 955595.25\n",
            "BCE: -955473.12\n",
            "KLD: 36.27\n",
            "Loss: 955509.38\n",
            "BCE: -955399.12\n",
            "KLD: 30.14\n",
            "Loss: 955429.25\n",
            "BCE: -955311.06\n",
            "KLD: 23.64\n",
            "Loss: 955334.69\n",
            "BCE: -955283.81\n",
            "KLD: 16.43\n",
            "Loss: 955300.25\n",
            "BCE: -955158.56\n",
            "KLD: 24.27\n",
            "Loss: 955182.81\n",
            "BCE: -955128.56\n",
            "KLD: 52.58\n",
            "Loss: 955181.12\n",
            "BCE: -955022.38\n",
            "KLD: 18.52\n",
            "Loss: 955040.88\n",
            "BCE: -954946.12\n",
            "KLD: 17.02\n",
            "Loss: 954963.12\n",
            "BCE: -954911.50\n",
            "KLD: 23.93\n",
            "Loss: 954935.44\n",
            "BCE: -954840.00\n",
            "KLD: 34.19\n",
            "Loss: 954874.19\n",
            "BCE: -954792.75\n",
            "KLD: 18.43\n",
            "Loss: 954811.19\n",
            "BCE: -954730.12\n",
            "KLD: 19.74\n",
            "Loss: 954749.88\n",
            "BCE: -954662.56\n",
            "KLD: 22.52\n",
            "Loss: 954685.06\n",
            "BCE: -954613.06\n",
            "KLD: 36.88\n",
            "Loss: 954649.94\n",
            "BCE: -954552.19\n",
            "KLD: 16.73\n",
            "Loss: 954568.94\n",
            "BCE: -954455.62\n",
            "KLD: 47.63\n",
            "Loss: 954503.25\n",
            "BCE: -954429.69\n",
            "KLD: 32.84\n",
            "Loss: 954462.50\n",
            "BCE: -954383.12\n",
            "KLD: 17.47\n",
            "Loss: 954400.62\n",
            "BCE: -954352.31\n",
            "KLD: 270.26\n",
            "Loss: 954622.56\n",
            "BCE: -954238.19\n",
            "KLD: 31.33\n",
            "Loss: 954269.50\n",
            "BCE: -744294.31\n",
            "KLD: 17.53\n",
            "Loss: 744311.81\n",
            "====> Epoch: 15 Average loss: 832.0721\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -954166.25\n",
            "KLD: 14.64\n",
            "Loss: 954180.88\n",
            "Train Epoch: 16 [0/1139 (0%)]\tLoss: 41886.781250\n",
            "BCE: -954120.56\n",
            "KLD: 16.91\n",
            "Loss: 954137.50\n",
            "BCE: -954084.69\n",
            "KLD: 23.83\n",
            "Loss: 954108.50\n",
            "BCE: -954020.25\n",
            "KLD: 22.13\n",
            "Loss: 954042.38\n",
            "BCE: -953977.88\n",
            "KLD: 55.34\n",
            "Loss: 954033.19\n",
            "BCE: -953918.94\n",
            "KLD: 274.44\n",
            "Loss: 954193.38\n",
            "BCE: -953886.69\n",
            "KLD: 35.99\n",
            "Loss: 953922.69\n",
            "BCE: -953845.31\n",
            "KLD: 18.76\n",
            "Loss: 953864.06\n",
            "BCE: -953810.44\n",
            "KLD: 27.15\n",
            "Loss: 953837.56\n",
            "BCE: -953766.25\n",
            "KLD: 22.74\n",
            "Loss: 953789.00\n",
            "BCE: -953704.62\n",
            "KLD: 58.16\n",
            "Loss: 953762.81\n",
            "BCE: -953657.69\n",
            "KLD: 19.04\n",
            "Loss: 953676.75\n",
            "BCE: -953633.75\n",
            "KLD: 24.12\n",
            "Loss: 953657.88\n",
            "BCE: -953586.44\n",
            "KLD: 59.39\n",
            "Loss: 953645.81\n",
            "BCE: -953565.38\n",
            "KLD: 17.93\n",
            "Loss: 953583.31\n",
            "BCE: -953541.88\n",
            "KLD: 24.13\n",
            "Loss: 953566.00\n",
            "BCE: -953486.38\n",
            "KLD: 33.46\n",
            "Loss: 953519.81\n",
            "BCE: -953447.88\n",
            "KLD: 25.05\n",
            "Loss: 953472.94\n",
            "BCE: -953417.56\n",
            "KLD: 19.70\n",
            "Loss: 953437.25\n",
            "BCE: -953384.00\n",
            "KLD: 27.49\n",
            "Loss: 953411.50\n",
            "BCE: -953362.00\n",
            "KLD: 23.71\n",
            "Loss: 953385.69\n",
            "BCE: -953326.62\n",
            "KLD: 24.56\n",
            "Loss: 953351.19\n",
            "BCE: -743564.06\n",
            "KLD: 29.97\n",
            "Loss: 743594.00\n",
            "====> Epoch: 16 Average loss: 831.0427\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -953295.31\n",
            "KLD: 19.58\n",
            "Loss: 953314.88\n",
            "Train Epoch: 17 [0/1139 (0%)]\tLoss: 41848.765625\n",
            "BCE: -953239.12\n",
            "KLD: 18.75\n",
            "Loss: 953257.88\n",
            "BCE: -953209.94\n",
            "KLD: 21.49\n",
            "Loss: 953231.44\n",
            "BCE: -953186.69\n",
            "KLD: 18.37\n",
            "Loss: 953205.06\n",
            "BCE: -953150.75\n",
            "KLD: 271.20\n",
            "Loss: 953421.94\n",
            "BCE: -953149.38\n",
            "KLD: 23.50\n",
            "Loss: 953172.88\n",
            "BCE: -953111.88\n",
            "KLD: 17.95\n",
            "Loss: 953129.81\n",
            "BCE: -953095.50\n",
            "KLD: 20.73\n",
            "Loss: 953116.25\n",
            "BCE: -953072.50\n",
            "KLD: 22.42\n",
            "Loss: 953094.94\n",
            "BCE: -953047.25\n",
            "KLD: 34.65\n",
            "Loss: 953081.88\n",
            "BCE: -953026.62\n",
            "KLD: 35.45\n",
            "Loss: 953062.06\n",
            "BCE: -953002.62\n",
            "KLD: 48.03\n",
            "Loss: 953050.69\n",
            "BCE: -952964.62\n",
            "KLD: 22.22\n",
            "Loss: 952986.88\n",
            "BCE: -952959.88\n",
            "KLD: 20.21\n",
            "Loss: 952980.06\n",
            "BCE: -952944.25\n",
            "KLD: 22.46\n",
            "Loss: 952966.69\n",
            "BCE: -952918.56\n",
            "KLD: 16.92\n",
            "Loss: 952935.50\n",
            "BCE: -952898.88\n",
            "KLD: 35.93\n",
            "Loss: 952934.81\n",
            "BCE: -952893.50\n",
            "KLD: 32.49\n",
            "Loss: 952926.00\n",
            "BCE: -952862.38\n",
            "KLD: 49.53\n",
            "Loss: 952911.88\n",
            "BCE: -952862.19\n",
            "KLD: 39.75\n",
            "Loss: 952901.94\n",
            "BCE: -952835.69\n",
            "KLD: 22.25\n",
            "Loss: 952857.94\n",
            "BCE: -952835.69\n",
            "KLD: 62.40\n",
            "Loss: 952898.06\n",
            "BCE: -743193.50\n",
            "KLD: 22.35\n",
            "Loss: 743215.88\n",
            "====> Epoch: 17 Average loss: 830.4478\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952803.19\n",
            "KLD: 290.39\n",
            "Loss: 953093.56\n",
            "Train Epoch: 18 [0/1139 (0%)]\tLoss: 41839.050781\n",
            "BCE: -952798.00\n",
            "KLD: 26.08\n",
            "Loss: 952824.06\n",
            "BCE: -952767.19\n",
            "KLD: 25.38\n",
            "Loss: 952792.56\n",
            "BCE: -952759.69\n",
            "KLD: 25.19\n",
            "Loss: 952784.88\n",
            "BCE: -952745.44\n",
            "KLD: 42.12\n",
            "Loss: 952787.56\n",
            "BCE: -952742.19\n",
            "KLD: 24.56\n",
            "Loss: 952766.75\n",
            "BCE: -952729.94\n",
            "KLD: 20.13\n",
            "Loss: 952750.06\n",
            "BCE: -952714.00\n",
            "KLD: 19.39\n",
            "Loss: 952733.38\n",
            "BCE: -952707.38\n",
            "KLD: 21.13\n",
            "Loss: 952728.50\n",
            "BCE: -952710.12\n",
            "KLD: 19.40\n",
            "Loss: 952729.50\n",
            "BCE: -952685.12\n",
            "KLD: 36.36\n",
            "Loss: 952721.50\n",
            "BCE: -952676.06\n",
            "KLD: 23.71\n",
            "Loss: 952699.75\n",
            "BCE: -952669.44\n",
            "KLD: 20.79\n",
            "Loss: 952690.25\n",
            "BCE: -952658.06\n",
            "KLD: 22.05\n",
            "Loss: 952680.12\n",
            "BCE: -952645.12\n",
            "KLD: 54.87\n",
            "Loss: 952700.00\n",
            "BCE: -952645.25\n",
            "KLD: 34.03\n",
            "Loss: 952679.25\n",
            "BCE: -952639.00\n",
            "KLD: 20.09\n",
            "Loss: 952659.06\n",
            "BCE: -952628.94\n",
            "KLD: 16.69\n",
            "Loss: 952645.62\n",
            "BCE: -952624.19\n",
            "KLD: 32.78\n",
            "Loss: 952657.00\n",
            "BCE: -952619.81\n",
            "KLD: 30.62\n",
            "Loss: 952650.44\n",
            "BCE: -952599.69\n",
            "KLD: 16.39\n",
            "Loss: 952616.06\n",
            "BCE: -952603.19\n",
            "KLD: 25.95\n",
            "Loss: 952629.12\n",
            "BCE: -743017.19\n",
            "KLD: 50.55\n",
            "Loss: 743067.75\n",
            "====> Epoch: 18 Average loss: 830.1574\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952588.06\n",
            "KLD: 35.29\n",
            "Loss: 952623.38\n",
            "Train Epoch: 19 [0/1139 (0%)]\tLoss: 41818.410156\n",
            "BCE: -952584.62\n",
            "KLD: 19.27\n",
            "Loss: 952603.88\n",
            "BCE: -952591.62\n",
            "KLD: 284.85\n",
            "Loss: 952876.50\n",
            "BCE: -952573.56\n",
            "KLD: 25.39\n",
            "Loss: 952598.94\n",
            "BCE: -952558.38\n",
            "KLD: 44.70\n",
            "Loss: 952603.06\n",
            "BCE: -952550.69\n",
            "KLD: 21.90\n",
            "Loss: 952572.56\n",
            "BCE: -952551.81\n",
            "KLD: 23.19\n",
            "Loss: 952575.00\n",
            "BCE: -952553.69\n",
            "KLD: 47.49\n",
            "Loss: 952601.19\n",
            "BCE: -952541.62\n",
            "KLD: 48.06\n",
            "Loss: 952589.69\n",
            "BCE: -952538.19\n",
            "KLD: 26.10\n",
            "Loss: 952564.31\n",
            "BCE: -952531.88\n",
            "KLD: 22.98\n",
            "Loss: 952554.88\n",
            "BCE: -952525.94\n",
            "KLD: 30.76\n",
            "Loss: 952556.69\n",
            "BCE: -952522.88\n",
            "KLD: 16.40\n",
            "Loss: 952539.25\n",
            "BCE: -952516.56\n",
            "KLD: 25.24\n",
            "Loss: 952541.81\n",
            "BCE: -952510.06\n",
            "KLD: 27.31\n",
            "Loss: 952537.38\n",
            "BCE: -952522.88\n",
            "KLD: 28.25\n",
            "Loss: 952551.12\n",
            "BCE: -952516.62\n",
            "KLD: 17.37\n",
            "Loss: 952534.00\n",
            "BCE: -952502.81\n",
            "KLD: 21.71\n",
            "Loss: 952524.50\n",
            "BCE: -952496.38\n",
            "KLD: 18.72\n",
            "Loss: 952515.12\n",
            "BCE: -952485.69\n",
            "KLD: 46.75\n",
            "Loss: 952532.44\n",
            "BCE: -952485.12\n",
            "KLD: 31.00\n",
            "Loss: 952516.12\n",
            "BCE: -952486.44\n",
            "KLD: 17.66\n",
            "Loss: 952504.12\n",
            "BCE: -742935.88\n",
            "KLD: 18.23\n",
            "Loss: 742954.12\n",
            "====> Epoch: 19 Average loss: 830.0228\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952476.12\n",
            "KLD: 22.44\n",
            "Loss: 952498.56\n",
            "Train Epoch: 20 [0/1139 (0%)]\tLoss: 41812.933594\n",
            "BCE: -952472.94\n",
            "KLD: 21.62\n",
            "Loss: 952494.56\n",
            "BCE: -952465.88\n",
            "KLD: 19.08\n",
            "Loss: 952484.94\n",
            "BCE: -952457.50\n",
            "KLD: 38.58\n",
            "Loss: 952496.06\n",
            "BCE: -952455.56\n",
            "KLD: 34.89\n",
            "Loss: 952490.44\n",
            "BCE: -952458.44\n",
            "KLD: 22.66\n",
            "Loss: 952481.12\n",
            "BCE: -952450.94\n",
            "KLD: 19.20\n",
            "Loss: 952470.12\n",
            "BCE: -952447.06\n",
            "KLD: 29.50\n",
            "Loss: 952476.56\n",
            "BCE: -952441.44\n",
            "KLD: 34.95\n",
            "Loss: 952476.38\n",
            "BCE: -952436.81\n",
            "KLD: 25.22\n",
            "Loss: 952462.00\n",
            "BCE: -952437.69\n",
            "KLD: 42.86\n",
            "Loss: 952480.56\n",
            "BCE: -952429.69\n",
            "KLD: 19.16\n",
            "Loss: 952448.88\n",
            "BCE: -952427.44\n",
            "KLD: 32.86\n",
            "Loss: 952460.31\n",
            "BCE: -952420.00\n",
            "KLD: 29.06\n",
            "Loss: 952449.06\n",
            "BCE: -952426.06\n",
            "KLD: 273.92\n",
            "Loss: 952700.00\n",
            "BCE: -952414.75\n",
            "KLD: 21.66\n",
            "Loss: 952436.44\n",
            "BCE: -952416.69\n",
            "KLD: 13.94\n",
            "Loss: 952430.62\n",
            "BCE: -952411.75\n",
            "KLD: 17.49\n",
            "Loss: 952429.25\n",
            "BCE: -952405.69\n",
            "KLD: 34.00\n",
            "Loss: 952439.69\n",
            "BCE: -952405.44\n",
            "KLD: 38.31\n",
            "Loss: 952443.75\n",
            "BCE: -952399.44\n",
            "KLD: 36.87\n",
            "Loss: 952436.31\n",
            "BCE: -952397.06\n",
            "KLD: 50.65\n",
            "Loss: 952447.69\n",
            "BCE: -742866.19\n",
            "KLD: 19.71\n",
            "Loss: 742885.88\n",
            "====> Epoch: 20 Average loss: 829.9367\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952391.81\n",
            "KLD: 31.93\n",
            "Loss: 952423.75\n",
            "Train Epoch: 21 [0/1139 (0%)]\tLoss: 41809.648438\n",
            "BCE: -952390.94\n",
            "KLD: 46.25\n",
            "Loss: 952437.19\n",
            "BCE: -952387.50\n",
            "KLD: 14.19\n",
            "Loss: 952401.69\n",
            "BCE: -952383.06\n",
            "KLD: 287.97\n",
            "Loss: 952671.06\n",
            "BCE: -952377.88\n",
            "KLD: 23.04\n",
            "Loss: 952400.94\n",
            "BCE: -952376.06\n",
            "KLD: 31.71\n",
            "Loss: 952407.75\n",
            "BCE: -952373.38\n",
            "KLD: 25.32\n",
            "Loss: 952398.69\n",
            "BCE: -952368.69\n",
            "KLD: 17.68\n",
            "Loss: 952386.38\n",
            "BCE: -952365.38\n",
            "KLD: 21.34\n",
            "Loss: 952386.69\n",
            "BCE: -952360.62\n",
            "KLD: 19.94\n",
            "Loss: 952380.56\n",
            "BCE: -952363.50\n",
            "KLD: 13.99\n",
            "Loss: 952377.50\n",
            "BCE: -952358.94\n",
            "KLD: 37.62\n",
            "Loss: 952396.56\n",
            "BCE: -952354.44\n",
            "KLD: 15.60\n",
            "Loss: 952370.06\n",
            "BCE: -952350.81\n",
            "KLD: 22.06\n",
            "Loss: 952372.88\n",
            "BCE: -952349.00\n",
            "KLD: 32.55\n",
            "Loss: 952381.56\n",
            "BCE: -952346.44\n",
            "KLD: 27.06\n",
            "Loss: 952373.50\n",
            "BCE: -952346.00\n",
            "KLD: 23.18\n",
            "Loss: 952369.19\n",
            "BCE: -952341.50\n",
            "KLD: 44.81\n",
            "Loss: 952386.31\n",
            "BCE: -952337.69\n",
            "KLD: 18.16\n",
            "Loss: 952355.88\n",
            "BCE: -952336.69\n",
            "KLD: 38.68\n",
            "Loss: 952375.38\n",
            "BCE: -952333.31\n",
            "KLD: 53.74\n",
            "Loss: 952387.06\n",
            "BCE: -952328.81\n",
            "KLD: 19.10\n",
            "Loss: 952347.94\n",
            "BCE: -742813.62\n",
            "KLD: 32.75\n",
            "Loss: 742846.38\n",
            "====> Epoch: 21 Average loss: 829.8721\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952324.69\n",
            "KLD: 48.79\n",
            "Loss: 952373.50\n",
            "Train Epoch: 22 [0/1139 (0%)]\tLoss: 41807.441406\n",
            "BCE: -952323.44\n",
            "KLD: 24.47\n",
            "Loss: 952347.88\n",
            "BCE: -952319.38\n",
            "KLD: 51.39\n",
            "Loss: 952370.75\n",
            "BCE: -952319.38\n",
            "KLD: 15.13\n",
            "Loss: 952334.50\n",
            "BCE: -952314.44\n",
            "KLD: 38.73\n",
            "Loss: 952353.19\n",
            "BCE: -952311.62\n",
            "KLD: 25.73\n",
            "Loss: 952337.38\n",
            "BCE: -952308.75\n",
            "KLD: 28.98\n",
            "Loss: 952337.75\n",
            "BCE: -952305.31\n",
            "KLD: 15.42\n",
            "Loss: 952320.75\n",
            "BCE: -952304.88\n",
            "KLD: 27.64\n",
            "Loss: 952332.50\n",
            "BCE: -952302.94\n",
            "KLD: 18.99\n",
            "Loss: 952321.94\n",
            "BCE: -952302.00\n",
            "KLD: 24.07\n",
            "Loss: 952326.06\n",
            "BCE: -952298.44\n",
            "KLD: 21.97\n",
            "Loss: 952320.38\n",
            "BCE: -952297.06\n",
            "KLD: 33.92\n",
            "Loss: 952331.00\n",
            "BCE: -952293.88\n",
            "KLD: 27.88\n",
            "Loss: 952321.75\n",
            "BCE: -952293.69\n",
            "KLD: 21.32\n",
            "Loss: 952315.00\n",
            "BCE: -952288.62\n",
            "KLD: 31.57\n",
            "Loss: 952320.19\n",
            "BCE: -952289.31\n",
            "KLD: 274.96\n",
            "Loss: 952564.25\n",
            "BCE: -952287.38\n",
            "KLD: 25.48\n",
            "Loss: 952312.88\n",
            "BCE: -952286.88\n",
            "KLD: 18.35\n",
            "Loss: 952305.25\n",
            "BCE: -952281.62\n",
            "KLD: 24.67\n",
            "Loss: 952306.31\n",
            "BCE: -952279.56\n",
            "KLD: 50.02\n",
            "Loss: 952329.56\n",
            "BCE: -952276.19\n",
            "KLD: 34.11\n",
            "Loss: 952310.31\n",
            "BCE: -742774.19\n",
            "KLD: 15.05\n",
            "Loss: 742789.25\n",
            "====> Epoch: 22 Average loss: 829.8204\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952272.56\n",
            "KLD: 19.69\n",
            "Loss: 952292.25\n",
            "Train Epoch: 23 [0/1139 (0%)]\tLoss: 41803.875000\n",
            "BCE: -952271.00\n",
            "KLD: 26.44\n",
            "Loss: 952297.44\n",
            "BCE: -952268.62\n",
            "KLD: 18.21\n",
            "Loss: 952286.81\n",
            "BCE: -952267.81\n",
            "KLD: 33.91\n",
            "Loss: 952301.75\n",
            "BCE: -952267.25\n",
            "KLD: 40.70\n",
            "Loss: 952307.94\n",
            "BCE: -952263.88\n",
            "KLD: 16.99\n",
            "Loss: 952280.88\n",
            "BCE: -952261.50\n",
            "KLD: 24.06\n",
            "Loss: 952285.56\n",
            "BCE: -952260.50\n",
            "KLD: 43.49\n",
            "Loss: 952304.00\n",
            "BCE: -952257.75\n",
            "KLD: 45.83\n",
            "Loss: 952303.56\n",
            "BCE: -952257.50\n",
            "KLD: 34.05\n",
            "Loss: 952291.56\n",
            "BCE: -952254.75\n",
            "KLD: 48.35\n",
            "Loss: 952303.12\n",
            "BCE: -952253.25\n",
            "KLD: 273.05\n",
            "Loss: 952526.31\n",
            "BCE: -952251.00\n",
            "KLD: 18.43\n",
            "Loss: 952269.44\n",
            "BCE: -952251.06\n",
            "KLD: 28.50\n",
            "Loss: 952279.56\n",
            "BCE: -952248.00\n",
            "KLD: 28.39\n",
            "Loss: 952276.38\n",
            "BCE: -952243.81\n",
            "KLD: 31.02\n",
            "Loss: 952274.81\n",
            "BCE: -952244.06\n",
            "KLD: 21.28\n",
            "Loss: 952265.38\n",
            "BCE: -952242.81\n",
            "KLD: 18.71\n",
            "Loss: 952261.50\n",
            "BCE: -952240.88\n",
            "KLD: 24.80\n",
            "Loss: 952265.69\n",
            "BCE: -952238.38\n",
            "KLD: 19.41\n",
            "Loss: 952257.81\n",
            "BCE: -952238.00\n",
            "KLD: 43.09\n",
            "Loss: 952281.06\n",
            "BCE: -952235.94\n",
            "KLD: 23.67\n",
            "Loss: 952259.62\n",
            "BCE: -742741.81\n",
            "KLD: 16.55\n",
            "Loss: 742758.38\n",
            "====> Epoch: 23 Average loss: 829.7802\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952232.94\n",
            "KLD: 15.51\n",
            "Loss: 952248.44\n",
            "Train Epoch: 24 [0/1139 (0%)]\tLoss: 41801.953125\n",
            "BCE: -952230.44\n",
            "KLD: 30.86\n",
            "Loss: 952261.31\n",
            "BCE: -952229.56\n",
            "KLD: 37.28\n",
            "Loss: 952266.81\n",
            "BCE: -952226.50\n",
            "KLD: 20.04\n",
            "Loss: 952246.56\n",
            "BCE: -952227.38\n",
            "KLD: 277.94\n",
            "Loss: 952505.31\n",
            "BCE: -952223.44\n",
            "KLD: 37.14\n",
            "Loss: 952260.56\n",
            "BCE: -952223.50\n",
            "KLD: 17.42\n",
            "Loss: 952240.94\n",
            "BCE: -952220.88\n",
            "KLD: 17.33\n",
            "Loss: 952238.19\n",
            "BCE: -952220.19\n",
            "KLD: 46.94\n",
            "Loss: 952267.12\n",
            "BCE: -952218.62\n",
            "KLD: 37.64\n",
            "Loss: 952256.25\n",
            "BCE: -952217.00\n",
            "KLD: 23.74\n",
            "Loss: 952240.75\n",
            "BCE: -952214.75\n",
            "KLD: 24.47\n",
            "Loss: 952239.19\n",
            "BCE: -952214.06\n",
            "KLD: 36.51\n",
            "Loss: 952250.56\n",
            "BCE: -952212.50\n",
            "KLD: 37.60\n",
            "Loss: 952250.12\n",
            "BCE: -952210.88\n",
            "KLD: 15.39\n",
            "Loss: 952226.25\n",
            "BCE: -952209.81\n",
            "KLD: 18.04\n",
            "Loss: 952227.88\n",
            "BCE: -952208.50\n",
            "KLD: 22.73\n",
            "Loss: 952231.25\n",
            "BCE: -952207.69\n",
            "KLD: 17.44\n",
            "Loss: 952225.12\n",
            "BCE: -952205.94\n",
            "KLD: 25.75\n",
            "Loss: 952231.69\n",
            "BCE: -952205.12\n",
            "KLD: 34.21\n",
            "Loss: 952239.31\n",
            "BCE: -952202.94\n",
            "KLD: 24.73\n",
            "Loss: 952227.69\n",
            "BCE: -952201.50\n",
            "KLD: 29.98\n",
            "Loss: 952231.50\n",
            "BCE: -742717.50\n",
            "KLD: 49.95\n",
            "Loss: 742767.44\n",
            "====> Epoch: 24 Average loss: 829.7473\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952199.75\n",
            "KLD: 29.77\n",
            "Loss: 952229.50\n",
            "Train Epoch: 25 [0/1139 (0%)]\tLoss: 41801.121094\n",
            "BCE: -952197.50\n",
            "KLD: 36.17\n",
            "Loss: 952233.69\n",
            "BCE: -952196.38\n",
            "KLD: 30.24\n",
            "Loss: 952226.62\n",
            "BCE: -952195.12\n",
            "KLD: 28.16\n",
            "Loss: 952223.31\n",
            "BCE: -952193.75\n",
            "KLD: 16.73\n",
            "Loss: 952210.50\n",
            "BCE: -952193.12\n",
            "KLD: 45.56\n",
            "Loss: 952238.69\n",
            "BCE: -952191.12\n",
            "KLD: 18.15\n",
            "Loss: 952209.25\n",
            "BCE: -952190.81\n",
            "KLD: 33.05\n",
            "Loss: 952223.88\n",
            "BCE: -952188.81\n",
            "KLD: 21.55\n",
            "Loss: 952210.38\n",
            "BCE: -952187.88\n",
            "KLD: 19.30\n",
            "Loss: 952207.19\n",
            "BCE: -952186.94\n",
            "KLD: 28.60\n",
            "Loss: 952215.56\n",
            "BCE: -952186.00\n",
            "KLD: 17.61\n",
            "Loss: 952203.62\n",
            "BCE: -952184.00\n",
            "KLD: 28.01\n",
            "Loss: 952212.00\n",
            "BCE: -952182.69\n",
            "KLD: 20.92\n",
            "Loss: 952203.62\n",
            "BCE: -952182.69\n",
            "KLD: 31.56\n",
            "Loss: 952214.25\n",
            "BCE: -952180.50\n",
            "KLD: 24.48\n",
            "Loss: 952205.00\n",
            "BCE: -952179.56\n",
            "KLD: 28.45\n",
            "Loss: 952208.00\n",
            "BCE: -952178.25\n",
            "KLD: 54.50\n",
            "Loss: 952232.75\n",
            "BCE: -952177.75\n",
            "KLD: 269.58\n",
            "Loss: 952447.31\n",
            "BCE: -952176.50\n",
            "KLD: 52.10\n",
            "Loss: 952228.62\n",
            "BCE: -952175.62\n",
            "KLD: 32.07\n",
            "Loss: 952207.69\n",
            "BCE: -952174.06\n",
            "KLD: 15.75\n",
            "Loss: 952189.81\n",
            "BCE: -742695.38\n",
            "KLD: 16.32\n",
            "Loss: 742711.69\n",
            "====> Epoch: 25 Average loss: 829.7213\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952171.62\n",
            "KLD: 26.10\n",
            "Loss: 952197.75\n",
            "Train Epoch: 26 [0/1139 (0%)]\tLoss: 41799.726562\n",
            "BCE: -952170.75\n",
            "KLD: 25.12\n",
            "Loss: 952195.88\n",
            "BCE: -952169.88\n",
            "KLD: 49.19\n",
            "Loss: 952219.06\n",
            "BCE: -952169.31\n",
            "KLD: 17.51\n",
            "Loss: 952186.81\n",
            "BCE: -952167.69\n",
            "KLD: 19.80\n",
            "Loss: 952187.50\n",
            "BCE: -952166.12\n",
            "KLD: 54.28\n",
            "Loss: 952220.38\n",
            "BCE: -952165.81\n",
            "KLD: 25.14\n",
            "Loss: 952190.94\n",
            "BCE: -952164.88\n",
            "KLD: 16.25\n",
            "Loss: 952181.12\n",
            "BCE: -952163.69\n",
            "KLD: 38.47\n",
            "Loss: 952202.19\n",
            "BCE: -952162.38\n",
            "KLD: 28.77\n",
            "Loss: 952191.12\n",
            "BCE: -952161.94\n",
            "KLD: 30.13\n",
            "Loss: 952192.06\n",
            "BCE: -952160.62\n",
            "KLD: 28.64\n",
            "Loss: 952189.25\n",
            "BCE: -952159.88\n",
            "KLD: 18.57\n",
            "Loss: 952178.44\n",
            "BCE: -952158.88\n",
            "KLD: 14.84\n",
            "Loss: 952173.69\n",
            "BCE: -952158.12\n",
            "KLD: 17.98\n",
            "Loss: 952176.12\n",
            "BCE: -952156.88\n",
            "KLD: 32.88\n",
            "Loss: 952189.75\n",
            "BCE: -952156.00\n",
            "KLD: 24.44\n",
            "Loss: 952180.44\n",
            "BCE: -952155.19\n",
            "KLD: 39.79\n",
            "Loss: 952195.00\n",
            "BCE: -952154.00\n",
            "KLD: 29.14\n",
            "Loss: 952183.12\n",
            "BCE: -952152.94\n",
            "KLD: 24.14\n",
            "Loss: 952177.06\n",
            "BCE: -952152.50\n",
            "KLD: 285.76\n",
            "Loss: 952438.25\n",
            "BCE: -952151.31\n",
            "KLD: 37.90\n",
            "Loss: 952189.19\n",
            "BCE: -742677.62\n",
            "KLD: 13.79\n",
            "Loss: 742691.44\n",
            "====> Epoch: 26 Average loss: 829.6996\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952149.38\n",
            "KLD: 20.57\n",
            "Loss: 952169.94\n",
            "Train Epoch: 27 [0/1139 (0%)]\tLoss: 41798.507812\n",
            "BCE: -952148.06\n",
            "KLD: 284.86\n",
            "Loss: 952432.94\n",
            "BCE: -952147.81\n",
            "KLD: 24.88\n",
            "Loss: 952172.69\n",
            "BCE: -952146.62\n",
            "KLD: 56.24\n",
            "Loss: 952202.88\n",
            "BCE: -952145.69\n",
            "KLD: 34.44\n",
            "Loss: 952180.12\n",
            "BCE: -952144.69\n",
            "KLD: 36.48\n",
            "Loss: 952181.19\n",
            "BCE: -952144.00\n",
            "KLD: 29.91\n",
            "Loss: 952173.94\n",
            "BCE: -952143.00\n",
            "KLD: 29.48\n",
            "Loss: 952172.50\n",
            "BCE: -952142.44\n",
            "KLD: 20.01\n",
            "Loss: 952162.44\n",
            "BCE: -952141.62\n",
            "KLD: 18.85\n",
            "Loss: 952160.50\n",
            "BCE: -952140.88\n",
            "KLD: 25.46\n",
            "Loss: 952166.31\n",
            "BCE: -952140.00\n",
            "KLD: 46.38\n",
            "Loss: 952186.38\n",
            "BCE: -952139.00\n",
            "KLD: 17.52\n",
            "Loss: 952156.50\n",
            "BCE: -952138.19\n",
            "KLD: 32.68\n",
            "Loss: 952170.88\n",
            "BCE: -952137.25\n",
            "KLD: 15.43\n",
            "Loss: 952152.69\n",
            "BCE: -952136.69\n",
            "KLD: 47.64\n",
            "Loss: 952184.31\n",
            "BCE: -952136.00\n",
            "KLD: 28.54\n",
            "Loss: 952164.56\n",
            "BCE: -952135.25\n",
            "KLD: 17.72\n",
            "Loss: 952153.00\n",
            "BCE: -952133.88\n",
            "KLD: 23.12\n",
            "Loss: 952157.00\n",
            "BCE: -952133.38\n",
            "KLD: 28.86\n",
            "Loss: 952162.25\n",
            "BCE: -952133.00\n",
            "KLD: 21.45\n",
            "Loss: 952154.44\n",
            "BCE: -952132.12\n",
            "KLD: 22.15\n",
            "Loss: 952154.25\n",
            "BCE: -742662.44\n",
            "KLD: 15.97\n",
            "Loss: 742678.44\n",
            "====> Epoch: 27 Average loss: 829.6814\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952130.44\n",
            "KLD: 32.75\n",
            "Loss: 952163.19\n",
            "Train Epoch: 28 [0/1139 (0%)]\tLoss: 41798.210938\n",
            "BCE: -952129.25\n",
            "KLD: 22.47\n",
            "Loss: 952151.75\n",
            "BCE: -952128.88\n",
            "KLD: 32.02\n",
            "Loss: 952160.88\n",
            "BCE: -952128.19\n",
            "KLD: 16.86\n",
            "Loss: 952145.06\n",
            "BCE: -952127.50\n",
            "KLD: 17.74\n",
            "Loss: 952145.25\n",
            "BCE: -952126.75\n",
            "KLD: 26.38\n",
            "Loss: 952153.12\n",
            "BCE: -952126.25\n",
            "KLD: 20.80\n",
            "Loss: 952147.06\n",
            "BCE: -952125.31\n",
            "KLD: 27.12\n",
            "Loss: 952152.44\n",
            "BCE: -952124.62\n",
            "KLD: 23.67\n",
            "Loss: 952148.31\n",
            "BCE: -952123.81\n",
            "KLD: 38.70\n",
            "Loss: 952162.50\n",
            "BCE: -952123.25\n",
            "KLD: 26.51\n",
            "Loss: 952149.75\n",
            "BCE: -952122.56\n",
            "KLD: 22.44\n",
            "Loss: 952145.00\n",
            "BCE: -952121.94\n",
            "KLD: 48.01\n",
            "Loss: 952169.94\n",
            "BCE: -952121.19\n",
            "KLD: 41.48\n",
            "Loss: 952162.69\n",
            "BCE: -952120.31\n",
            "KLD: 16.23\n",
            "Loss: 952136.56\n",
            "BCE: -952119.50\n",
            "KLD: 22.86\n",
            "Loss: 952142.38\n",
            "BCE: -952119.25\n",
            "KLD: 273.68\n",
            "Loss: 952392.94\n",
            "BCE: -952118.19\n",
            "KLD: 37.07\n",
            "Loss: 952155.25\n",
            "BCE: -952117.81\n",
            "KLD: 49.29\n",
            "Loss: 952167.12\n",
            "BCE: -952117.06\n",
            "KLD: 32.68\n",
            "Loss: 952149.75\n",
            "BCE: -952116.50\n",
            "KLD: 29.76\n",
            "Loss: 952146.25\n",
            "BCE: -952115.88\n",
            "KLD: 19.44\n",
            "Loss: 952135.31\n",
            "BCE: -742649.94\n",
            "KLD: 20.64\n",
            "Loss: 742670.56\n",
            "====> Epoch: 28 Average loss: 829.6662\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952114.75\n",
            "KLD: 285.69\n",
            "Loss: 952400.44\n",
            "Train Epoch: 29 [0/1139 (0%)]\tLoss: 41808.625000\n",
            "BCE: -952113.94\n",
            "KLD: 28.02\n",
            "Loss: 952141.94\n",
            "BCE: -952112.94\n",
            "KLD: 22.09\n",
            "Loss: 952135.00\n",
            "BCE: -952112.69\n",
            "KLD: 49.29\n",
            "Loss: 952162.00\n",
            "BCE: -952111.88\n",
            "KLD: 25.38\n",
            "Loss: 952137.25\n",
            "BCE: -952111.38\n",
            "KLD: 28.35\n",
            "Loss: 952139.75\n",
            "BCE: -952110.88\n",
            "KLD: 22.16\n",
            "Loss: 952133.06\n",
            "BCE: -952110.38\n",
            "KLD: 32.77\n",
            "Loss: 952143.12\n",
            "BCE: -952109.56\n",
            "KLD: 16.25\n",
            "Loss: 952125.81\n",
            "BCE: -952108.94\n",
            "KLD: 21.56\n",
            "Loss: 952130.50\n",
            "BCE: -952108.25\n",
            "KLD: 20.97\n",
            "Loss: 952129.25\n",
            "BCE: -952107.81\n",
            "KLD: 35.28\n",
            "Loss: 952143.12\n",
            "BCE: -952107.25\n",
            "KLD: 19.87\n",
            "Loss: 952127.12\n",
            "BCE: -952106.81\n",
            "KLD: 38.22\n",
            "Loss: 952145.06\n",
            "BCE: -952105.94\n",
            "KLD: 56.71\n",
            "Loss: 952162.62\n",
            "BCE: -952105.38\n",
            "KLD: 19.08\n",
            "Loss: 952124.44\n",
            "BCE: -952104.81\n",
            "KLD: 40.50\n",
            "Loss: 952145.31\n",
            "BCE: -952104.19\n",
            "KLD: 21.46\n",
            "Loss: 952125.62\n",
            "BCE: -952103.69\n",
            "KLD: 18.51\n",
            "Loss: 952122.19\n",
            "BCE: -952103.31\n",
            "KLD: 20.79\n",
            "Loss: 952124.12\n",
            "BCE: -952102.62\n",
            "KLD: 19.38\n",
            "Loss: 952122.00\n",
            "BCE: -952102.19\n",
            "KLD: 21.70\n",
            "Loss: 952123.88\n",
            "BCE: -742639.44\n",
            "KLD: 34.60\n",
            "Loss: 742674.06\n",
            "====> Epoch: 29 Average loss: 829.6532\n",
            "updating encoder variables\n",
            "updating decoder variables\n",
            "BCE: -952100.75\n",
            "KLD: 18.86\n",
            "Loss: 952119.62\n",
            "Train Epoch: 30 [0/1139 (0%)]\tLoss: 41796.296875\n",
            "BCE: -952100.50\n",
            "KLD: 37.35\n",
            "Loss: 952137.88\n",
            "BCE: -952099.88\n",
            "KLD: 29.18\n",
            "Loss: 952129.06\n",
            "BCE: -952099.31\n",
            "KLD: 33.44\n",
            "Loss: 952132.75\n",
            "BCE: -952098.81\n",
            "KLD: 35.72\n",
            "Loss: 952134.50\n",
            "BCE: -952098.12\n",
            "KLD: 16.30\n",
            "Loss: 952114.44\n",
            "BCE: -952097.75\n",
            "KLD: 23.51\n",
            "Loss: 952121.25\n",
            "BCE: -952097.25\n",
            "KLD: 40.74\n",
            "Loss: 952138.00\n",
            "BCE: -952096.88\n",
            "KLD: 20.46\n",
            "Loss: 952117.31\n",
            "BCE: -952096.25\n",
            "KLD: 29.15\n",
            "Loss: 952125.38\n",
            "BCE: -952095.81\n",
            "KLD: 30.50\n",
            "Loss: 952126.31\n",
            "BCE: -952095.31\n",
            "KLD: 290.75\n",
            "Loss: 952386.06\n",
            "BCE: -952094.75\n",
            "KLD: 73.96\n",
            "Loss: 952168.69\n",
            "BCE: -952094.12\n",
            "KLD: 30.20\n",
            "Loss: 952124.31\n",
            "BCE: -952093.69\n",
            "KLD: 24.86\n",
            "Loss: 952118.56\n",
            "BCE: -952093.19\n",
            "KLD: 19.73\n",
            "Loss: 952112.94\n",
            "BCE: -952092.62\n",
            "KLD: 14.88\n",
            "Loss: 952107.50\n",
            "BCE: -952092.31\n",
            "KLD: 19.71\n",
            "Loss: 952112.00\n",
            "BCE: -952091.88\n",
            "KLD: 20.41\n",
            "Loss: 952112.31\n",
            "BCE: -952091.38\n",
            "KLD: 21.40\n",
            "Loss: 952112.75\n",
            "BCE: -952090.88\n",
            "KLD: 24.10\n",
            "Loss: 952115.00\n",
            "BCE: -952090.50\n",
            "KLD: 26.37\n",
            "Loss: 952116.88\n",
            "BCE: -742630.44\n",
            "KLD: 17.06\n",
            "Loss: 742647.50\n",
            "====> Epoch: 30 Average loss: 829.6424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GyQ3-xkrR1XX",
        "colab_type": "code",
        "outputId": "5718603b-8521-4b2d-ecd2-a18fd1c7cfe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "model.beta\n",
        "_, ind = torch.sort(model.beta, 0)\n",
        "# ind.numpy()[0:50, 0] - ind.numpy()[0:50, 1]\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:25, 0])\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:25, 1])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 2])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 3])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 4])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 5])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 7])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 8])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 9])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 15])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 19])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['present' 'background' 'ability' 'bms' 'car' 'letters' 'hardware'\n",
            " 'version' 'try' 'handheld' 'manufacture' 'governments' 'sure' 'offered'\n",
            " 'planning' 'scott' 'security' 'function' 'escaped' 'bob' 'apparently'\n",
            " 'reading' 'editor' 'inside' 'lets']\n",
            "['minutes' 'utzoo' 'earlier' 'view' 'tactics' 'relatively' 'body'\n",
            " 'pressure' 'site' 'helps' 'student' 'relative' 'conducted' 'twist'\n",
            " 'mission' 'account' 'nation' 'earth' 'consideration' 'desk' 'urbana'\n",
            " 'sex' 'wouldn' 'need' 'mention']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KeltSp1qJJNH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Stuff"
      ]
    },
    {
      "metadata": {
        "id": "-X7Z51K4EDI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "90625fd8-2b87-4c67-a873-386cf05edd91"
      },
      "cell_type": "code",
      "source": [
        "model.beta"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0554e-03, 3.9097e-13],\n",
              "        [6.3868e-16, 6.6957e-04],\n",
              "        [1.9011e-26, 6.6957e-04],\n",
              "        ...,\n",
              "        [1.0554e-03, 1.0889e-09],\n",
              "        [4.1879e-09, 6.6956e-04],\n",
              "        [1.0554e-03, 1.9359e-16]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "-geZP7XKtadI",
        "colab_type": "code",
        "outputId": "c380c1d8-879f-4c9c-d9e6-9b968b766e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# a = torch.randn(100, 128)\n",
        "a = torch.tensor([[1,2,3], [4,5,6]]).float()\n",
        "b = torch.tensor([[1,2,3], [4,5,6]]).float()\n",
        "\n",
        "F.cosine_similarity(a, b)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "1n0TXZhZG9Er",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get topic distributions"
      ]
    },
    {
      "metadata": {
        "id": "EIvkM_22DhY-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unscaled_topics = torch.mm(model.word_embedding(torch.tensor(np.arange(doc_term_matrix.shape[1]))),\n",
        "#          torch.transpose(model.topicslayer.weight, 0, 1))\n",
        "# topic_dist = torch.softmax(unscaled_topics, dim = 0)\n",
        "# topic_dist.sum(dim = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bR5eDznAHONV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This one helped us a lot"
      ]
    },
    {
      "metadata": {
        "id": "z16IGTridRPf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model.encode(torch.LongTensor(doc_term_matrix[0]))\n",
        "#input = torch.tensor(doc_term_matrix).float()\n",
        "input = torch.tensor(doc_term_matrix).float()[[0, 1], ]\n",
        "mu, sigma = model.encode(input)\n",
        "z = model.reparameterize(mu, sigma)\n",
        "# model.decode(x, input.shape[0])\n",
        "\n",
        "x = model.fc3(z)\n",
        "theta = F.softmax(x) # to get theta\n",
        "embedding_matrix = model.word_embedding(torch.tensor(np.arange(14)))\n",
        "word_dot_topic = model.fc4(embedding_matrix) # weights corresp to topic vector\n",
        "beta = F.softmax(word_dot_topic)\n",
        "log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(beta, 0, 1)))\n",
        "#theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "# print(theta.shape)\n",
        "# print(theta)\n",
        "# print(embedding_matrix)\n",
        "# print(word_dot_topic)\n",
        "print(beta.shape)\n",
        "print(beta)\n",
        "print(log_theta_dot_beta)\n",
        "print(torch.exp(log_theta_dot_beta_normalized))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YPbnTiNpCcaO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model.encode(torch.LongTensor(doc_term_matrix[0]))\n",
        "#input = torch.tensor(doc_term_matrix).float()\n",
        "input = torch.tensor(doc_term_matrix).float()[[0, 1], ]\n",
        "print(input)\n",
        "mu, sigma = model.encode(input)\n",
        "z = model.reparameterize(mu, sigma)\n",
        "print(z)\n",
        "# model.decode(x, input.shape[0])\n",
        "\n",
        "\n",
        "# x = model.lin2(z)\n",
        "# theta = F.softmax(x) # to get theta\n",
        "# embedding_matrix = model.word_embedding(torch.tensor(np.arange(model.num_docs)))\n",
        "# word_dot_topic = model.topicslayer(embedding_matrix) # weights corresp to topic vector\n",
        "# model.beta = F.softmax(word_dot_topic, dim = 0)\n",
        "# log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(model.beta, 0, 1)))\n",
        "# #theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "# log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "# print(embedding_matrix.shape) # dim of embedding matrix is 1544 x 100\n",
        "\n",
        "\n",
        "x = model.lin2(z)\n",
        "theta = F.softmax(x, 1) # to get theta\n",
        "print(theta.sum(1))\n",
        "embedding_matrix = model.word_embedding.weight\n",
        "print(model.word_embedding(torch.tensor(np.arange(model.num_docs))).shape)\n",
        "print(embedding_matrix.shape)\n",
        "word_dot_topic = model.topicslayer(embedding_matrix) # weights corresp to topic vector\n",
        "model.beta = F.softmax(word_dot_topic, dim = 0)\n",
        "log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(model.beta, 0, 1)))\n",
        "#theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "# print(embedding_matrix.shape) # dim of embedding matrix is still 1544 x 100\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gqevKMN6pdEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d4662534-81cf-4f6a-c59b-1c38f2ff9cd4"
      },
      "cell_type": "code",
      "source": [
        "###########################\n",
        "###########################\n",
        "###########################\n",
        "\n",
        "test_input = torch.tensor(doc_term_matrix).float()[[0, 1], ] # pretend_batch_size = 2\n",
        "# print(test_input.shape) # 2 x 2441, where 2441 is vocab size\n",
        "mu, logvar = model.encode(test_input)\n",
        "# print(mu.shape) # 2 x 50\n",
        "# print(sigma.shape) # 2 x 50\n",
        "z = model.reparameterize(mu, sigma) # 2 x 50\n",
        "# print(z.shape) # 2 x 50\n",
        "output = model.decode(z)\n",
        "# print(output)\n",
        "# print(output.shape)\n",
        "\n",
        "pretend_num_docs = 50\n",
        "pretend_batch_size = test_input.shape[0]\n",
        "#print(output.sum())\n",
        "BCE = pretend_num_docs * 1.0 / pretend_batch_size * output.sum() # we sum the log probabilities\n",
        "# print(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "KLD0 = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "print(logvar)\n",
        "#KLD1 = -0.5 * torch.sum(2 + torch.sum(torch.cumprod(logvar)) - torch.mm(torch.transpose(mu, 1, 0),mu) - logvar.exp()) # this is a number, should be \n",
        "############################################################################################################\n",
        "#KLD2 = 0.5 * (torch.sum(logvar.exp()) + torch.dot(mu, mu) - 50 - torch.log(torch.cumprod(logvar))) ###########################\n",
        "# print(float(BCE))\n",
        "\n",
        "# print(\"BCE: \" + \"{:.2f}\".format(float(BCE)))\n",
        "# print(\"KLD: \" + \"{:.2f}\".format(float(KLD)))\n",
        "# print(\"Loss: \" + \"{:.2f}\".format(float(- BCE + KLD)))\n",
        "# return - BCE + KLD # - .1 * (zeta - nu)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0176,  0.0258,  0.2086, -0.0757, -0.1349,  0.1515,  0.0422,  0.1266,\n",
            "          0.1552,  0.0714, -0.1026,  0.0122, -0.0432, -0.0601, -0.0545, -0.0498,\n",
            "          0.0034, -0.1475, -0.1544, -0.0403, -0.0020,  0.0638, -0.1058,  0.0358,\n",
            "         -0.1366, -0.1131,  0.0661, -0.0645,  0.0669,  0.0143,  0.0352,  0.1203,\n",
            "         -0.0325, -0.0359,  0.1152,  0.0315, -0.1065,  0.0167,  0.0462, -0.0026,\n",
            "         -0.0827,  0.0001, -0.0119,  0.0560,  0.1036, -0.0483, -0.0862,  0.0005,\n",
            "          0.0187,  0.0275],\n",
            "        [-0.0438,  0.0082,  0.0904, -0.0102, -0.0217, -0.0268,  0.0012, -0.0640,\n",
            "          0.0917,  0.0690, -0.0300, -0.0058,  0.0595, -0.0108, -0.0603, -0.0158,\n",
            "          0.0698, -0.0569, -0.0790,  0.0202,  0.0228,  0.0589, -0.0955,  0.0672,\n",
            "         -0.0046, -0.0454, -0.0373, -0.0053,  0.0295, -0.0219, -0.0386, -0.0106,\n",
            "         -0.0526,  0.0714,  0.0268,  0.0146,  0.0012, -0.0256,  0.0176,  0.0066,\n",
            "         -0.0199,  0.1099,  0.0021,  0.0288,  0.0588,  0.0684, -0.0177,  0.0291,\n",
            "          0.0556, -0.0923]], grad_fn=<ThAddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J0K5OnFvuqcG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b6aa5c32-f494-4018-94f6-b0540aee4d6b"
      },
      "cell_type": "code",
      "source": [
        "# X'X\n",
        "torch.mm(torch.transpose(torch.tensor([[1, 2], [3, 4]]), 1, 0),torch.tensor([[1, 2], [3, 4]]))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10, 14],\n",
              "        [14, 20]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "oFEeCibxJFLQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Topic Coherence"
      ]
    },
    {
      "metadata": {
        "id": "Lh2eLudHDncY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def topic_coherence(beta, M, doc_term_matrix):\n",
        "  K = beta.shape[1] # beta has dim V x K\n",
        "  coherences = np.zeros(K)\n",
        "  for t in range(K):\n",
        "    index = np.argsort(-beta[:, t])[0:M]\n",
        "    cart_prod = product(list(index), list(index))\n",
        "    for ind1, ind2 in cart_prod:\n",
        "      if ind1 == ind2:\n",
        "        pass\n",
        "      else:\n",
        "        d_ind1 = (doc_term_matrix[:, ind1] > 0).sum()\n",
        "        d_ind12 = ((doc_term_matrix[:, ind1] > 0) & (doc_term_matrix[:, ind2] > 0)).sum()\n",
        "        coherences[t] += np.log1p(d_ind12) - np.log(d_ind1)\n",
        "\n",
        "  return coherences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LKM9IxgsLWKT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a3fbd8e-9f89-4788-d527-20250794a4d8"
      },
      "cell_type": "code",
      "source": [
        "topic_coherence(model.beta.detach().numpy(), 20, doc_term_matrix)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1002.65439037,  -970.85404501])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "7BuiupBeNYqZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}