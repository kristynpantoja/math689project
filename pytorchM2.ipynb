{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorchM2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "FrATqONaiDBo",
        "KeltSp1qJJNH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristynpantoja/math689project/blob/master/pytorchM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Yzom68CRUCUt",
        "outputId": "7b677247-bda3-4e08-bc22-a144c95bdfb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "L382anKLUwuE",
        "outputId": "0b417eeb-46fe-4751-b91d-2747007ec49d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 3.4MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pJ2ybE0_FuhI"
      },
      "cell_type": "markdown",
      "source": [
        "Model Parameters"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Y96bktYniWeB",
        "outputId": "e28b9ba3-4594-4ab3-ad0e-d81cf44557c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.6MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 11.0MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/d9/5c892a943fede7db0c0d3bd653570208ffb3c163bc57c1a9354acd65c921/boto3-1.9.42-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 17.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 16.1MB/s \n",
            "\u001b[?25hCollecting botocore<1.13.0,>=1.12.42 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/ad/706263fda4a8c673fd58c1cf03160dfdcf093d6614130193d3ce12a81fad/botocore-1.12.42-py2.py3-none-any.whl (4.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.8MB 6.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting docutils>=0.10 (from botocore<1.13.0,>=1.12.42->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 24.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.42->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Building wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.42 botocore-1.12.42 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f5gixJ2PhDZy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "# from torchvision import datasets, transforms\n",
        "# from torchvision.utils import save_image\n",
        "\n",
        "from types import SimpleNamespace\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-D3sQR-gF0Vk"
      },
      "cell_type": "markdown",
      "source": [
        "Data: 20newsgroups\n",
        "We get the document-term matrix"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TNUmE8UMGRMh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# categories = ['talk.politics.guns', 'sci.space', 'soc.religion.christian',\n",
        "#               'misc.forsale', 'rec.sport.baseball', 'comp.sys.mac.hardware']\n",
        "categories = ['talk.politics.guns', 'sci.space']\n",
        "# newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "vectorizer = CountVectorizer(stop_words = 'english', min_df=.01, max_df=0.9, \n",
        "                             token_pattern = u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
        "count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
        "doc_term_matrix = count_vecs.toarray()\n",
        "doc_term_matrix.shape # number of documents, number of words (in vocab)\n",
        "tokenizer = vectorizer.build_tokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XtIb9CFS59LP",
        "outputId": "46f516e1-5494-4699-9b89-308878c4d1e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "doc_term_matrix.shape # vocab size"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1139, 2441)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ilQe6iCsZ9VO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oft5ScQFZyEw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dict_word_freq = dict(zip(vectorizer.get_feature_names(), list(doc_term_matrix.sum(0))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ECE01K5OF9r0"
      },
      "cell_type": "markdown",
      "source": [
        "ragged array of words in each document (by index in vocabulary)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NKDN2_x45V44",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def countsToInput(row):\n",
        "#   return np.repeat(np.arange(doc_term_matrix.shape[1]),row)\n",
        "  \n",
        "# def numWords(row):\n",
        "#   return row.sum()\n",
        "\n",
        "# N_train = np.apply_along_axis(numWords, axis=1, arr=doc_term_matrix)\n",
        "# data_train = []\n",
        "# for d in range(doc_term_matrix.shape[0]):\n",
        "#   data_train.append(torch.from_numpy(countsToInput(doc_term_matrix[d])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "FrATqONaiDBo"
      },
      "cell_type": "markdown",
      "source": [
        "## Word2vec"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aLXYF7P7xSEn",
        "outputId": "42d5e689-d30e-4033-b7cc-580066da3576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# ideally, we would do some preprocessing\n",
        "newsgroups_train_preproc = []\n",
        "for document in newsgroups_train.data:\n",
        "    newsgroups_train_preproc.append(document.split())\n",
        "\n",
        "# make the model\n",
        "w2v = Word2Vec(sg=1, negative=5, size=100, window=10, min_count=1, max_vocab_size=None, max_final_vocab=None)\n",
        "# w2v.build_vocab(newsgroups_train_preproc)\n",
        "w2v.build_vocab_from_freq(word_freq = dict_word_freq)\n",
        "# train the model\n",
        "w2v.train(sentences=newsgroups_train_preproc,epochs=10, total_examples=doc_term_matrix.shape[1])\n",
        "# save the model\n",
        "# w2v.save(\"sg_1_M2\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(625260, 3791210)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-nNkaKgu7Dsp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(set([item for sublist in newsgroups_train_preproc for item in sublist]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2EBD-baJ70bC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  torch.tensor(w2v.syn1neg).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eTmRNeXU6Od7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(w2v.wv.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LcpyWmcDGrjR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc_term_matrix.shape[1] == len(w2v.wv.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DTX4Rsqf4IVr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w2v.syn1neg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Qe0oJvkE21H3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w2v.wv.most_similar(\"university\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8ASbgnVqGDvU"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup for Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HoGhPn8n4Gok",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from types import SimpleNamespace\n",
        "# args_dict = {\"batch_size\" : 50, \"epochs\" : 50, \"no_cuda\" : False, \"seed\" : 1, \"log_interval\" : 10}\n",
        "# args = SimpleNamespace(**args_dict)\n",
        "# args.epochs\n",
        "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "args_dict = {\"batch_size\" : 50, \"epochs\" : 30, \"no_cuda\" : False, \"seed\" : 1, \"log_interval\" : 100}\n",
        "args = SimpleNamespace(**args_dict)\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=True, download=True,\n",
        "#                    transform=transforms.ToTensor()),\n",
        "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6E6F6rVgGXnS"
      },
      "cell_type": "markdown",
      "source": [
        "Define model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vwZfWV16T97w",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, vocab_size, num_docs, wordvec_dim, encoder_hidden, rp_normal_dim, num_samples, K):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_samples = num_samples\n",
        "        self.num_docs = num_docs\n",
        "        \n",
        "        self.word_embedding = nn.Embedding(vocab_size, wordvec_dim) # decoder\n",
        "#         self.word_embedding = nn.Embedding.from_pretrained(torch.tensor(w2v.syn1neg), freeze=True)\n",
        "        self.topic_embedding = nn.Embedding(K, wordvec_dim) # decoder\n",
        "        self.lin1 = nn.Linear(vocab_size, encoder_hidden) # encoder\n",
        "        self.mean = nn.Linear(encoder_hidden, rp_normal_dim) # encoder\n",
        "#         self.mean_bn = nn.BatchNorm1d(rp_normal_dim)\n",
        "        self.logvar = nn.Linear(encoder_hidden, rp_normal_dim) # encoder\n",
        "#         self.logvar_bn = nn.BatchNorm1d(rp_normal_dim)\n",
        "        self.lin2 = nn.Linear(rp_normal_dim, K) # decoder \n",
        "        self.dropout = nn.Dropout(p=0.8)\n",
        "        \n",
        "        self.beta = torch.zeros([K, vocab_size], dtype = torch.float32) # decoder\n",
        "#         self.theta = torch.zeros([10, K], dtype = torch.float32)\n",
        "\n",
        "        ###\n",
        "#         self.decoder = nn.Linear(K, vocab_size) ### vocab_size == 1995 in the paper\n",
        "#         self.decoder_bn = nn.BatchNorm1d(vocab_size) \n",
        "        ###\n",
        "\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.lin1(x))\n",
        "        h2 = self.dropout(h1)\n",
        "#         return self.mean_bn(self.mean(h2)), self.mean_bn(self.logvar(h2))\n",
        "        return self.mean(h2), self.logvar(h2)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar) # get sigma\n",
        "        eps = torch.randn_like(std) # get epsilon, generated from N(0, I_k) where k is dimension of std: k x 1\n",
        "        return eps.mul(std).add_(mu) # this gives x ~ N(mu, var)\n",
        "        # note: .mul is element-wise multiplication - this is fine, since sigma is diagonal matrix\n",
        "\n",
        "      \n",
        "    def decode(self, z, docs):\n",
        "        x = self.lin2(z) \n",
        "        theta = F.softmax(x, dim = 1) # to get theta, dim = batch size x K\n",
        "        \n",
        "        ###\n",
        "#         recon = F.softmax(self.decoder_bn(self.decoder(theta))) # batch size x V\n",
        "#         return recon\n",
        "        ###\n",
        "        \n",
        "        self.beta = F.softmax(self.word_embedding.weight.mm(self.topic_embedding.weight.t()), dim = 0) # beta, dim = V x K\n",
        "#         print(theta)\n",
        "#         print(self.beta)\n",
        "#         print(docs.mm(self.beta).mm(theta.t()).diag())\n",
        "\n",
        "        log_theta_dot_beta = docs.mm(self.beta).mm(theta.t()).diag().log() # dim is B x B\n",
        "        return log_theta_dot_beta.sum()\n",
        "        \n",
        "        \n",
        "    def forward(self, docs, batch_size):\n",
        "        mu, logvar = self.encode(docs)\n",
        "        log_p = torch.zeros(1)\n",
        "        for sample in range(self.num_samples):\n",
        "            z = self.reparameterize(mu, logvar)\n",
        "            decoded = self.decode(z, docs)\n",
        "#             print(\"z \" + str(z.shape))\n",
        "#             print(\"log_p \" + str(log_p.shape))\n",
        "#             print(\"decoded \" + str(decoded.shape))\n",
        "            log_p = log_p.add(decoded)\n",
        "        log_p /= self.num_samples\n",
        "        \n",
        "        return log_p, mu, logvar, self.topic_embedding.weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uoGmoqZjaAHr"
      },
      "cell_type": "markdown",
      "source": [
        "Load training data (separate into batches)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yWSJgB41sicr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = torch.utils.data.TensorDataset(torch.tensor(doc_term_matrix))\n",
        "train_loader = torch.utils.data.DataLoader(train_data,                                            \n",
        "    batch_size = args.batch_size, shuffle = True)\n",
        "#     batch_size = args.batch_size, shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dt0_YBz5aQEI"
      },
      "cell_type": "markdown",
      "source": [
        "instantiate model and define functions for training"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DO5T6YFkCTIN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = VAE(vocab_size = doc_term_matrix.shape[1], \n",
        "            num_docs = doc_term_matrix.shape[0], \n",
        "            wordvec_dim = 100, \n",
        "            encoder_hidden = 256, \n",
        "            rp_normal_dim = 75, \n",
        "            num_samples = 1,\n",
        "            K = 50)#.to(device) \n",
        "      \n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# optimizer = optim.RMSprop(model.parameters(), lr = 1e-3)\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(data, train_loader, log_theta_dot_beta, x, mu, logvar, t):\n",
        "\n",
        "    KLD = (-0.5) * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # KL divergence; it's fine to sum them all up now, rather than for each sample in mini-batch, because they'll all be summed up anyways\n",
        "    \n",
        "    BCE = log_theta_dot_beta.sum()\n",
        "\n",
        "#     K = t.shape[0]\n",
        "#     arccos = []\n",
        "#     for j in range(K):\n",
        "#         for k in range(j, K):\n",
        "#             arccos.append(torch.acos(torch.dot(t[:, j], t[:, k]) /\n",
        "#                                                    (max(t[:, j].norm() * t[:, k].norm(), 1e-5))))\n",
        "#             arccos.append(F.cosine_similarity(t[:, j], t[:, k]))\n",
        "#     arccos = torch.tensor(arccos)\n",
        "#     print(arccos.max())\n",
        "#     zeta = (1 / (K * K)) * arccos.sum()\n",
        "#     nu = torch.zeros(1)\n",
        "#     print(\"zeta: \" + str(zeta) + \"nu: \" + str(nu))\n",
        "#     for a in arccos:\n",
        "#         nu = nu.add((a - zeta).pow(2))\n",
        "#     nu = (1 / (K * K)) * nu\n",
        "\n",
        "#     print(\"BCE: \" + \"{:.2f}\".format(float(BCE)))\n",
        "#     print(\"KLD: \" + \"{:.2f}\".format(float(KLD)))\n",
        "    \n",
        "    return data[0].shape[0] * 1.0 / len(train_loader.dataset) * (-BCE + KLD)\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "enc_variables = list(model.lin1.parameters()) + list(model.mean.parameters()) + list(model.logvar.parameters())\n",
        "dec_variables = list(model.word_embedding.parameters()) + list(model.lin2.parameters()) + list(model.topic_embedding.parameters())\n",
        "\n",
        "optim_enc = optim.Adam(enc_variables, lr=1e-3)\n",
        "optim_dec = optim.Adam(dec_variables, lr=1e-3)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for switch in range(2):\n",
        "        if switch == 0:\n",
        "            print(\"updating encoder variables\")\n",
        "            optimizer = optim_enc\n",
        "        else:\n",
        "            print(\"updating decoder variables\")\n",
        "            optimizer = optim_dec\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "    #         data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            log_theta_beta, mu, logvar, topic_vecs = model(data[0].float(), data[0].shape[0])\n",
        "            loss = loss_function(data, train_loader, log_theta_beta, data, mu, logvar, topic_vecs)\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "            if batch_idx % args.log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * data[0].shape[0], len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "    #                 loss.item() / data[0].shape[0]))\n",
        "                    loss.item() ))\n",
        "\n",
        "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "#               epoch, train_loss / len(train_loader.dataset)))\n",
        "              epoch, train_loss))\n",
        "\n",
        "        return train_loss #/ len(train_loader.dataset)\n",
        "\n",
        "\n",
        "# def test(epoch):\n",
        "#     model.eval()\n",
        "#     test_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for i, (data, _) in enumerate(test_loader):\n",
        "#             data = data.to(device)\n",
        "#             recon_batch, mu, logvar = model(data)\n",
        "#             test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "#             if i == 0:\n",
        "#                 n = min(data.size(0), 8)\n",
        "#                 comparison = torch.cat([data[:n],\n",
        "#                                       recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
        "# #                 save_image(comparison.cpu(),\n",
        "# #                          'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "#     test_loss /= len(test_loader.dataset)\n",
        "#     print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     for epoch in range(1, args.epochs + 1):\n",
        "#         train(epoch)\n",
        "#         test(epoch)\n",
        "#         with torch.no_grad():\n",
        "#             sample = torch.randn(64, 20).to(device)\n",
        "#             sample = model.decode(sample).cpu()\n",
        "#             save_image(sample.view(64, 1, 28, 28),\n",
        "#                        'results/sample_' + str(epoch) + '.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zuF9sAfDGjUE"
      },
      "cell_type": "markdown",
      "source": [
        " train the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XY-GsKzb3b_h",
        "outputId": "d0eda440-305d-4662-c473-6a733fd150de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2306
        }
      },
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    losses.append(train(epoch))\n",
        "#     if epoch > 1:\n",
        "#         if np.abs(losses[epoch-1] - losses[epoch-2]) < 1e-2:\n",
        "#             break\n",
        "\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating encoder variables\n",
            "Train Epoch: 1 [0/11314 (0%)]\tLoss: 1.185834\n",
            "Train Epoch: 1 [5000/11314 (44%)]\tLoss: 0.841659\n",
            "Train Epoch: 1 [10000/11314 (88%)]\tLoss: 0.932125\n",
            "====> Epoch: 1 Average loss: 30359882.8854\n",
            "updating encoder variables\n",
            "Train Epoch: 2 [0/11314 (0%)]\tLoss: 0.887393\n",
            "Train Epoch: 2 [5000/11314 (44%)]\tLoss: 0.854758\n",
            "Train Epoch: 2 [10000/11314 (88%)]\tLoss: 0.919071\n",
            "====> Epoch: 2 Average loss: 33259.6992\n",
            "updating encoder variables\n",
            "Train Epoch: 3 [0/11314 (0%)]\tLoss: 0.898820\n",
            "Train Epoch: 3 [5000/11314 (44%)]\tLoss: 0.860871\n",
            "Train Epoch: 3 [10000/11314 (88%)]\tLoss: 0.799085\n",
            "====> Epoch: 3 Average loss: 230.5585\n",
            "updating encoder variables\n",
            "Train Epoch: 4 [0/11314 (0%)]\tLoss: 0.891209\n",
            "Train Epoch: 4 [5000/11314 (44%)]\tLoss: 0.965291\n",
            "Train Epoch: 4 [10000/11314 (88%)]\tLoss: 1.464685\n",
            "====> Epoch: 4 Average loss: 206.5474\n",
            "updating encoder variables\n",
            "Train Epoch: 5 [0/11314 (0%)]\tLoss: 0.849416\n",
            "Train Epoch: 5 [5000/11314 (44%)]\tLoss: 0.806831\n",
            "Train Epoch: 5 [10000/11314 (88%)]\tLoss: 0.855713\n",
            "====> Epoch: 5 Average loss: 209.5556\n",
            "updating encoder variables\n",
            "Train Epoch: 6 [0/11314 (0%)]\tLoss: 0.908859\n",
            "Train Epoch: 6 [5000/11314 (44%)]\tLoss: 0.771686\n",
            "Train Epoch: 6 [10000/11314 (88%)]\tLoss: 0.938998\n",
            "====> Epoch: 6 Average loss: 245.5726\n",
            "updating encoder variables\n",
            "Train Epoch: 7 [0/11314 (0%)]\tLoss: 0.861091\n",
            "Train Epoch: 7 [5000/11314 (44%)]\tLoss: 0.946353\n",
            "Train Epoch: 7 [10000/11314 (88%)]\tLoss: 0.810966\n",
            "====> Epoch: 7 Average loss: 205.9114\n",
            "updating encoder variables\n",
            "Train Epoch: 8 [0/11314 (0%)]\tLoss: 0.848774\n",
            "Train Epoch: 8 [5000/11314 (44%)]\tLoss: 0.833246\n",
            "Train Epoch: 8 [10000/11314 (88%)]\tLoss: 0.880824\n",
            "====> Epoch: 8 Average loss: 239.3625\n",
            "updating encoder variables\n",
            "Train Epoch: 9 [0/11314 (0%)]\tLoss: 0.860308\n",
            "Train Epoch: 9 [5000/11314 (44%)]\tLoss: 0.804103\n",
            "Train Epoch: 9 [10000/11314 (88%)]\tLoss: 0.818179\n",
            "====> Epoch: 9 Average loss: 203.1200\n",
            "updating encoder variables\n",
            "Train Epoch: 10 [0/11314 (0%)]\tLoss: 0.846252\n",
            "Train Epoch: 10 [5000/11314 (44%)]\tLoss: 0.819146\n",
            "Train Epoch: 10 [10000/11314 (88%)]\tLoss: 0.842950\n",
            "====> Epoch: 10 Average loss: 206.3686\n",
            "updating encoder variables\n",
            "Train Epoch: 11 [0/11314 (0%)]\tLoss: 0.833305\n",
            "Train Epoch: 11 [5000/11314 (44%)]\tLoss: 0.883089\n",
            "Train Epoch: 11 [10000/11314 (88%)]\tLoss: 2.053554\n",
            "====> Epoch: 11 Average loss: 205.4676\n",
            "updating encoder variables\n",
            "Train Epoch: 12 [0/11314 (0%)]\tLoss: 0.821874\n",
            "Train Epoch: 12 [5000/11314 (44%)]\tLoss: 0.856616\n",
            "Train Epoch: 12 [10000/11314 (88%)]\tLoss: 0.858565\n",
            "====> Epoch: 12 Average loss: 204.7387\n",
            "updating encoder variables\n",
            "Train Epoch: 13 [0/11314 (0%)]\tLoss: 0.840902\n",
            "Train Epoch: 13 [5000/11314 (44%)]\tLoss: 0.883316\n",
            "Train Epoch: 13 [10000/11314 (88%)]\tLoss: 0.822289\n",
            "====> Epoch: 13 Average loss: 201.2675\n",
            "updating encoder variables\n",
            "Train Epoch: 14 [0/11314 (0%)]\tLoss: 0.941632\n",
            "Train Epoch: 14 [5000/11314 (44%)]\tLoss: 0.867353\n",
            "Train Epoch: 14 [10000/11314 (88%)]\tLoss: 0.891614\n",
            "====> Epoch: 14 Average loss: 202.9424\n",
            "updating encoder variables\n",
            "Train Epoch: 15 [0/11314 (0%)]\tLoss: 0.873689\n",
            "Train Epoch: 15 [5000/11314 (44%)]\tLoss: 0.863692\n",
            "Train Epoch: 15 [10000/11314 (88%)]\tLoss: 0.905242\n",
            "====> Epoch: 15 Average loss: 202.5294\n",
            "updating encoder variables\n",
            "Train Epoch: 16 [0/11314 (0%)]\tLoss: 0.832808\n",
            "Train Epoch: 16 [5000/11314 (44%)]\tLoss: 0.946365\n",
            "Train Epoch: 16 [10000/11314 (88%)]\tLoss: 0.772636\n",
            "====> Epoch: 16 Average loss: 199.6863\n",
            "updating encoder variables\n",
            "Train Epoch: 17 [0/11314 (0%)]\tLoss: 0.848594\n",
            "Train Epoch: 17 [5000/11314 (44%)]\tLoss: 0.903605\n",
            "Train Epoch: 17 [10000/11314 (88%)]\tLoss: 0.887899\n",
            "====> Epoch: 17 Average loss: 205.5129\n",
            "updating encoder variables\n",
            "Train Epoch: 18 [0/11314 (0%)]\tLoss: 0.888967\n",
            "Train Epoch: 18 [5000/11314 (44%)]\tLoss: 0.795253\n",
            "Train Epoch: 18 [10000/11314 (88%)]\tLoss: 0.913032\n",
            "====> Epoch: 18 Average loss: 200.6640\n",
            "updating encoder variables\n",
            "Train Epoch: 19 [0/11314 (0%)]\tLoss: 0.898484\n",
            "Train Epoch: 19 [5000/11314 (44%)]\tLoss: 0.889237\n",
            "Train Epoch: 19 [10000/11314 (88%)]\tLoss: 0.892480\n",
            "====> Epoch: 19 Average loss: 199.2067\n",
            "updating encoder variables\n",
            "Train Epoch: 20 [0/11314 (0%)]\tLoss: 0.882459\n",
            "Train Epoch: 20 [5000/11314 (44%)]\tLoss: 0.838688\n",
            "Train Epoch: 20 [10000/11314 (88%)]\tLoss: 0.905919\n",
            "====> Epoch: 20 Average loss: 213.5081\n",
            "updating encoder variables\n",
            "Train Epoch: 21 [0/11314 (0%)]\tLoss: 0.894440\n",
            "Train Epoch: 21 [5000/11314 (44%)]\tLoss: 0.923786\n",
            "Train Epoch: 21 [10000/11314 (88%)]\tLoss: 0.908098\n",
            "====> Epoch: 21 Average loss: 203.5471\n",
            "updating encoder variables\n",
            "Train Epoch: 22 [0/11314 (0%)]\tLoss: 0.888255\n",
            "Train Epoch: 22 [5000/11314 (44%)]\tLoss: 0.866009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-957d54685dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     if epoch > 1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#         if np.abs(losses[epoch-1] - losses[epoch-2]) < 1e-2:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-fde4f1002be1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Np_Ow5NG7ZIH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "associations = {\n",
        "    'jesus': ['prophet', 'jesus', 'matthew', 'christ', 'worship', 'church'],\n",
        "    'comp ': ['floppy', 'windows', 'microsoft', 'monitor', 'workstation', 'macintosh', \n",
        "              'printer', 'programmer', 'colormap', 'scsi', 'jpeg', 'compression'],\n",
        "    'car  ': ['wheel', 'tire'],\n",
        "    'polit': ['amendment', 'libert', 'regulation', 'president'],\n",
        "    'crime': ['violent', 'homicide', 'rape'],\n",
        "    'midea': ['lebanese', 'israel', 'lebanon', 'palest'],\n",
        "    'sport': ['coach', 'hitter', 'pitch'],\n",
        "    'gears': ['helmet', 'bike'],\n",
        "    'nasa ': ['orbit', 'spacecraft'],\n",
        "}\n",
        "def identify_topic_in_line(line):\n",
        "    topics = []\n",
        "    for topic, keywords in associations.items():\n",
        "        for word in keywords:\n",
        "            if word in line:\n",
        "                topics.append(topic)\n",
        "                break\n",
        "    return topics\n",
        "def print_top_words(beta, feature_names, n_top_words=10):\n",
        "    print('---------------Printing the Topics------------------')\n",
        "    for i in range(len(beta)):\n",
        "        line = \" \".join([feature_names[j] \n",
        "                            for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
        "        topics = identify_topic_in_line(line)\n",
        "        print('|'.join(topics))\n",
        "        print('     {}'.format(line))\n",
        "    print('---------------End of Topics------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uH5NRioC7aRs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        },
        "outputId": "4effbe07-8846-42be-acf1-8ca305c50d57"
      },
      "cell_type": "code",
      "source": [
        "# emb = model.decoder.weight.data.cpu().numpy().T\n",
        "# print(\"shape of beta is \" + str(emb.shape))\n",
        "print_top_words(model.beta.detach().numpy().T, vectorizer.get_feature_names())"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------Printing the Topics------------------\n",
            "comp \n",
            "     lower berkeley floppy city enforcement generally hard paul practice poster\n",
            "\n",
            "     communication mil buying oil released attack constitution noticed legal bell\n",
            "\n",
            "     asked harvard important talking centre includes reply individuals driven machines\n",
            "\n",
            "     hewlett research problem close differences thread rights players bible contains\n",
            "\n",
            "     proposed honest tried discovered alive csd round writes freedom technical\n",
            "\n",
            "     angeles max playing disease arab quite finally follows ground interested\n",
            "\n",
            "     announcement unless rochester houston groups rob products posting nntp disks\n",
            "jesus\n",
            "     sending computer felt want misc kevin particularly covered christ current\n",
            "\n",
            "     functions thousands historical sign imagine established mentioned cmu release useful\n",
            "midea\n",
            "     freedom ontario uxa domain drives internal israel tel guys fair\n",
            "\n",
            "     build rose ryan harvard center friend brian nearly clinton united\n",
            "\n",
            "     uxa digex eng office safe brian knowing died truly bad\n",
            "jesus\n",
            "     support provided values christ writing evidence published cool gmt know\n",
            "\n",
            "     iii francisco wants islam disclaimer peter park just hardly unit\n",
            "comp \n",
            "     red monitor cup ibm lead expect larger born cut aren\n",
            "comp \n",
            "     argue sex soviet generally windows chris complex nntp roads morning\n",
            "\n",
            "     used somebody personally super greg machines soon motif type college\n",
            "\n",
            "     man washington evil adams air alive load turks generally times\n",
            "\n",
            "     worked recently ahead imagine andrew nntp bitnet topic essentially standard\n",
            "\n",
            "     generally section applications known player medicine file users response blame\n",
            "\n",
            "     slow copies douglas alan lord known giving story guns level\n",
            "\n",
            "     somebody poster reasonable charles adams took correct regardless pub view\n",
            "\n",
            "     nyx interested sending lord respond wish chance america worry driven\n",
            "\n",
            "     carrying risk led consider dan images type ecn patrick digital\n",
            "jesus\n",
            "     air associated contact mil company lived simply night views jesus\n",
            "\n",
            "     anymore possibly radio middle explain life details love possible dead\n",
            "\n",
            "     reasonable capital manner needed placed excuse easy pretty archive error\n",
            "\n",
            "     providing terms brian medicine received rutgers highly newsgroups academic idea\n",
            "\n",
            "     prism love readers cases relevant plan level seriously areas address\n",
            "\n",
            "     secret location showed description end iii sex remain respond hell\n",
            "\n",
            "     day description main fan centre things houston kill essentially absolutely\n",
            "nasa \n",
            "     traffic events rates brad selling closed orbit universe fan service\n",
            "midea\n",
            "     israeli city year interpretation claimed unless simple ohio interested cause\n",
            "\n",
            "     detroit happen hear basically private dept alan having mit buy\n",
            "\n",
            "     fixed years hope bank man surrender forces defined unless pain\n",
            "car  \n",
            "     search basic effort true fixed knowing entirely crypto installed team\n",
            "\n",
            "     world fan holy greek step fail ice lived provides bob\n",
            "\n",
            "     privacy written ignore apply koresh intelligence david decide guns final\n",
            "\n",
            "     sin turks usenet century asked high interpretation present keeping context\n",
            "\n",
            "     raised com ftp complex holy responses replaced increase peter slightly\n",
            "\n",
            "     comment legal plus wait bit god author appropriate uxa let\n",
            "midea\n",
            "     break tells voice prefer yes facts israel athos lcs united\n",
            "\n",
            "     live described wayne essentially compatible list fbi require argue reasonable\n",
            "\n",
            "     tin interested effective generation main traffic wanted speed today story\n",
            "\n",
            "     father easy print corporation newsgroup evil typical frank resolution response\n",
            "\n",
            "     plus bunch particularly insurance allows armenian yes america drive document\n",
            "\n",
            "     cards personally documentation dave suggestions days nearly big series supply\n",
            "\n",
            "     motif checked florida white wasn acceptable pre expect week united\n",
            "\n",
            "     chips auto rates kevin important motif folks steven comes escrow\n",
            "\n",
            "     required kevin chips crime cso placed anymore author eric motorola\n",
            "---------------End of Topics------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wL6q7Pzo7aXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GU8GwP1t7aaj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f6aRZQQ47aea",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Dvbti3h7aVF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "539a0c41-2f02-462c-954d-4d61885204cc"
      },
      "cell_type": "code",
      "source": [
        "model.beta.shape\n",
        "model.theta.weight.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-7f022c388c8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'VAE' object has no attribute 'theta'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GyQ3-xkrR1XX",
        "outputId": "62e106a1-ddcf-4a6c-f80a-6e3e910825bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# beta = F.softmax(model.word_embedding.weight.mm(model.topic_embedding.weight.t()))\n",
        "\n",
        "beta = model.decoder.weight.softmax(1)\n",
        "print(beta.sum(1).shape)\n",
        "print(beta.shape)\n",
        "_, ind = torch.sort(beta, 0)\n",
        "# ind.numpy()[0:50, 0] - ind.numpy()[0:50, 1]\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:25, 0])\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:25, 1])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 2])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 3])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 4])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 5])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 7])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 8])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 9])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 15])\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 19])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2441])\n",
            "torch.Size([2441, 2])\n",
            "['ins' 'ended' 'units' 'suggestion' 'model' 'bruce' 'jupiter' 'myers'\n",
            " 'charles' 'cadlab' 'obviously' 'term' 'charge' 'ohio' 'morning' 'lies'\n",
            " 'stated' 'began' 'machine' 'supposed' 'george' 'nation' 'signature'\n",
            " 'loved' 'drugs']\n",
            "['position' 'university' 'forms' 'success' 'trying' 'administrative'\n",
            " 'fighting' 'specific' 'orbital' 'chances' 'opportunity' 'gnv' 'math'\n",
            " 'like' 'car' 'weight' 'articles' 'worthwhile' 'exists' 'galaxy'\n",
            " 'understand' 'known' 'estimate' 'cosmic' 'exactly']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KeltSp1qJJNH"
      },
      "cell_type": "markdown",
      "source": [
        "##  Stuff"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-X7Z51K4EDI2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-geZP7XKtadI",
        "outputId": "389e6a19-9783-44b3-c44f-f1a861ee3a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# a = torch.randn(100, 128)\n",
        "a = torch.tensor([[1,2,3], [4,5,6]]).float()\n",
        "b = torch.tensor([[1,2,3], [4,5,6]]).float()\n",
        "\n",
        "F.cosine_similarity(a, b)\n",
        "b.t()\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 4.],\n",
              "        [2., 5.],\n",
              "        [3., 6.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1n0TXZhZG9Er"
      },
      "cell_type": "markdown",
      "source": [
        "Get topic distributions"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EIvkM_22DhY-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unscaled_topics = torch.mm(model.word_embedding(torch.tensor(np.arange(doc_term_matrix.shape[1]))),\n",
        "#          torch.transpose(model.topicslayer.weight, 0, 1))\n",
        "# topic_dist = torch.softmax(unscaled_topics, dim = 0)\n",
        "# topic_dist.sum(dim = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bR5eDznAHONV"
      },
      "cell_type": "markdown",
      "source": [
        "This one helped us a lot"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z16IGTridRPf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model.encode(torch.LongTensor(doc_term_matrix[0]))\n",
        "#input = torch.tensor(doc_term_matrix).float()\n",
        "input = torch.tensor(doc_term_matrix).float()[[0, 1], ]\n",
        "mu, sigma = model.encode(input)\n",
        "z = model.reparameterize(mu, sigma)\n",
        "# model.decode(x, input.shape[0])\n",
        "\n",
        "x = model.fc3(z)\n",
        "theta = F.softmax(x) # to get theta\n",
        "embedding_matrix = model.word_embedding(torch.tensor(np.arange(14)))\n",
        "word_dot_topic = model.fc4(embedding_matrix) # weights corresp to topic vector\n",
        "beta = F.softmax(word_dot_topic)\n",
        "log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(beta, 0, 1)))\n",
        "#theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "# print(theta.shape)\n",
        "# print(theta)\n",
        "# print(embedding_matrix)\n",
        "# print(word_dot_topic)\n",
        "print(beta.shape)\n",
        "print(beta)\n",
        "print(log_theta_dot_beta)\n",
        "print(torch.exp(log_theta_dot_beta_normalized))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YPbnTiNpCcaO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model.encode(torch.LongTensor(doc_term_matrix[0]))\n",
        "#input = torch.tensor(doc_term_matrix).float()\n",
        "input = torch.tensor(doc_term_matrix).float()[[0, 1], ]\n",
        "print(input)\n",
        "mu, sigma = model.encode(input)\n",
        "z = model.reparameterize(mu, sigma)\n",
        "print(z)\n",
        "# model.decode(x, input.shape[0])\n",
        "\n",
        "\n",
        "# x = model.lin2(z)\n",
        "# theta = F.softmax(x) # to get theta\n",
        "# embedding_matrix = model.word_embedding(torch.tensor(np.arange(model.num_docs)))\n",
        "# word_dot_topic = model.topicslayer(embedding_matrix) # weights corresp to topic vector\n",
        "# model.beta = F.softmax(word_dot_topic, dim = 0)\n",
        "# log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(model.beta, 0, 1)))\n",
        "# #theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "# log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "# print(embedding_matrix.shape) # dim of embedding matrix is 1544 x 100\n",
        "\n",
        "\n",
        "x = model.lin2(z)\n",
        "theta = F.softmax(x, 1) # to get theta\n",
        "print(theta.sum(1))\n",
        "embedding_matrix = model.word_embedding.weight\n",
        "print(model.word_embedding(torch.tensor(np.arange(model.num_docs))).shape)\n",
        "print(embedding_matrix.shape)\n",
        "word_dot_topic = model.topicslayer(embedding_matrix) # weights corresp to topic vector\n",
        "model.beta = F.softmax(word_dot_topic, dim = 0)\n",
        "log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(model.beta, 0, 1)))\n",
        "#theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "# print(embedding_matrix.shape) # dim of embedding matrix is still 1544 x 100\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gqevKMN6pdEn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###########################\n",
        "###########################\n",
        "###########################\n",
        "\n",
        "test_input = torch.tensor(doc_term_matrix).float()[[0, 1], ] # pretend_batch_size = 2\n",
        "# print(test_input.shape) # 2 x 2441, where 2441 is vocab size\n",
        "mu, logvar = model.encode(test_input)\n",
        "# print(mu.shape) # 2 x 50\n",
        "# print(sigma.shape) # 2 x 50\n",
        "z = model.reparameterize(mu, sigma) # 2 x 50\n",
        "# print(z.shape) # 2 x 50\n",
        "output = model.decode(z)\n",
        "# print(output)\n",
        "# print(output.shape)\n",
        "\n",
        "pretend_num_docs = 50\n",
        "pretend_batch_size = test_input.shape[0]\n",
        "#print(output.sum())\n",
        "BCE = pretend_num_docs * 1.0 / pretend_batch_size * output.sum() # we sum the log probabilities\n",
        "# print(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "KLD0 = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "print(logvar)\n",
        "#KLD1 = -0.5 * torch.sum(2 + torch.sum(torch.cumprod(logvar)) - torch.mm(torch.transpose(mu, 1, 0),mu) - logvar.exp()) # this is a number, should be \n",
        "############################################################################################################\n",
        "#KLD2 = 0.5 * (torch.sum(logvar.exp()) + torch.dot(mu, mu) - 50 - torch.log(torch.cumprod(logvar))) ###########################\n",
        "# print(float(BCE))\n",
        "\n",
        "# print(\"BCE: \" + \"{:.2f}\".format(float(BCE)))\n",
        "# print(\"KLD: \" + \"{:.2f}\".format(float(KLD)))\n",
        "# print(\"Loss: \" + \"{:.2f}\".format(float(- BCE + KLD)))\n",
        "# return - BCE + KLD # - .1 * (zeta - nu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "J0K5OnFvuqcG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# X'X\n",
        "torch.mm(torch.transpose(torch.tensor([[1, 2], [3, 4]]), 1, 0),torch.tensor([[1, 2], [3, 4]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oFEeCibxJFLQ"
      },
      "cell_type": "markdown",
      "source": [
        "# Topic Coherence"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Lh2eLudHDncY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def topic_coherence(beta, M, doc_term_matrix):\n",
        "  K = beta.shape[1] # beta has dim V x K\n",
        "  coherences = np.zeros(K)\n",
        "  for t in range(K):\n",
        "    index = np.argsort(-beta[:, t])[0:M]\n",
        "    cart_prod = product(list(index), list(index))\n",
        "    for ind1, ind2 in cart_prod:\n",
        "      if ind1 == ind2:\n",
        "        pass\n",
        "      else:\n",
        "        d_ind1 = (doc_term_matrix[:, ind1] > 0).sum()\n",
        "        d_ind12 = ((doc_term_matrix[:, ind1] > 0) & (doc_term_matrix[:, ind2] > 0)).sum()\n",
        "        coherences[t] += np.log1p(d_ind12) - np.log(d_ind1)\n",
        "\n",
        "  return coherences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LKM9IxgsLWKT",
        "outputId": "633d6e1d-9501-4821-b48a-34e509a88d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "topic_coherence(model.beta.detach().numpy(), 20, doc_term_matrix)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5.05942546, -5.05942546, -5.05942546, ..., -5.05942546,\n",
              "       -5.05942546, -5.05942546])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7BuiupBeNYqZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}