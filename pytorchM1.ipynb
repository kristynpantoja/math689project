{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorchM1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristynpantoja/math689project/blob/master/pytorchM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Yzom68CRUCUt",
        "colab_type": "code",
        "outputId": "3dc69534-d0fe-41ce-a82a-37a23072d2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L382anKLUwuE",
        "colab_type": "code",
        "outputId": "80457377-19ea-4d28-85ba-87dac14da90f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pJ2ybE0_FuhI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model Parameters"
      ]
    },
    {
      "metadata": {
        "id": "lsJa9n2aXLrD",
        "colab_type": "code",
        "outputId": "12670c06-a05e-454d-ea62-87cb7eda3fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# from types import SimpleNamespace\n",
        "\n",
        "# args_dict = {\"batch_size\" : 50, \"epochs\" : 50, \"no_cuda\" : False, \"seed\" : 1, \"log_interval\" : 10}\n",
        "# args = SimpleNamespace(**args_dict)\n",
        "# args.epochs"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "metadata": {
        "id": "-D3sQR-gF0Vk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data: 20newsgroups\n",
        "We get the document-term matrix"
      ]
    },
    {
      "metadata": {
        "id": "f5gixJ2PhDZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a2de103-95ed-4c87-a170-4e90bb1abd41"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from types import SimpleNamespace\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "# newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "vectorizer = CountVectorizer(min_df=.01, max_df=0.05, stop_words = 'english',\n",
        "                             token_pattern = u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
        "count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
        "doc_term_matrix = count_vecs.toarray()\n",
        "doc_term_matrix.shape # number of documents, number of words (in vocab)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 1544)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "6Sm8WG-SqMko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ECE01K5OF9r0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ragged array of words in each document (by index in vocabulary)"
      ]
    },
    {
      "metadata": {
        "id": "NKDN2_x45V44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def countsToInput(row):\n",
        "  return np.repeat(np.arange(doc_term_matrix.shape[1]),row)\n",
        "  \n",
        "def numWords(row):\n",
        "  return row.sum()\n",
        "\n",
        "N_train = np.apply_along_axis(numWords, axis=1, arr=doc_term_matrix)\n",
        "data_train = []\n",
        "for d in range(doc_term_matrix.shape[0]):\n",
        "  data_train.append(torch.from_numpy(countsToInput(doc_term_matrix[d])))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ax8q70r25toc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ea650c4b-8a91-4ed2-a0da-f5d86d79b950"
      },
      "cell_type": "code",
      "source": [
        "data_train[1030]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  27,   65,  160,  160,  161,  161,  240,  266,  323,  339,  416,  427,\n",
              "         549,  617,  658,  772,  844,  844,  844, 1082, 1083, 1099, 1120, 1120,\n",
              "        1120, 1120, 1120, 1124, 1165, 1214, 1292, 1297, 1318, 1333, 1449])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "8ASbgnVqGDvU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setup and packages"
      ]
    },
    {
      "metadata": {
        "id": "HoGhPn8n4Gok",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# from types import SimpleNamespace\n",
        "\n",
        "\n",
        "# args_dict = {\"batch_size\" : 50, \"epochs\" : 50, \"no_cuda\" : False, \"seed\" : 1, \"log_interval\" : 10}\n",
        "# args = SimpleNamespace(**args_dict)\n",
        "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "\n",
        "args_dict = {\"batch_size\" : 50, \"epochs\" : 50, \"no_cuda\" : False, \"seed\" : 1, \"log_interval\" : 10}\n",
        "args = SimpleNamespace(**args_dict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####3\n",
        "\n",
        "\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.ToTensor()),\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6E6F6rVgGXnS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define model"
      ]
    },
    {
      "metadata": {
        "id": "vwZfWV16T97w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, num_docs):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        vocab_size = doc_term_matrix.shape[1]\n",
        "        wordvec_dim = 100\n",
        "        K = 20\n",
        "        self.num_docs = num_docs\n",
        "        self.word_embedding = nn.Embedding(vocab_size, wordvec_dim) # decoder\n",
        "        self.lin1 = nn.Linear(vocab_size, 100) # encoder\n",
        "        self.mean = nn.Linear(100, 25) # encoder\n",
        "        self.logvar = nn.Linear(100, 25) # encoder\n",
        "        self.lin2 = nn.Linear(25, K) # decoder \n",
        "        self.topicslayer = nn.Linear(wordvec_dim, K) # decoder\n",
        "        self.beta = torch.zeros([K, vocab_size], dtype = torch.float32) # decoder\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.lin1(x))\n",
        "#         h1 = self.lin1(x)\n",
        "        return self.mean(h1), self.logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu) # this gives x ~ N(mu, var)\n",
        "\n",
        "      \n",
        "    def decode(self, z):\n",
        "        x = self.lin2(z)\n",
        "        theta = F.softmax(x, dim = 1) # to get theta\n",
        "        word_dot_topic = self.topicslayer(self.word_embedding.weight) # weights corresp to topic vector\n",
        "        self.beta = F.softmax(word_dot_topic, dim = 0)\n",
        "        log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(self.beta, 0, 1)))\n",
        "        #theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "        log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "        return log_theta_dot_beta_normalized\n",
        "        \n",
        "    def forward(self, doc):\n",
        "        mu, logvar = self.encode(doc)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "        \n",
        "\n",
        "\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uoGmoqZjaAHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load training data (separate into batches)"
      ]
    },
    {
      "metadata": {
        "id": "yWSJgB41sicr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# mnist_train_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=True, download=True,\n",
        "#                    transform=transforms.ToTensor()),\n",
        "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "# # enumerate(train_loader)\n",
        "\n",
        "train_data = torch.utils.data.TensorDataset(torch.tensor(doc_term_matrix))\n",
        "train_loader = torch.utils.data.DataLoader(train_data,                                            \n",
        "    batch_size = args.batch_size, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dt0_YBz5aQEI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "instantiate model and define functions for training"
      ]
    },
    {
      "metadata": {
        "id": "DO5T6YFkCTIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = VAE(doc_term_matrix.shape[1]).to(device) \n",
        "      \n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# optimizer = optim.RMSprop(model.parameters(), lr = 1e-3)\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(log_theta_dot_beta_normalized, x, mu, logvar):\n",
        "    BCE = log_theta_dot_beta_normalized.sum() # ?\n",
        "    #print(\"BCE: \" + str(BCE.max()))\n",
        "    #BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "\n",
        "    # see Appendix B from VAE paper:\n",
        "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "    # https://arxiv.org/abs/1312.6114\n",
        "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    \n",
        "    # KLD = 0.5 * (1/logvar.exp() + )\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    #print(\"KLD: \" + str(KLD))\n",
        "#     print(\"logvar: \" + str(logvar.max()))\n",
        "#     print(\"mu: \" + str(mu.max()))\n",
        "    return - BCE + KLD\n",
        "\n",
        "enc_variables = list(model.lin1.parameters()) + list(model.mean.parameters()) +  list(model.logvar.parameters())\n",
        "dec_variables = list(model.word_embedding.parameters()) + list(model.lin2.parameters()) + list(model.topicslayer.parameters())\n",
        "\n",
        "\n",
        "optim_enc = optim.Adam(enc_variables, lr=1e-3)\n",
        "optim_dec = optim.Adam(dec_variables, lr=1e-3)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for switch in range(0,2):\n",
        "        if switch == 0:\n",
        "            print(\"updating encoder variables\")\n",
        "            optimizer = optim_enc\n",
        "        else:\n",
        "            print(\"updating decoder variables\")\n",
        "            optimizer = optim_dec\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "            #data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            log_theta_beta, mu, logvar = model(data[0].float())\n",
        "            loss = loss_function(log_theta_beta, data, mu, logvar)\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "            if batch_idx % args.log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * data[0].shape[0], len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    loss.item() / data[0].shape[0]))\n",
        "\n",
        "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "              epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "# def test(epoch):\n",
        "#     model.eval()\n",
        "#     test_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for i, (data, _) in enumerate(test_loader):\n",
        "#             data = data.to(device)\n",
        "#             recon_batch, mu, logvar = model(data)\n",
        "#             test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "#             if i == 0:\n",
        "#                 n = min(data.size(0), 8)\n",
        "#                 comparison = torch.cat([data[:n],\n",
        "#                                       recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
        "# #                 save_image(comparison.cpu(),\n",
        "# #                          'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "#     test_loss /= len(test_loader.dataset)\n",
        "#     print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     for epoch in range(1, args.epochs + 1):\n",
        "#         train(epoch)\n",
        "#         test(epoch)\n",
        "#         with torch.no_grad():\n",
        "#             sample = torch.randn(64, 20).to(device)\n",
        "#             sample = model.decode(sample).cpu()\n",
        "#             save_image(sample.view(64, 1, 28, 28),\n",
        "#                        'results/sample_' + str(epoch) + '.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zuF9sAfDGjUE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " train the model"
      ]
    },
    {
      "metadata": {
        "id": "WfzHoXkeDrx7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model = VAE(doc_term_matrix.shape[1])#.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U6YlBb8cmhUa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "XY-GsKzb3b_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5913
        },
        "outputId": "14058310-4d65-47df-a6de-aba45598531a"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(epoch)"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating encoder variables\n",
            "Train Epoch: 1 [0/11314 (0%)]\tLoss: 6044.851250\n",
            "Train Epoch: 1 [500/11314 (4%)]\tLoss: 6044.493750\n",
            "Train Epoch: 1 [1000/11314 (9%)]\tLoss: 6044.499375\n",
            "Train Epoch: 1 [1500/11314 (13%)]\tLoss: 6043.973750\n",
            "Train Epoch: 1 [2000/11314 (18%)]\tLoss: 6044.098125\n",
            "Train Epoch: 1 [2500/11314 (22%)]\tLoss: 6044.462500\n",
            "Train Epoch: 1 [3000/11314 (26%)]\tLoss: 6043.849375\n",
            "Train Epoch: 1 [3500/11314 (31%)]\tLoss: 6044.201875\n",
            "Train Epoch: 1 [4000/11314 (35%)]\tLoss: 6044.183125\n",
            "Train Epoch: 1 [4500/11314 (40%)]\tLoss: 6044.398125\n",
            "Train Epoch: 1 [5000/11314 (44%)]\tLoss: 6043.795000\n",
            "Train Epoch: 1 [5500/11314 (48%)]\tLoss: 6044.512500\n",
            "Train Epoch: 1 [6000/11314 (53%)]\tLoss: 6044.432500\n",
            "Train Epoch: 1 [6500/11314 (57%)]\tLoss: 6044.485000\n",
            "Train Epoch: 1 [7000/11314 (62%)]\tLoss: 6045.356875\n",
            "Train Epoch: 1 [7500/11314 (66%)]\tLoss: 6043.785625\n",
            "Train Epoch: 1 [8000/11314 (70%)]\tLoss: 6043.729375\n",
            "Train Epoch: 1 [8500/11314 (75%)]\tLoss: 6044.022500\n",
            "Train Epoch: 1 [9000/11314 (79%)]\tLoss: 6043.952500\n",
            "Train Epoch: 1 [9500/11314 (84%)]\tLoss: 6043.882500\n",
            "Train Epoch: 1 [10000/11314 (88%)]\tLoss: 6043.789375\n",
            "Train Epoch: 1 [10500/11314 (93%)]\tLoss: 6043.869375\n",
            "Train Epoch: 1 [11000/11314 (97%)]\tLoss: 6043.591250\n",
            "====> Epoch: 1 Average loss: 6041.7642\n",
            "updating decoder variables\n",
            "Train Epoch: 1 [0/11314 (0%)]\tLoss: 6043.707500\n",
            "Train Epoch: 1 [500/11314 (4%)]\tLoss: 6042.425625\n",
            "Train Epoch: 1 [1000/11314 (9%)]\tLoss: 6041.759375\n",
            "Train Epoch: 1 [1500/11314 (13%)]\tLoss: 6041.402500\n",
            "Train Epoch: 1 [2000/11314 (18%)]\tLoss: 6041.208750\n",
            "Train Epoch: 1 [2500/11314 (22%)]\tLoss: 6041.005625\n",
            "Train Epoch: 1 [3000/11314 (26%)]\tLoss: 6040.928125\n",
            "Train Epoch: 1 [3500/11314 (31%)]\tLoss: 6040.846250\n",
            "Train Epoch: 1 [4000/11314 (35%)]\tLoss: 6040.853750\n",
            "Train Epoch: 1 [4500/11314 (40%)]\tLoss: 6040.794375\n",
            "Train Epoch: 1 [5000/11314 (44%)]\tLoss: 6040.803750\n",
            "Train Epoch: 1 [5500/11314 (48%)]\tLoss: 6040.797500\n",
            "Train Epoch: 1 [6000/11314 (53%)]\tLoss: 6040.748750\n",
            "Train Epoch: 1 [6500/11314 (57%)]\tLoss: 6040.775000\n",
            "Train Epoch: 1 [7000/11314 (62%)]\tLoss: 6040.896250\n",
            "Train Epoch: 1 [7500/11314 (66%)]\tLoss: 6040.756875\n",
            "Train Epoch: 1 [8000/11314 (70%)]\tLoss: 6040.754375\n",
            "Train Epoch: 1 [8500/11314 (75%)]\tLoss: 6041.273125\n",
            "Train Epoch: 1 [9000/11314 (79%)]\tLoss: 6040.717500\n",
            "Train Epoch: 1 [9500/11314 (84%)]\tLoss: 6040.761875\n",
            "Train Epoch: 1 [10000/11314 (88%)]\tLoss: 6040.727500\n",
            "Train Epoch: 1 [10500/11314 (93%)]\tLoss: 6040.754375\n",
            "Train Epoch: 1 [11000/11314 (97%)]\tLoss: 6040.729375\n",
            "====> Epoch: 1 Average loss: 12080.3936\n",
            "updating encoder variables\n",
            "Train Epoch: 2 [0/11314 (0%)]\tLoss: 6040.740000\n",
            "Train Epoch: 2 [500/11314 (4%)]\tLoss: 6040.477500\n",
            "Train Epoch: 2 [1000/11314 (9%)]\tLoss: 6040.305625\n",
            "Train Epoch: 2 [1500/11314 (13%)]\tLoss: 6040.275000\n",
            "Train Epoch: 2 [2000/11314 (18%)]\tLoss: 6040.231875\n",
            "Train Epoch: 2 [2500/11314 (22%)]\tLoss: 6040.222500\n",
            "Train Epoch: 2 [3000/11314 (26%)]\tLoss: 6040.213750\n",
            "Train Epoch: 2 [3500/11314 (31%)]\tLoss: 6040.207500\n",
            "Train Epoch: 2 [4000/11314 (35%)]\tLoss: 6040.205000\n",
            "Train Epoch: 2 [4500/11314 (40%)]\tLoss: 6040.200000\n",
            "Train Epoch: 2 [5000/11314 (44%)]\tLoss: 6040.195000\n",
            "Train Epoch: 2 [5500/11314 (48%)]\tLoss: 6040.198750\n",
            "Train Epoch: 2 [6000/11314 (53%)]\tLoss: 6040.195000\n",
            "Train Epoch: 2 [6500/11314 (57%)]\tLoss: 6040.275000\n",
            "Train Epoch: 2 [7000/11314 (62%)]\tLoss: 6040.187500\n",
            "Train Epoch: 2 [7500/11314 (66%)]\tLoss: 6040.199375\n",
            "Train Epoch: 2 [8000/11314 (70%)]\tLoss: 6040.187500\n",
            "Train Epoch: 2 [8500/11314 (75%)]\tLoss: 6040.190625\n",
            "Train Epoch: 2 [9000/11314 (79%)]\tLoss: 6040.189375\n",
            "Train Epoch: 2 [9500/11314 (84%)]\tLoss: 6040.187500\n",
            "Train Epoch: 2 [10000/11314 (88%)]\tLoss: 6040.187500\n",
            "Train Epoch: 2 [10500/11314 (93%)]\tLoss: 6040.182500\n",
            "Train Epoch: 2 [11000/11314 (97%)]\tLoss: 6040.182500\n",
            "====> Epoch: 2 Average loss: 6037.8031\n",
            "updating decoder variables\n",
            "Train Epoch: 2 [0/11314 (0%)]\tLoss: 6040.185000\n",
            "Train Epoch: 2 [500/11314 (4%)]\tLoss: 6040.183750\n",
            "Train Epoch: 2 [1000/11314 (9%)]\tLoss: 6040.178750\n",
            "Train Epoch: 2 [1500/11314 (13%)]\tLoss: 6040.175000\n",
            "Train Epoch: 2 [2000/11314 (18%)]\tLoss: 6040.176250\n",
            "Train Epoch: 2 [2500/11314 (22%)]\tLoss: 6040.176875\n",
            "Train Epoch: 2 [3000/11314 (26%)]\tLoss: 6040.173750\n",
            "Train Epoch: 2 [3500/11314 (31%)]\tLoss: 6040.173125\n",
            "Train Epoch: 2 [4000/11314 (35%)]\tLoss: 6040.173750\n",
            "Train Epoch: 2 [4500/11314 (40%)]\tLoss: 6040.170625\n",
            "Train Epoch: 2 [5000/11314 (44%)]\tLoss: 6040.172500\n",
            "Train Epoch: 2 [5500/11314 (48%)]\tLoss: 6040.170000\n",
            "Train Epoch: 2 [6000/11314 (53%)]\tLoss: 6040.171875\n",
            "Train Epoch: 2 [6500/11314 (57%)]\tLoss: 6040.171250\n",
            "Train Epoch: 2 [7000/11314 (62%)]\tLoss: 6040.169375\n",
            "Train Epoch: 2 [7500/11314 (66%)]\tLoss: 6040.169375\n",
            "Train Epoch: 2 [8000/11314 (70%)]\tLoss: 6040.170000\n",
            "Train Epoch: 2 [8500/11314 (75%)]\tLoss: 6040.169375\n",
            "Train Epoch: 2 [9000/11314 (79%)]\tLoss: 6040.168750\n",
            "Train Epoch: 2 [9500/11314 (84%)]\tLoss: 6040.168125\n",
            "Train Epoch: 2 [10000/11314 (88%)]\tLoss: 6040.168125\n",
            "Train Epoch: 2 [10500/11314 (93%)]\tLoss: 6040.168750\n",
            "Train Epoch: 2 [11000/11314 (97%)]\tLoss: 6040.169375\n",
            "====> Epoch: 2 Average loss: 12075.5474\n",
            "updating encoder variables\n",
            "Train Epoch: 3 [0/11314 (0%)]\tLoss: 6040.166250\n",
            "Train Epoch: 3 [500/11314 (4%)]\tLoss: 6040.167500\n",
            "Train Epoch: 3 [1000/11314 (9%)]\tLoss: 6040.167500\n",
            "Train Epoch: 3 [1500/11314 (13%)]\tLoss: 6040.166875\n",
            "Train Epoch: 3 [2000/11314 (18%)]\tLoss: 6040.181875\n",
            "Train Epoch: 3 [2500/11314 (22%)]\tLoss: 6040.166250\n",
            "Train Epoch: 3 [3000/11314 (26%)]\tLoss: 6040.166875\n",
            "Train Epoch: 3 [3500/11314 (31%)]\tLoss: 6040.166875\n",
            "Train Epoch: 3 [4000/11314 (35%)]\tLoss: 6040.166250\n",
            "Train Epoch: 3 [4500/11314 (40%)]\tLoss: 6040.165625\n",
            "Train Epoch: 3 [5000/11314 (44%)]\tLoss: 6040.165625\n",
            "Train Epoch: 3 [5500/11314 (48%)]\tLoss: 6040.166250\n",
            "Train Epoch: 3 [6000/11314 (53%)]\tLoss: 6040.165625\n",
            "Train Epoch: 3 [6500/11314 (57%)]\tLoss: 6040.166250\n",
            "Train Epoch: 3 [7000/11314 (62%)]\tLoss: 6040.165625\n",
            "Train Epoch: 3 [7500/11314 (66%)]\tLoss: 6040.165000\n",
            "Train Epoch: 3 [8000/11314 (70%)]\tLoss: 6040.166250\n",
            "Train Epoch: 3 [8500/11314 (75%)]\tLoss: 6040.165000\n",
            "Train Epoch: 3 [9000/11314 (79%)]\tLoss: 6040.165625\n",
            "Train Epoch: 3 [9500/11314 (84%)]\tLoss: 6040.166250\n",
            "Train Epoch: 3 [10000/11314 (88%)]\tLoss: 6040.165625\n",
            "Train Epoch: 3 [10500/11314 (93%)]\tLoss: 6040.165000\n",
            "Train Epoch: 3 [11000/11314 (97%)]\tLoss: 6040.166250\n",
            "====> Epoch: 3 Average loss: 6037.7357\n",
            "updating decoder variables\n",
            "Train Epoch: 3 [0/11314 (0%)]\tLoss: 6040.165000\n",
            "Train Epoch: 3 [500/11314 (4%)]\tLoss: 6040.165625\n",
            "Train Epoch: 3 [1000/11314 (9%)]\tLoss: 6040.163750\n",
            "Train Epoch: 3 [1500/11314 (13%)]\tLoss: 6040.165625\n",
            "Train Epoch: 3 [2000/11314 (18%)]\tLoss: 6040.164375\n",
            "Train Epoch: 3 [2500/11314 (22%)]\tLoss: 6040.165625\n",
            "Train Epoch: 3 [3000/11314 (26%)]\tLoss: 6040.165625\n",
            "Train Epoch: 3 [3500/11314 (31%)]\tLoss: 6040.164375\n",
            "Train Epoch: 3 [4000/11314 (35%)]\tLoss: 6040.163750\n",
            "Train Epoch: 3 [4500/11314 (40%)]\tLoss: 6040.164375\n",
            "Train Epoch: 3 [5000/11314 (44%)]\tLoss: 6040.163750\n",
            "Train Epoch: 3 [5500/11314 (48%)]\tLoss: 6040.169375\n",
            "Train Epoch: 3 [6000/11314 (53%)]\tLoss: 6040.164375\n",
            "Train Epoch: 3 [6500/11314 (57%)]\tLoss: 6040.163750\n",
            "Train Epoch: 3 [7000/11314 (62%)]\tLoss: 6040.163750\n",
            "Train Epoch: 3 [7500/11314 (66%)]\tLoss: 6040.164375\n",
            "Train Epoch: 3 [8000/11314 (70%)]\tLoss: 6040.163125\n",
            "Train Epoch: 3 [8500/11314 (75%)]\tLoss: 6040.163750\n",
            "Train Epoch: 3 [9000/11314 (79%)]\tLoss: 6040.163750\n",
            "Train Epoch: 3 [9500/11314 (84%)]\tLoss: 6040.163750\n",
            "Train Epoch: 3 [10000/11314 (88%)]\tLoss: 6040.162500\n",
            "Train Epoch: 3 [10500/11314 (93%)]\tLoss: 6040.174375\n",
            "Train Epoch: 3 [11000/11314 (97%)]\tLoss: 6040.163750\n",
            "====> Epoch: 3 Average loss: 12075.4682\n",
            "updating encoder variables\n",
            "Train Epoch: 4 [0/11314 (0%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [500/11314 (4%)]\tLoss: 6040.163750\n",
            "Train Epoch: 4 [1000/11314 (9%)]\tLoss: 6040.163750\n",
            "Train Epoch: 4 [1500/11314 (13%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [2000/11314 (18%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [2500/11314 (22%)]\tLoss: 6040.165000\n",
            "Train Epoch: 4 [3000/11314 (26%)]\tLoss: 6040.163750\n",
            "Train Epoch: 4 [3500/11314 (31%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [4000/11314 (35%)]\tLoss: 6040.163750\n",
            "Train Epoch: 4 [4500/11314 (40%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [5000/11314 (44%)]\tLoss: 6040.163750\n",
            "Train Epoch: 4 [5500/11314 (48%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [6000/11314 (53%)]\tLoss: 6040.163750\n",
            "Train Epoch: 4 [6500/11314 (57%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [7000/11314 (62%)]\tLoss: 6040.163750\n",
            "Train Epoch: 4 [7500/11314 (66%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [8000/11314 (70%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [8500/11314 (75%)]\tLoss: 6040.163750\n",
            "Train Epoch: 4 [9000/11314 (79%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [9500/11314 (84%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [10000/11314 (88%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [10500/11314 (93%)]\tLoss: 6040.163750\n",
            "Train Epoch: 4 [11000/11314 (97%)]\tLoss: 6040.163750\n",
            "====> Epoch: 4 Average loss: 6037.7323\n",
            "updating decoder variables\n",
            "Train Epoch: 4 [0/11314 (0%)]\tLoss: 6040.163750\n",
            "Train Epoch: 4 [500/11314 (4%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [1000/11314 (9%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [1500/11314 (13%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [2000/11314 (18%)]\tLoss: 6040.226250\n",
            "Train Epoch: 4 [2500/11314 (22%)]\tLoss: 6040.163750\n",
            "Train Epoch: 4 [3000/11314 (26%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [3500/11314 (31%)]\tLoss: 6040.161875\n",
            "Train Epoch: 4 [4000/11314 (35%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [4500/11314 (40%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [5000/11314 (44%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [5500/11314 (48%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [6000/11314 (53%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [6500/11314 (57%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [7000/11314 (62%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [7500/11314 (66%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [8000/11314 (70%)]\tLoss: 6040.163750\n",
            "Train Epoch: 4 [8500/11314 (75%)]\tLoss: 6040.161875\n",
            "Train Epoch: 4 [9000/11314 (79%)]\tLoss: 6040.161875\n",
            "Train Epoch: 4 [9500/11314 (84%)]\tLoss: 6040.161250\n",
            "Train Epoch: 4 [10000/11314 (88%)]\tLoss: 6040.163125\n",
            "Train Epoch: 4 [10500/11314 (93%)]\tLoss: 6040.162500\n",
            "Train Epoch: 4 [11000/11314 (97%)]\tLoss: 6040.162500\n",
            "====> Epoch: 4 Average loss: 12075.4651\n",
            "updating encoder variables\n",
            "Train Epoch: 5 [0/11314 (0%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [500/11314 (4%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [1000/11314 (9%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [1500/11314 (13%)]\tLoss: 6040.163750\n",
            "Train Epoch: 5 [2000/11314 (18%)]\tLoss: 6040.161875\n",
            "Train Epoch: 5 [2500/11314 (22%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [3000/11314 (26%)]\tLoss: 6040.163125\n",
            "Train Epoch: 5 [3500/11314 (31%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [4000/11314 (35%)]\tLoss: 6040.163125\n",
            "Train Epoch: 5 [4500/11314 (40%)]\tLoss: 6040.163125\n",
            "Train Epoch: 5 [5000/11314 (44%)]\tLoss: 6040.161875\n",
            "Train Epoch: 5 [5500/11314 (48%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [6000/11314 (53%)]\tLoss: 6040.163750\n",
            "Train Epoch: 5 [6500/11314 (57%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [7000/11314 (62%)]\tLoss: 6040.161875\n",
            "Train Epoch: 5 [7500/11314 (66%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [8000/11314 (70%)]\tLoss: 6040.161875\n",
            "Train Epoch: 5 [8500/11314 (75%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [9000/11314 (79%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [9500/11314 (84%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [10000/11314 (88%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [10500/11314 (93%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [11000/11314 (97%)]\tLoss: 6040.162500\n",
            "====> Epoch: 5 Average loss: 6037.7307\n",
            "updating decoder variables\n",
            "Train Epoch: 5 [0/11314 (0%)]\tLoss: 6040.163125\n",
            "Train Epoch: 5 [500/11314 (4%)]\tLoss: 6040.161250\n",
            "Train Epoch: 5 [1000/11314 (9%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [1500/11314 (13%)]\tLoss: 6040.163125\n",
            "Train Epoch: 5 [2000/11314 (18%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [2500/11314 (22%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [3000/11314 (26%)]\tLoss: 6040.161875\n",
            "Train Epoch: 5 [3500/11314 (31%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [4000/11314 (35%)]\tLoss: 6040.161875\n",
            "Train Epoch: 5 [4500/11314 (40%)]\tLoss: 6040.161875\n",
            "Train Epoch: 5 [5000/11314 (44%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [5500/11314 (48%)]\tLoss: 6040.161875\n",
            "Train Epoch: 5 [6000/11314 (53%)]\tLoss: 6040.161875\n",
            "Train Epoch: 5 [6500/11314 (57%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [7000/11314 (62%)]\tLoss: 6040.161875\n",
            "Train Epoch: 5 [7500/11314 (66%)]\tLoss: 6040.163125\n",
            "Train Epoch: 5 [8000/11314 (70%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [8500/11314 (75%)]\tLoss: 6040.163125\n",
            "Train Epoch: 5 [9000/11314 (79%)]\tLoss: 6040.161875\n",
            "Train Epoch: 5 [9500/11314 (84%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [10000/11314 (88%)]\tLoss: 6040.161875\n",
            "Train Epoch: 5 [10500/11314 (93%)]\tLoss: 6040.162500\n",
            "Train Epoch: 5 [11000/11314 (97%)]\tLoss: 6040.162500\n",
            "====> Epoch: 5 Average loss: 12075.4610\n",
            "updating encoder variables\n",
            "Train Epoch: 6 [0/11314 (0%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [500/11314 (4%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [1000/11314 (9%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [1500/11314 (13%)]\tLoss: 6040.163125\n",
            "Train Epoch: 6 [2000/11314 (18%)]\tLoss: 6040.163125\n",
            "Train Epoch: 6 [2500/11314 (22%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [3000/11314 (26%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [3500/11314 (31%)]\tLoss: 6040.161250\n",
            "Train Epoch: 6 [4000/11314 (35%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [4500/11314 (40%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [5000/11314 (44%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [5500/11314 (48%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [6000/11314 (53%)]\tLoss: 6040.161875\n",
            "Train Epoch: 6 [6500/11314 (57%)]\tLoss: 6040.161875\n",
            "Train Epoch: 6 [7000/11314 (62%)]\tLoss: 6040.161875\n",
            "Train Epoch: 6 [7500/11314 (66%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [8000/11314 (70%)]\tLoss: 6040.161250\n",
            "Train Epoch: 6 [8500/11314 (75%)]\tLoss: 6040.161250\n",
            "Train Epoch: 6 [9000/11314 (79%)]\tLoss: 6040.161875\n",
            "Train Epoch: 6 [9500/11314 (84%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [10000/11314 (88%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [10500/11314 (93%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [11000/11314 (97%)]\tLoss: 6040.161875\n",
            "====> Epoch: 6 Average loss: 6037.7304\n",
            "updating decoder variables\n",
            "Train Epoch: 6 [0/11314 (0%)]\tLoss: 6040.161250\n",
            "Train Epoch: 6 [500/11314 (4%)]\tLoss: 6040.161875\n",
            "Train Epoch: 6 [1000/11314 (9%)]\tLoss: 6040.161250\n",
            "Train Epoch: 6 [1500/11314 (13%)]\tLoss: 6040.161875\n",
            "Train Epoch: 6 [2000/11314 (18%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [2500/11314 (22%)]\tLoss: 6040.161875\n",
            "Train Epoch: 6 [3000/11314 (26%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [3500/11314 (31%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [4000/11314 (35%)]\tLoss: 6040.161875\n",
            "Train Epoch: 6 [4500/11314 (40%)]\tLoss: 6040.161250\n",
            "Train Epoch: 6 [5000/11314 (44%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [5500/11314 (48%)]\tLoss: 6040.161875\n",
            "Train Epoch: 6 [6000/11314 (53%)]\tLoss: 6040.163125\n",
            "Train Epoch: 6 [6500/11314 (57%)]\tLoss: 6040.163750\n",
            "Train Epoch: 6 [7000/11314 (62%)]\tLoss: 6040.163750\n",
            "Train Epoch: 6 [7500/11314 (66%)]\tLoss: 6040.163125\n",
            "Train Epoch: 6 [8000/11314 (70%)]\tLoss: 6040.163125\n",
            "Train Epoch: 6 [8500/11314 (75%)]\tLoss: 6040.163750\n",
            "Train Epoch: 6 [9000/11314 (79%)]\tLoss: 6040.163750\n",
            "Train Epoch: 6 [9500/11314 (84%)]\tLoss: 6040.163750\n",
            "Train Epoch: 6 [10000/11314 (88%)]\tLoss: 6040.162500\n",
            "Train Epoch: 6 [10500/11314 (93%)]\tLoss: 6040.161250\n",
            "Train Epoch: 6 [11000/11314 (97%)]\tLoss: 6040.167500\n",
            "====> Epoch: 6 Average loss: 12075.4609\n",
            "updating encoder variables\n",
            "Train Epoch: 7 [0/11314 (0%)]\tLoss: 6040.163125\n",
            "Train Epoch: 7 [500/11314 (4%)]\tLoss: 6040.164375\n",
            "Train Epoch: 7 [1000/11314 (9%)]\tLoss: 6040.163750\n",
            "Train Epoch: 7 [1500/11314 (13%)]\tLoss: 6040.161875\n",
            "Train Epoch: 7 [2000/11314 (18%)]\tLoss: 6040.161250\n",
            "Train Epoch: 7 [2500/11314 (22%)]\tLoss: 6040.163750\n",
            "Train Epoch: 7 [3000/11314 (26%)]\tLoss: 6040.163125\n",
            "Train Epoch: 7 [3500/11314 (31%)]\tLoss: 6040.164375\n",
            "Train Epoch: 7 [4000/11314 (35%)]\tLoss: 6040.165000\n",
            "Train Epoch: 7 [4500/11314 (40%)]\tLoss: 6040.162500\n",
            "Train Epoch: 7 [5000/11314 (44%)]\tLoss: 6040.161875\n",
            "Train Epoch: 7 [5500/11314 (48%)]\tLoss: 6040.163125\n",
            "Train Epoch: 7 [6000/11314 (53%)]\tLoss: 6040.161875\n",
            "Train Epoch: 7 [6500/11314 (57%)]\tLoss: 6040.161875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-268-54751addb18e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-267-e33c6bfb55eb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mlog_theta_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_theta_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "GyQ3-xkrR1XX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "b29590f3-328d-458e-f395-3bfeccfa8c11"
      },
      "cell_type": "code",
      "source": [
        "model.beta\n",
        "_, ind = torch.sort(model.beta, 0)\n",
        "# ind.numpy()[0:50, 0] - ind.numpy()[0:50, 1]\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 0])\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 1])\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 2])\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 3])\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 4])\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 5])\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 7])\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 8])\n",
        "print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:20, 9])"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['operations' 'san' 'price' 'release' 'andy' 'color' 'require' 'described'\n",
            " 'edge' 'america' 'criminals' 'self' 'feel' 'honest' 'meaning' 'monitor'\n",
            " 'silver' 'direct' 'suggestions' 'definition']\n",
            "['san' 'release' 'operations' 'self' 'silver' 'color' 'america'\n",
            " 'understand' 'require' 'suggestions' 'separate' 'direct' 'occur' 'war'\n",
            " 'andy' 'friends' 'meaning' 'joseph' 'criminals' 'honest']\n",
            "['operations' 'san' 'release' 'color' 'self' 'require' 'andy' 'price'\n",
            " 'america' 'direct' 'described' 'silver' 'edge' 'meaning' 'suggestions'\n",
            " 'honest' 'feel' 'criminals' 'separate' 'monitor']\n",
            "['release' 'san' 'operations' 'self' 'edge' 'require' 'color' 'america'\n",
            " 'suggestions' 'criminals' 'described' 'silver' 'andy' 'monitor' 'war'\n",
            " 'school' 'direct' 'feel' 'understand' 'separate']\n",
            "['operations' 'san' 'self' 'require' 'release' 'andy' 'color' 'school'\n",
            " 'direct' 'america' 'suggestions' 'separate' 'meaning' 'price' 'silver'\n",
            " 'war' 'edge' 'honest' 'cpu' 'described']\n",
            "['operations' 'san' 'self' 'release' 'require' 'school' 'andy' 'color'\n",
            " 'america' 'suggestions' 'direct' 'war' 'separate' 'edge' 'silver'\n",
            " 'meaning' 'price' 'cpu' 'criminals' 'described']\n",
            "['operations' 'require' 'color' 'san' 'images' 'release' 'cambridge'\n",
            " 'ready' 'honest' 'meaning' 'andy' 'damn' 'adams' 'electronic' 'guns'\n",
            " 'america' 'somebody' 'direct' 'feel' 'billion']\n",
            "['operations' 'san' 'self' 'require' 'andy' 'release' 'direct' 'color'\n",
            " 'school' 'america' 'separate' 'suggestions' 'price' 'meaning' 'war'\n",
            " 'silver' 'honest' 'feel' 'cpu' 'edge']\n",
            "['operations' 'san' 'self' 'require' 'andy' 'color' 'release' 'direct'\n",
            " 'price' 'america' 'meaning' 'separate' 'silver' 'honest' 'suggestions'\n",
            " 'feel' 'war' 'described' 'edge' 'criminals']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-geZP7XKtadI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "726ae4f7-8817-4017-b538-d843af00f225"
      },
      "cell_type": "code",
      "source": [
        "ind[:, 0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-16a58aa5c2fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ind' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "SCcYRL2ySugm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed38325e-bab3-4420-f83f-39ffcd3e907b"
      },
      "cell_type": "code",
      "source": [
        "torch.tensor((4, 25))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 4, 25])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "i5N3s6HCGpxy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get word vectors and topic vectors"
      ]
    },
    {
      "metadata": {
        "id": "mOdUu6iSGBVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3a8f25b8-c968-4794-cebf-06a6afebdde5"
      },
      "cell_type": "code",
      "source": [
        "for i in range(0, 2):\n",
        "  print(i)"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6dG8db3b3c-y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "542421f7-0775-4485-c8c8-732e2df68091"
      },
      "cell_type": "code",
      "source": [
        "model.word_embedding.weight"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 1.0982, -0.6489,  0.0177,  ...,  0.7344,  0.6163,  0.6307],\n",
              "        [-0.6461,  0.5238, -0.1179,  ..., -1.1260, -0.5322,  0.7694],\n",
              "        [ 1.2182,  0.6254,  0.0470,  ...,  1.6700,  0.4856,  2.6060],\n",
              "        ...,\n",
              "        [ 0.5594,  1.2776,  1.0770,  ...,  0.8158,  1.5054, -0.4085],\n",
              "        [ 1.0393, -0.1039,  0.9977,  ...,  1.5477, -0.1037,  0.0440],\n",
              "        [ 0.7130,  1.8861, -1.6776,  ..., -0.1981,  1.5925, -1.3581]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "metadata": {
        "id": "1n0TXZhZG9Er",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get topic distributions"
      ]
    },
    {
      "metadata": {
        "id": "EIvkM_22DhY-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86746194-5412-421a-f76a-5813328d9663"
      },
      "cell_type": "code",
      "source": [
        "unscaled_topics = torch.mm(model.word_embedding(torch.tensor(np.arange(doc_term_matrix.shape[1]))),\n",
        "         torch.transpose(model.topicslayer.weight, 0, 1))\n",
        "topic_dist = torch.softmax(unscaled_topics, dim = 0) \n",
        "topic_dist.sum(dim = 0)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "metadata": {
        "id": "bR5eDznAHONV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This one helped us a lot"
      ]
    },
    {
      "metadata": {
        "id": "z16IGTridRPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "9e684533-0527-4dfc-929b-402285e716ee"
      },
      "cell_type": "code",
      "source": [
        "#model.encode(torch.LongTensor(doc_term_matrix[0]))\n",
        "#input = torch.tensor(doc_term_matrix).float()\n",
        "input = torch.tensor(doc_term_matrix).float()[[0, 1], ]\n",
        "mu, sigma = model.encode(input)\n",
        "z = model.reparameterize(mu, sigma)\n",
        "# model.decode(x, input.shape[0])\n",
        "\n",
        "x = model.fc3(z)\n",
        "theta = F.softmax(x) # to get theta\n",
        "embedding_matrix = model.word_embedding(torch.tensor(np.arange(14)))\n",
        "word_dot_topic = model.fc4(embedding_matrix) # weights corresp to topic vector\n",
        "beta = F.softmax(word_dot_topic)\n",
        "log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(beta, 0, 1)))\n",
        "#theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "# print(theta.shape)\n",
        "# print(theta)\n",
        "# print(embedding_matrix)\n",
        "# print(word_dot_topic)\n",
        "print(beta.shape)\n",
        "print(beta)\n",
        "print(log_theta_dot_beta)\n",
        "print(torch.exp(log_theta_dot_beta_normalized))\n"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-270-b9bfe4a0b8a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# model.decode(x, input.shape[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to get theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'VAE' object has no attribute 'fc3'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "YPbnTiNpCcaO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "11798aae-8123-4d39-8576-3c00e612beef"
      },
      "cell_type": "code",
      "source": [
        "#model.encode(torch.LongTensor(doc_term_matrix[0]))\n",
        "#input = torch.tensor(doc_term_matrix).float()\n",
        "input = torch.tensor(doc_term_matrix).float()[[0, 1], ]\n",
        "print(input)\n",
        "mu, sigma = model.encode(input)\n",
        "z = model.reparameterize(mu, sigma)\n",
        "print(z)\n",
        "# model.decode(x, input.shape[0])\n",
        "\n",
        "\n",
        "# x = model.lin2(z)\n",
        "# theta = F.softmax(x) # to get theta\n",
        "# embedding_matrix = model.word_embedding(torch.tensor(np.arange(model.num_docs)))\n",
        "# word_dot_topic = model.topicslayer(embedding_matrix) # weights corresp to topic vector\n",
        "# model.beta = F.softmax(word_dot_topic, dim = 0)\n",
        "# log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(model.beta, 0, 1)))\n",
        "# #theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "# log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "# print(embedding_matrix.shape) # dim of embedding matrix is 1544 x 100\n",
        "\n",
        "\n",
        "x = model.lin2(z)\n",
        "theta = F.softmax(x, 1) # to get theta\n",
        "print(theta.sum(1))\n",
        "embedding_matrix = model.word_embedding.weight\n",
        "print(model.word_embedding(torch.tensor(np.arange(model.num_docs))).shape)\n",
        "print(embedding_matrix.shape)\n",
        "word_dot_topic = model.topicslayer(embedding_matrix) # weights corresp to topic vector\n",
        "model.beta = F.softmax(word_dot_topic, dim = 0)\n",
        "log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(model.beta, 0, 1)))\n",
        "#theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "# print(embedding_matrix.shape) # dim of embedding matrix is still 1544 x 100\n"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "tensor([[-0.3542, -0.9824,  1.6668,  0.2909, -1.7727,  1.6447, -0.0335, -0.3574,\n",
            "         -1.0229,  0.9524,  0.7662, -0.0573,  0.7221, -0.9453,  0.6689, -0.3142,\n",
            "          1.1116, -0.5060, -0.5279,  0.1749,  0.5559, -0.7530, -0.2019, -0.1538,\n",
            "          1.5552],\n",
            "        [ 1.1045,  0.5738, -1.3214,  0.2868,  0.1640, -0.9203,  0.2256, -1.9079,\n",
            "         -0.8485, -0.1582,  0.7142, -0.5238,  0.4466, -0.1109,  0.0060,  0.5935,\n",
            "         -2.4257, -1.1120, -0.1708, -0.0022, -3.3380, -0.6230, -0.6414,  2.1400,\n",
            "          0.5324]], grad_fn=<ThAddBackward>)\n",
            "tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
            "torch.Size([1544, 100])\n",
            "torch.Size([1544, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lh2eLudHDncY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}