{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorchM1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristynpantoja/math689project/blob/master/pytorchM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Yzom68CRUCUt",
        "colab_type": "code",
        "outputId": "d44f470f-b989-4921-dd2c-7d4bd55c3c9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L382anKLUwuE",
        "colab_type": "code",
        "outputId": "80457377-19ea-4d28-85ba-87dac14da90f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ma_hmJTmj1W0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pJ2ybE0_FuhI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model Parameters"
      ]
    },
    {
      "metadata": {
        "id": "lsJa9n2aXLrD",
        "colab_type": "code",
        "outputId": "12670c06-a05e-454d-ea62-87cb7eda3fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# from types import SimpleNamespace\n",
        "\n",
        "# args_dict = {\"batch_size\" : 50, \"epochs\" : 50, \"no_cuda\" : False, \"seed\" : 1, \"log_interval\" : 10}\n",
        "# args = SimpleNamespace(**args_dict)\n",
        "# args.epochs"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "metadata": {
        "id": "-D3sQR-gF0Vk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data: 20newsgroups\n",
        "We get the document-term matrix"
      ]
    },
    {
      "metadata": {
        "id": "f5gixJ2PhDZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "726b40af-b008-4b13-a6a2-1e39022d8d03"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from types import SimpleNamespace\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "vectorizer = CountVectorizer(min_df=.01)\n",
        "count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
        "doc_term_matrix = count_vecs.toarray()\n",
        "doc_term_matrix.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2034, 2400)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "ECE01K5OF9r0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ragged array of words in each document (by index in vocabulary)"
      ]
    },
    {
      "metadata": {
        "id": "NKDN2_x45V44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def countsToInput(row):\n",
        "  return np.repeat(np.arange(doc_term_matrix.shape[1]),row)\n",
        "  \n",
        "def numWords(row):\n",
        "  return row.sum()\n",
        "\n",
        "N_train = np.apply_along_axis(numWords, axis=1, arr=doc_term_matrix)\n",
        "data_train = []\n",
        "for d in range(doc_term_matrix.shape[0]):\n",
        "  data_train.append(torch.from_numpy(countsToInput(doc_term_matrix[d])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ax8q70r25toc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f6d0abc1-3917-47b4-e216-e2ba904bf261"
      },
      "cell_type": "code",
      "source": [
        "data_train[1030]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  49,  158,  161,  169,  188,  188,  188,  188,  188,  262,  297,  297,\n",
              "         297,  322,  371,  398,  493,  644,  707,  707,  721,  721,  855,  889,\n",
              "         889,  897,  945,  970, 1015, 1029, 1029, 1042, 1060, 1072, 1072, 1072,\n",
              "        1074, 1114, 1127, 1160, 1167, 1245, 1247, 1252, 1303, 1356, 1371, 1410,\n",
              "        1410, 1410, 1429, 1434, 1455, 1460, 1460, 1472, 1476, 1497, 1510, 1582,\n",
              "        1747, 1760, 1768, 1877, 1893, 1895, 1906, 1976, 1990, 2056, 2121, 2124,\n",
              "        2124, 2124, 2124, 2125, 2125, 2125, 2127, 2131, 2133, 2142, 2164, 2164,\n",
              "        2164, 2193, 2238, 2298, 2333, 2345, 2373, 2388, 2388])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "8ASbgnVqGDvU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setup and packages"
      ]
    },
    {
      "metadata": {
        "id": "HoGhPn8n4Gok",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# from types import SimpleNamespace\n",
        "\n",
        "\n",
        "# args_dict = {\"batch_size\" : 50, \"epochs\" : 50, \"no_cuda\" : False, \"seed\" : 1, \"log_interval\" : 10}\n",
        "# args = SimpleNamespace(**args_dict)\n",
        "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "\n",
        "args_dict = {\"batch_size\" : 50, \"epochs\" : 50, \"no_cuda\" : False, \"seed\" : 1, \"log_interval\" : 10}\n",
        "args = SimpleNamespace(**args_dict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####3\n",
        "\n",
        "\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.ToTensor()),\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6E6F6rVgGXnS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define model"
      ]
    },
    {
      "metadata": {
        "id": "vwZfWV16T97w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, num_docs):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        vocab_size = doc_term_matrix.shape[1]\n",
        "        wordvec_dim = 25\n",
        "        K = 4\n",
        "        self.num_docs = num_docs\n",
        "        self.word_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
        "        self.lin1 = nn.Linear(vocab_size, 10)\n",
        "        self.mean = nn.Linear(10, 3)\n",
        "        self.logvar = nn.Linear(10, 3)\n",
        "        self.lin2 = nn.Linear(3, K)\n",
        "        self.topicslayer = nn.Linear(wordvec_dim, K)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.lin1(x))\n",
        "#         h1 = self.lin1(x)\n",
        "        return self.mean(h1), self.logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu) # this gives x ~ N(mu, var)\n",
        "\n",
        "      \n",
        "    def decode(self, z):\n",
        "        x = self.lin2(z)\n",
        "        theta = F.softmax(x) # to get theta\n",
        "        embedding_matrix = self.word_embedding(torch.tensor(np.arange(self.num_docs)))\n",
        "        word_dot_topic = self.topicslayer(embedding_matrix) # weights corresp to topic vector\n",
        "        beta = F.softmax(word_dot_topic)\n",
        "        log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(beta, 0, 1)))\n",
        "        #theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "        log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "        return log_theta_dot_beta_normalized\n",
        "        \n",
        "    def forward(self, doc):\n",
        "        mu, logvar = self.encode(doc)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uoGmoqZjaAHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load training data (separate into batches)"
      ]
    },
    {
      "metadata": {
        "id": "yWSJgB41sicr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# mnist_train_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=True, download=True,\n",
        "#                    transform=transforms.ToTensor()),\n",
        "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "# # enumerate(train_loader)\n",
        "\n",
        "train_data = torch.utils.data.TensorDataset(torch.tensor(doc_term_matrix))\n",
        "train_loader = torch.utils.data.DataLoader(train_data,                                            \n",
        "    batch_size = args.batch_size, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dt0_YBz5aQEI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "instantiate model and define functions for training"
      ]
    },
    {
      "metadata": {
        "id": "DO5T6YFkCTIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = VAE(doc_term_matrix.shape[1]).to(device) \n",
        "      \n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "optimizer = optim.RMSprop(model.parameters(), lr = 1e-3)\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(log_theta_dot_beta_normalized, x, mu, logvar):\n",
        "    BCE = log_theta_dot_beta_normalized.sum() # ?\n",
        "#     print(\"BCE: \" + str(BCE.max()))\n",
        "    #BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "\n",
        "    # see Appendix B from VAE paper:\n",
        "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "    # https://arxiv.org/abs/1312.6114\n",
        "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    \n",
        "    # KLD = 0.5 * (1/logvar.exp() + )\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "#     print(\"KLD: \" + str(KLD))\n",
        "#     print(\"logvar: \" + str(logvar.max()))\n",
        "#     print(\"mu: \" + str(mu.max()))\n",
        "    return - BCE + KLD\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        #data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        log_theta_beta, mu, logvar = model(data[0].float())\n",
        "        loss = loss_function(log_theta_beta, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * data[0].shape[0], len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / data[0].shape[0]))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "# def test(epoch):\n",
        "#     model.eval()\n",
        "#     test_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for i, (data, _) in enumerate(test_loader):\n",
        "#             data = data.to(device)\n",
        "#             recon_batch, mu, logvar = model(data)\n",
        "#             test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "#             if i == 0:\n",
        "#                 n = min(data.size(0), 8)\n",
        "#                 comparison = torch.cat([data[:n],\n",
        "#                                       recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
        "# #                 save_image(comparison.cpu(),\n",
        "# #                          'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "#     test_loss /= len(test_loader.dataset)\n",
        "#     print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     for epoch in range(1, args.epochs + 1):\n",
        "#         train(epoch)\n",
        "#         test(epoch)\n",
        "#         with torch.no_grad():\n",
        "#             sample = torch.randn(64, 20).to(device)\n",
        "#             sample = model.decode(sample).cpu()\n",
        "#             save_image(sample.view(64, 1, 28, 28),\n",
        "#                        'results/sample_' + str(epoch) + '.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eHh1R_hsP8lV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e46e6d2-f443-4384-aea6-b9dee073aa5a"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f9834eb9ca8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "metadata": {
        "id": "zuF9sAfDGjUE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " train the model"
      ]
    },
    {
      "metadata": {
        "id": "WfzHoXkeDrx7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model = VAE(doc_term_matrix.shape[1])#.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_mAl6lutcQB7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XY-GsKzb3b_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5171
        },
        "outputId": "321d42b9-19de-4a2d-bea3-59ae3759c867"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(epoch)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/2034 (0%)]\tLoss: 9418.816250\n",
            "Train Epoch: 1 [500/2034 (24%)]\tLoss: 9395.883125\n",
            "Train Epoch: 1 [1000/2034 (49%)]\tLoss: 9393.932500\n",
            "Train Epoch: 1 [1500/2034 (73%)]\tLoss: 9393.284375\n",
            "Train Epoch: 1 [1360/2034 (98%)]\tLoss: 8466.025735\n",
            "====> Epoch: 1 Average loss: 9380.6761\n",
            "Train Epoch: 2 [0/2034 (0%)]\tLoss: 9391.083125\n",
            "Train Epoch: 2 [500/2034 (24%)]\tLoss: 9391.081875\n",
            "Train Epoch: 2 [1000/2034 (49%)]\tLoss: 9390.561250\n",
            "Train Epoch: 2 [1500/2034 (73%)]\tLoss: 9390.215000\n",
            "Train Epoch: 2 [1360/2034 (98%)]\tLoss: 8464.304228\n",
            "====> Epoch: 2 Average loss: 9375.1091\n",
            "Train Epoch: 3 [0/2034 (0%)]\tLoss: 9389.973125\n",
            "Train Epoch: 3 [500/2034 (24%)]\tLoss: 9389.650000\n",
            "Train Epoch: 3 [1000/2034 (49%)]\tLoss: 9389.494375\n",
            "Train Epoch: 3 [1500/2034 (73%)]\tLoss: 9389.474375\n",
            "Train Epoch: 3 [1360/2034 (98%)]\tLoss: 8463.738051\n",
            "====> Epoch: 3 Average loss: 9374.1038\n",
            "Train Epoch: 4 [0/2034 (0%)]\tLoss: 9389.291250\n",
            "Train Epoch: 4 [500/2034 (24%)]\tLoss: 9389.191250\n",
            "Train Epoch: 4 [1000/2034 (49%)]\tLoss: 9389.161250\n",
            "Train Epoch: 4 [1500/2034 (73%)]\tLoss: 9389.130000\n",
            "Train Epoch: 4 [1360/2034 (98%)]\tLoss: 8463.512868\n",
            "====> Epoch: 4 Average loss: 9373.7083\n",
            "Train Epoch: 5 [0/2034 (0%)]\tLoss: 9389.069375\n",
            "Train Epoch: 5 [500/2034 (24%)]\tLoss: 9389.037500\n",
            "Train Epoch: 5 [1000/2034 (49%)]\tLoss: 9389.028750\n",
            "Train Epoch: 5 [1500/2034 (73%)]\tLoss: 9388.975625\n",
            "Train Epoch: 5 [1360/2034 (98%)]\tLoss: 8463.391544\n",
            "====> Epoch: 5 Average loss: 9373.5442\n",
            "Train Epoch: 6 [0/2034 (0%)]\tLoss: 9388.973125\n",
            "Train Epoch: 6 [500/2034 (24%)]\tLoss: 9388.955000\n",
            "Train Epoch: 6 [1000/2034 (49%)]\tLoss: 9388.943125\n",
            "Train Epoch: 6 [1500/2034 (73%)]\tLoss: 9388.937500\n",
            "Train Epoch: 6 [1360/2034 (98%)]\tLoss: 8463.340993\n",
            "====> Epoch: 6 Average loss: 9373.4737\n",
            "Train Epoch: 7 [0/2034 (0%)]\tLoss: 9388.930625\n",
            "Train Epoch: 7 [500/2034 (24%)]\tLoss: 9388.914375\n",
            "Train Epoch: 7 [1000/2034 (49%)]\tLoss: 9388.915625\n",
            "Train Epoch: 7 [1500/2034 (73%)]\tLoss: 9388.914375\n",
            "Train Epoch: 7 [1360/2034 (98%)]\tLoss: 8463.319853\n",
            "====> Epoch: 7 Average loss: 9373.4449\n",
            "Train Epoch: 8 [0/2034 (0%)]\tLoss: 9388.907500\n",
            "Train Epoch: 8 [500/2034 (24%)]\tLoss: 9388.903750\n",
            "Train Epoch: 8 [1000/2034 (49%)]\tLoss: 9388.902500\n",
            "Train Epoch: 8 [1500/2034 (73%)]\tLoss: 9388.901250\n",
            "Train Epoch: 8 [1360/2034 (98%)]\tLoss: 8463.307904\n",
            "====> Epoch: 8 Average loss: 9373.4310\n",
            "Train Epoch: 9 [0/2034 (0%)]\tLoss: 9388.896875\n",
            "Train Epoch: 9 [500/2034 (24%)]\tLoss: 9388.896875\n",
            "Train Epoch: 9 [1000/2034 (49%)]\tLoss: 9388.892500\n",
            "Train Epoch: 9 [1500/2034 (73%)]\tLoss: 9388.892500\n",
            "Train Epoch: 9 [1360/2034 (98%)]\tLoss: 8463.302390\n",
            "====> Epoch: 9 Average loss: 9373.4227\n",
            "Train Epoch: 10 [0/2034 (0%)]\tLoss: 9388.891875\n",
            "Train Epoch: 10 [500/2034 (24%)]\tLoss: 9388.890000\n",
            "Train Epoch: 10 [1000/2034 (49%)]\tLoss: 9388.890000\n",
            "Train Epoch: 10 [1500/2034 (73%)]\tLoss: 9388.895000\n",
            "Train Epoch: 10 [1360/2034 (98%)]\tLoss: 8463.295037\n",
            "====> Epoch: 10 Average loss: 9373.4190\n",
            "Train Epoch: 11 [0/2034 (0%)]\tLoss: 9388.890625\n",
            "Train Epoch: 11 [500/2034 (24%)]\tLoss: 9388.893125\n",
            "Train Epoch: 11 [1000/2034 (49%)]\tLoss: 9388.886875\n",
            "Train Epoch: 11 [1500/2034 (73%)]\tLoss: 9388.881875\n",
            "Train Epoch: 11 [1360/2034 (98%)]\tLoss: 8463.291360\n",
            "====> Epoch: 11 Average loss: 9373.4156\n",
            "Train Epoch: 12 [0/2034 (0%)]\tLoss: 9388.885000\n",
            "Train Epoch: 12 [500/2034 (24%)]\tLoss: 9388.898125\n",
            "Train Epoch: 12 [1000/2034 (49%)]\tLoss: 9388.890625\n",
            "Train Epoch: 12 [1500/2034 (73%)]\tLoss: 9388.911875\n",
            "Train Epoch: 12 [1360/2034 (98%)]\tLoss: 8463.290441\n",
            "====> Epoch: 12 Average loss: 9373.4247\n",
            "Train Epoch: 13 [0/2034 (0%)]\tLoss: 9388.900625\n",
            "Train Epoch: 13 [500/2034 (24%)]\tLoss: 9388.915625\n",
            "Train Epoch: 13 [1000/2034 (49%)]\tLoss: 9388.891875\n",
            "Train Epoch: 13 [1500/2034 (73%)]\tLoss: 9388.872500\n",
            "Train Epoch: 13 [1360/2034 (98%)]\tLoss: 8463.281250\n",
            "====> Epoch: 13 Average loss: 9373.4175\n",
            "Train Epoch: 14 [0/2034 (0%)]\tLoss: 9388.883750\n",
            "Train Epoch: 14 [500/2034 (24%)]\tLoss: 9388.877500\n",
            "Train Epoch: 14 [1000/2034 (49%)]\tLoss: 9388.902500\n",
            "Train Epoch: 14 [1500/2034 (73%)]\tLoss: 9388.873125\n",
            "Train Epoch: 14 [1360/2034 (98%)]\tLoss: 8463.279412\n",
            "====> Epoch: 14 Average loss: 9373.4124\n",
            "Train Epoch: 15 [0/2034 (0%)]\tLoss: 9388.873750\n",
            "Train Epoch: 15 [500/2034 (24%)]\tLoss: 9388.870000\n",
            "Train Epoch: 15 [1000/2034 (49%)]\tLoss: 9388.879375\n",
            "Train Epoch: 15 [1500/2034 (73%)]\tLoss: 9388.880625\n",
            "Train Epoch: 15 [1360/2034 (98%)]\tLoss: 8463.277574\n",
            "====> Epoch: 15 Average loss: 9373.4047\n",
            "Train Epoch: 16 [0/2034 (0%)]\tLoss: 9388.865000\n",
            "Train Epoch: 16 [500/2034 (24%)]\tLoss: 9388.870000\n",
            "Train Epoch: 16 [1000/2034 (49%)]\tLoss: 9388.863750\n",
            "Train Epoch: 16 [1500/2034 (73%)]\tLoss: 9388.864375\n",
            "Train Epoch: 16 [1360/2034 (98%)]\tLoss: 8463.272059\n",
            "====> Epoch: 16 Average loss: 9373.3942\n",
            "Train Epoch: 17 [0/2034 (0%)]\tLoss: 9388.879375\n",
            "Train Epoch: 17 [500/2034 (24%)]\tLoss: 9388.925625\n",
            "Train Epoch: 17 [1000/2034 (49%)]\tLoss: 9388.918750\n",
            "Train Epoch: 17 [1500/2034 (73%)]\tLoss: 9388.864375\n",
            "Train Epoch: 17 [1360/2034 (98%)]\tLoss: 8463.269301\n",
            "====> Epoch: 17 Average loss: 9373.4191\n",
            "Train Epoch: 18 [0/2034 (0%)]\tLoss: 9388.864375\n",
            "Train Epoch: 18 [500/2034 (24%)]\tLoss: 9388.872500\n",
            "Train Epoch: 18 [1000/2034 (49%)]\tLoss: 9388.865625\n",
            "Train Epoch: 18 [1500/2034 (73%)]\tLoss: 9388.858750\n",
            "Train Epoch: 18 [1360/2034 (98%)]\tLoss: 8463.267463\n",
            "====> Epoch: 18 Average loss: 9373.3927\n",
            "Train Epoch: 19 [0/2034 (0%)]\tLoss: 9388.868750\n",
            "Train Epoch: 19 [500/2034 (24%)]\tLoss: 9388.860000\n",
            "Train Epoch: 19 [1000/2034 (49%)]\tLoss: 9388.858125\n",
            "Train Epoch: 19 [1500/2034 (73%)]\tLoss: 9388.864375\n",
            "Train Epoch: 19 [1360/2034 (98%)]\tLoss: 8463.278493\n",
            "====> Epoch: 19 Average loss: 9373.3927\n",
            "Train Epoch: 20 [0/2034 (0%)]\tLoss: 9388.919375\n",
            "Train Epoch: 20 [500/2034 (24%)]\tLoss: 9388.857500\n",
            "Train Epoch: 20 [1000/2034 (49%)]\tLoss: 9388.857500\n",
            "Train Epoch: 20 [1500/2034 (73%)]\tLoss: 9388.857500\n",
            "Train Epoch: 20 [1360/2034 (98%)]\tLoss: 8463.267463\n",
            "====> Epoch: 20 Average loss: 9373.3968\n",
            "Train Epoch: 21 [0/2034 (0%)]\tLoss: 9388.869375\n",
            "Train Epoch: 21 [500/2034 (24%)]\tLoss: 9388.863750\n",
            "Train Epoch: 21 [1000/2034 (49%)]\tLoss: 9388.859375\n",
            "Train Epoch: 21 [1500/2034 (73%)]\tLoss: 9388.862500\n",
            "Train Epoch: 21 [1360/2034 (98%)]\tLoss: 8463.265625\n",
            "====> Epoch: 21 Average loss: 9373.3880\n",
            "Train Epoch: 22 [0/2034 (0%)]\tLoss: 9388.865000\n",
            "Train Epoch: 22 [500/2034 (24%)]\tLoss: 9388.855625\n",
            "Train Epoch: 22 [1000/2034 (49%)]\tLoss: 9388.865000\n",
            "Train Epoch: 22 [1500/2034 (73%)]\tLoss: 9388.862500\n",
            "Train Epoch: 22 [1360/2034 (98%)]\tLoss: 8463.267463\n",
            "====> Epoch: 22 Average loss: 9373.3896\n",
            "Train Epoch: 23 [0/2034 (0%)]\tLoss: 9388.854375\n",
            "Train Epoch: 23 [500/2034 (24%)]\tLoss: 9388.855625\n",
            "Train Epoch: 23 [1000/2034 (49%)]\tLoss: 9388.925625\n",
            "Train Epoch: 23 [1500/2034 (73%)]\tLoss: 9388.925625\n",
            "Train Epoch: 23 [1360/2034 (98%)]\tLoss: 8463.268382\n",
            "====> Epoch: 23 Average loss: 9373.4233\n",
            "Train Epoch: 24 [0/2034 (0%)]\tLoss: 9388.886250\n",
            "Train Epoch: 24 [500/2034 (24%)]\tLoss: 9388.857500\n",
            "Train Epoch: 24 [1000/2034 (49%)]\tLoss: 9388.858750\n",
            "Train Epoch: 24 [1500/2034 (73%)]\tLoss: 9388.897500\n",
            "Train Epoch: 24 [1360/2034 (98%)]\tLoss: 8463.279412\n",
            "====> Epoch: 24 Average loss: 9373.4033\n",
            "Train Epoch: 25 [0/2034 (0%)]\tLoss: 9388.924375\n",
            "Train Epoch: 25 [500/2034 (24%)]\tLoss: 9388.863125\n",
            "Train Epoch: 25 [1000/2034 (49%)]\tLoss: 9388.856875\n",
            "Train Epoch: 25 [1500/2034 (73%)]\tLoss: 9388.872500\n",
            "Train Epoch: 25 [1360/2034 (98%)]\tLoss: 8463.266544\n",
            "====> Epoch: 25 Average loss: 9373.4011\n",
            "Train Epoch: 26 [0/2034 (0%)]\tLoss: 9388.862500\n",
            "Train Epoch: 26 [500/2034 (24%)]\tLoss: 9388.855000\n",
            "Train Epoch: 26 [1000/2034 (49%)]\tLoss: 9388.861875\n",
            "Train Epoch: 26 [1500/2034 (73%)]\tLoss: 9388.866875\n",
            "Train Epoch: 26 [1360/2034 (98%)]\tLoss: 8463.266544\n",
            "====> Epoch: 26 Average loss: 9373.3920\n",
            "Train Epoch: 27 [0/2034 (0%)]\tLoss: 9388.864375\n",
            "Train Epoch: 27 [500/2034 (24%)]\tLoss: 9388.868750\n",
            "Train Epoch: 27 [1000/2034 (49%)]\tLoss: 9388.857500\n",
            "Train Epoch: 27 [1500/2034 (73%)]\tLoss: 9388.856250\n",
            "Train Epoch: 27 [1360/2034 (98%)]\tLoss: 8463.266544\n",
            "====> Epoch: 27 Average loss: 9373.3915\n",
            "Train Epoch: 28 [0/2034 (0%)]\tLoss: 9388.862500\n",
            "Train Epoch: 28 [500/2034 (24%)]\tLoss: 9388.861875\n",
            "Train Epoch: 28 [1000/2034 (49%)]\tLoss: 9388.856875\n",
            "Train Epoch: 28 [1500/2034 (73%)]\tLoss: 9388.856250\n",
            "Train Epoch: 28 [1360/2034 (98%)]\tLoss: 8463.267463\n",
            "====> Epoch: 28 Average loss: 9373.3873\n",
            "Train Epoch: 29 [0/2034 (0%)]\tLoss: 9388.863750\n",
            "Train Epoch: 29 [500/2034 (24%)]\tLoss: 9388.914375\n",
            "Train Epoch: 29 [1000/2034 (49%)]\tLoss: 9388.857500\n",
            "Train Epoch: 29 [1500/2034 (73%)]\tLoss: 9388.858125\n",
            "Train Epoch: 29 [1360/2034 (98%)]\tLoss: 8463.265625\n",
            "====> Epoch: 29 Average loss: 9373.4055\n",
            "Train Epoch: 30 [0/2034 (0%)]\tLoss: 9388.887500\n",
            "Train Epoch: 30 [500/2034 (24%)]\tLoss: 9388.856250\n",
            "Train Epoch: 30 [1000/2034 (49%)]\tLoss: 9388.861875\n",
            "Train Epoch: 30 [1500/2034 (73%)]\tLoss: 9388.860625\n",
            "Train Epoch: 30 [1360/2034 (98%)]\tLoss: 8463.264706\n",
            "====> Epoch: 30 Average loss: 9373.3921\n",
            "Train Epoch: 31 [0/2034 (0%)]\tLoss: 9388.878750\n",
            "Train Epoch: 31 [500/2034 (24%)]\tLoss: 9388.928125\n",
            "Train Epoch: 31 [1000/2034 (49%)]\tLoss: 9388.928750\n",
            "Train Epoch: 31 [1500/2034 (73%)]\tLoss: 9388.926875\n",
            "Train Epoch: 31 [1360/2034 (98%)]\tLoss: 8463.277574\n",
            "====> Epoch: 31 Average loss: 9373.4509\n",
            "Train Epoch: 32 [0/2034 (0%)]\tLoss: 9388.925000\n",
            "Train Epoch: 32 [500/2034 (24%)]\tLoss: 9388.856250\n",
            "Train Epoch: 32 [1000/2034 (49%)]\tLoss: 9388.866875\n",
            "Train Epoch: 32 [1500/2034 (73%)]\tLoss: 9388.862500\n",
            "Train Epoch: 32 [1360/2034 (98%)]\tLoss: 8463.279412\n",
            "====> Epoch: 32 Average loss: 9373.4120\n",
            "Train Epoch: 33 [0/2034 (0%)]\tLoss: 9388.928125\n",
            "Train Epoch: 33 [500/2034 (24%)]\tLoss: 9388.928750\n",
            "Train Epoch: 33 [1000/2034 (49%)]\tLoss: 9388.928750\n",
            "Train Epoch: 33 [1500/2034 (73%)]\tLoss: 9388.928750\n",
            "Train Epoch: 33 [1360/2034 (98%)]\tLoss: 8463.280331\n",
            "====> Epoch: 33 Average loss: 9373.4557\n",
            "Train Epoch: 34 [0/2034 (0%)]\tLoss: 9388.928750\n",
            "Train Epoch: 34 [500/2034 (24%)]\tLoss: 9388.925000\n",
            "Train Epoch: 34 [1000/2034 (49%)]\tLoss: 9388.857500\n",
            "Train Epoch: 34 [1500/2034 (73%)]\tLoss: 9388.860000\n",
            "Train Epoch: 34 [1360/2034 (98%)]\tLoss: 8463.266544\n",
            "====> Epoch: 34 Average loss: 9373.4139\n",
            "Train Epoch: 35 [0/2034 (0%)]\tLoss: 9388.880000\n",
            "Train Epoch: 35 [500/2034 (24%)]\tLoss: 9388.908125\n",
            "Train Epoch: 35 [1000/2034 (49%)]\tLoss: 9388.876250\n",
            "Train Epoch: 35 [1500/2034 (73%)]\tLoss: 9388.856250\n",
            "Train Epoch: 35 [1360/2034 (98%)]\tLoss: 8463.269301\n",
            "====> Epoch: 35 Average loss: 9373.4106\n",
            "Train Epoch: 36 [0/2034 (0%)]\tLoss: 9388.911875\n",
            "Train Epoch: 36 [500/2034 (24%)]\tLoss: 9388.928750\n",
            "Train Epoch: 36 [1000/2034 (49%)]\tLoss: 9388.925625\n",
            "Train Epoch: 36 [1500/2034 (73%)]\tLoss: 9388.870000\n",
            "Train Epoch: 36 [1360/2034 (98%)]\tLoss: 8463.268382\n",
            "====> Epoch: 36 Average loss: 9373.4276\n",
            "Train Epoch: 37 [0/2034 (0%)]\tLoss: 9388.856875\n",
            "Train Epoch: 37 [500/2034 (24%)]\tLoss: 9388.857500\n",
            "Train Epoch: 37 [1000/2034 (49%)]\tLoss: 9388.866250\n",
            "Train Epoch: 37 [1500/2034 (73%)]\tLoss: 9388.928750\n",
            "Train Epoch: 37 [1360/2034 (98%)]\tLoss: 8463.280331\n",
            "====> Epoch: 37 Average loss: 9373.4133\n",
            "Train Epoch: 38 [0/2034 (0%)]\tLoss: 9388.928750\n",
            "Train Epoch: 38 [500/2034 (24%)]\tLoss: 9388.928750\n",
            "Train Epoch: 38 [1000/2034 (49%)]\tLoss: 9388.928750\n",
            "Train Epoch: 38 [1500/2034 (73%)]\tLoss: 9388.915000\n",
            "Train Epoch: 38 [1360/2034 (98%)]\tLoss: 8463.266544\n",
            "====> Epoch: 38 Average loss: 9373.4440\n",
            "Train Epoch: 39 [0/2034 (0%)]\tLoss: 9388.859375\n",
            "Train Epoch: 39 [500/2034 (24%)]\tLoss: 9388.858125\n",
            "Train Epoch: 39 [1000/2034 (49%)]\tLoss: 9388.856875\n",
            "Train Epoch: 39 [1500/2034 (73%)]\tLoss: 9388.872500\n",
            "Train Epoch: 39 [1360/2034 (98%)]\tLoss: 8463.265625\n",
            "====> Epoch: 39 Average loss: 9373.3874\n",
            "Train Epoch: 40 [0/2034 (0%)]\tLoss: 9388.867500\n",
            "Train Epoch: 40 [500/2034 (24%)]\tLoss: 9388.864375\n",
            "Train Epoch: 40 [1000/2034 (49%)]\tLoss: 9388.881875\n",
            "Train Epoch: 40 [1500/2034 (73%)]\tLoss: 9388.861875\n",
            "Train Epoch: 40 [1360/2034 (98%)]\tLoss: 8463.266544\n",
            "====> Epoch: 40 Average loss: 9373.3991\n",
            "Train Epoch: 41 [0/2034 (0%)]\tLoss: 9388.856875\n",
            "Train Epoch: 41 [500/2034 (24%)]\tLoss: 9388.871250\n",
            "Train Epoch: 41 [1000/2034 (49%)]\tLoss: 9388.874375\n",
            "Train Epoch: 41 [1500/2034 (73%)]\tLoss: 9388.888125\n",
            "Train Epoch: 41 [1360/2034 (98%)]\tLoss: 8463.266544\n",
            "====> Epoch: 41 Average loss: 9373.3957\n",
            "Train Epoch: 42 [0/2034 (0%)]\tLoss: 9388.862500\n",
            "Train Epoch: 42 [500/2034 (24%)]\tLoss: 9388.865000\n",
            "Train Epoch: 42 [1000/2034 (49%)]\tLoss: 9388.862500\n",
            "Train Epoch: 42 [1500/2034 (73%)]\tLoss: 9388.857500\n",
            "Train Epoch: 42 [1360/2034 (98%)]\tLoss: 8463.270221\n",
            "====> Epoch: 42 Average loss: 9373.3923\n",
            "Train Epoch: 43 [0/2034 (0%)]\tLoss: 9388.926875\n",
            "Train Epoch: 43 [500/2034 (24%)]\tLoss: 9388.921250\n",
            "Train Epoch: 43 [1000/2034 (49%)]\tLoss: 9388.856875\n",
            "Train Epoch: 43 [1500/2034 (73%)]\tLoss: 9388.857500\n",
            "Train Epoch: 43 [1360/2034 (98%)]\tLoss: 8463.266544\n",
            "====> Epoch: 43 Average loss: 9373.4075\n",
            "Train Epoch: 44 [0/2034 (0%)]\tLoss: 9388.898750\n",
            "Train Epoch: 44 [500/2034 (24%)]\tLoss: 9388.868750\n",
            "Train Epoch: 44 [1000/2034 (49%)]\tLoss: 9388.855000\n",
            "Train Epoch: 44 [1500/2034 (73%)]\tLoss: 9388.856875\n",
            "Train Epoch: 44 [1360/2034 (98%)]\tLoss: 8463.265625\n",
            "====> Epoch: 44 Average loss: 9373.3940\n",
            "Train Epoch: 45 [0/2034 (0%)]\tLoss: 9388.862500\n",
            "Train Epoch: 45 [500/2034 (24%)]\tLoss: 9388.873750\n",
            "Train Epoch: 45 [1000/2034 (49%)]\tLoss: 9388.870625\n",
            "Train Epoch: 45 [1500/2034 (73%)]\tLoss: 9388.856875\n",
            "Train Epoch: 45 [1360/2034 (98%)]\tLoss: 8463.278493\n",
            "====> Epoch: 45 Average loss: 9373.4017\n",
            "Train Epoch: 46 [0/2034 (0%)]\tLoss: 9388.926250\n",
            "Train Epoch: 46 [500/2034 (24%)]\tLoss: 9388.921250\n",
            "Train Epoch: 46 [1000/2034 (49%)]\tLoss: 9388.871250\n",
            "Train Epoch: 46 [1500/2034 (73%)]\tLoss: 9388.856250\n",
            "Train Epoch: 46 [1360/2034 (98%)]\tLoss: 8463.266544\n",
            "====> Epoch: 46 Average loss: 9373.4151\n",
            "Train Epoch: 47 [0/2034 (0%)]\tLoss: 9388.881250\n",
            "Train Epoch: 47 [500/2034 (24%)]\tLoss: 9388.926875\n",
            "Train Epoch: 47 [1000/2034 (49%)]\tLoss: 9388.903750\n",
            "Train Epoch: 47 [1500/2034 (73%)]\tLoss: 9388.856875\n",
            "Train Epoch: 47 [1360/2034 (98%)]\tLoss: 8463.267463\n",
            "====> Epoch: 47 Average loss: 9373.4199\n",
            "Train Epoch: 48 [0/2034 (0%)]\tLoss: 9388.878750\n",
            "Train Epoch: 48 [500/2034 (24%)]\tLoss: 9388.880625\n",
            "Train Epoch: 48 [1000/2034 (49%)]\tLoss: 9388.888125\n",
            "Train Epoch: 48 [1500/2034 (73%)]\tLoss: 9388.861875\n",
            "Train Epoch: 48 [1360/2034 (98%)]\tLoss: 8463.266544\n",
            "====> Epoch: 48 Average loss: 9373.4093\n",
            "Train Epoch: 49 [0/2034 (0%)]\tLoss: 9388.854375\n",
            "Train Epoch: 49 [500/2034 (24%)]\tLoss: 9388.882500\n",
            "Train Epoch: 49 [1000/2034 (49%)]\tLoss: 9388.921250\n",
            "Train Epoch: 49 [1500/2034 (73%)]\tLoss: 9388.928125\n",
            "Train Epoch: 49 [1360/2034 (98%)]\tLoss: 8463.278493\n",
            "====> Epoch: 49 Average loss: 9373.4362\n",
            "Train Epoch: 50 [0/2034 (0%)]\tLoss: 9388.928750\n",
            "Train Epoch: 50 [500/2034 (24%)]\tLoss: 9388.926875\n",
            "Train Epoch: 50 [1000/2034 (49%)]\tLoss: 9388.862500\n",
            "Train Epoch: 50 [1500/2034 (73%)]\tLoss: 9388.858750\n",
            "Train Epoch: 50 [1360/2034 (98%)]\tLoss: 8463.278493\n",
            "====> Epoch: 50 Average loss: 9373.4286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v2L_LFDdCfdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "f50e123d-ff34-48dd-8b69-878f7212fe6d"
      },
      "cell_type": "code",
      "source": [
        "model = VAE(doc_term_matrix.shape[1]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "train(epoch)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 7 [0/2034 (0%)]\tLoss: -9408.082500\n",
            "Train Epoch: 7 [500/2034 (24%)]\tLoss: nan\n",
            "Train Epoch: 7 [1000/2034 (49%)]\tLoss: nan\n",
            "Train Epoch: 7 [1500/2034 (73%)]\tLoss: nan\n",
            "Train Epoch: 7 [1360/2034 (98%)]\tLoss: nan\n",
            "====> Epoch: 7 Average loss: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GyQ3-xkrR1XX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "fbef770a-3026-41bb-8931-35ee164dbe53"
      },
      "cell_type": "code",
      "source": [
        "optimizer.\n",
        "model.word_embedding.weight\n",
        "model.lin1.weight\n",
        "model.lin2.weight\n",
        "model.topicslayer.weight"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.5191, -0.4505,  0.4671,  0.4755, -0.3381, -0.4420,  0.4226, -0.4525,\n",
              "          0.3360,  0.4570,  0.3691, -0.2974, -0.4897, -0.4309,  0.4802,  0.3489,\n",
              "          0.2622, -0.2527,  0.3144,  0.2765, -0.4011,  0.4457, -0.3800, -0.4889,\n",
              "          0.4484],\n",
              "        [-0.4174,  0.4940, -0.4468, -0.4230,  0.3201, -0.2856, -0.3884,  0.4389,\n",
              "          0.0144, -0.4571, -0.3816,  0.4967,  0.4741,  0.2404, -0.3002, -0.4741,\n",
              "         -0.4107,  0.4685, -0.4682, -0.4351,  0.3360, -0.2891,  0.4288,  0.3620,\n",
              "         -0.4533],\n",
              "        [ 0.3202, -0.2215, -0.0019,  0.0865, -0.2934,  0.2802,  0.1807, -0.0014,\n",
              "         -0.2988,  0.0420,  0.1480,  0.2780, -0.2968, -0.2348,  0.3020,  0.2106,\n",
              "          0.2236,  0.0830, -0.0792,  0.2026, -0.0954,  0.2134, -0.0432, -0.0955,\n",
              "          0.2622],\n",
              "        [ 0.3895, -0.2256,  0.4482,  0.1591, -0.3488,  0.4473,  0.4237,  0.3158,\n",
              "         -0.4711,  0.2832,  0.3595, -0.2066, -0.1595,  0.3625,  0.4815,  0.2891,\n",
              "         -0.4965, -0.3233,  0.3536,  0.3423, -0.3471, -0.3149, -0.4379,  0.4880,\n",
              "          0.0695]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "metadata": {
        "id": "SCcYRL2ySugm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "36c621c3-f95c-4b2a-f37b-8bc518a7ab9d"
      },
      "cell_type": "code",
      "source": [
        "train(epoch)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 4 [0/2034 (0%)]\tLoss: -5333305528012405160754020352.000000\n",
            "Train Epoch: 4 [10/2034 (24%)]\tLoss: -45943049707978675452193747763200000.000000\n",
            "Train Epoch: 4 [20/2034 (49%)]\tLoss: -34298240.000000\n",
            "Train Epoch: 4 [30/2034 (73%)]\tLoss: -23382464085326246707200.000000\n",
            "Train Epoch: 4 [40/2034 (98%)]\tLoss: -2425141670094831616.000000\n",
            "====> Epoch: 4 Average loss: -561390227454822543453852434169856.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i5N3s6HCGpxy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get word vectors and topic vectors"
      ]
    },
    {
      "metadata": {
        "id": "6dG8db3b3c-y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "542421f7-0775-4485-c8c8-732e2df68091"
      },
      "cell_type": "code",
      "source": [
        "model.word_embedding.weight"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 1.0982, -0.6489,  0.0177,  ...,  0.7344,  0.6163,  0.6307],\n",
              "        [-0.6461,  0.5238, -0.1179,  ..., -1.1260, -0.5322,  0.7694],\n",
              "        [ 1.2182,  0.6254,  0.0470,  ...,  1.6700,  0.4856,  2.6060],\n",
              "        ...,\n",
              "        [ 0.5594,  1.2776,  1.0770,  ...,  0.8158,  1.5054, -0.4085],\n",
              "        [ 1.0393, -0.1039,  0.9977,  ...,  1.5477, -0.1037,  0.0440],\n",
              "        [ 0.7130,  1.8861, -1.6776,  ..., -0.1981,  1.5925, -1.3581]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "metadata": {
        "id": "1n0TXZhZG9Er",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get topic distributions"
      ]
    },
    {
      "metadata": {
        "id": "EIvkM_22DhY-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86746194-5412-421a-f76a-5813328d9663"
      },
      "cell_type": "code",
      "source": [
        "unscaled_topics = torch.mm(model.word_embedding(torch.tensor(np.arange(doc_term_matrix.shape[1]))),\n",
        "         torch.transpose(model.topicslayer.weight, 0, 1))\n",
        "topic_dist = torch.softmax(unscaled_topics, dim = 0) \n",
        "topic_dist.sum(dim = 0)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "metadata": {
        "id": "bR5eDznAHONV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This one helped us a lot"
      ]
    },
    {
      "metadata": {
        "id": "z16IGTridRPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "3237dfd6-5a84-4ec6-96af-51e6e6b46b3f"
      },
      "cell_type": "code",
      "source": [
        "#model.encode(torch.LongTensor(doc_term_matrix[0]))\n",
        "#input = torch.tensor(doc_term_matrix).float()\n",
        "input = torch.tensor(doc_term_matrix).float()[[0, 1], ]\n",
        "mu, sigma = model.encode(input)\n",
        "z = model.reparameterize(mu, sigma)\n",
        "# model.decode(x, input.shape[0])\n",
        "\n",
        "x = model.fc3(z)\n",
        "theta = F.softmax(x) # to get theta\n",
        "embedding_matrix = model.word_embedding(torch.tensor(np.arange(14)))\n",
        "word_dot_topic = model.fc4(embedding_matrix) # weights corresp to topic vector\n",
        "beta = F.softmax(word_dot_topic)\n",
        "log_theta_dot_beta = torch.log(torch.mm(theta, torch.transpose(beta, 0, 1)))\n",
        "#theta_dot_beta = torch.exp(log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0))\n",
        "log_theta_dot_beta_normalized = log_theta_dot_beta - torch.logsumexp(log_theta_dot_beta, dim = 0)\n",
        "# print(theta.shape)\n",
        "# print(theta)\n",
        "# print(embedding_matrix)\n",
        "# print(word_dot_topic)\n",
        "print(beta.shape)\n",
        "print(beta)\n",
        "print(log_theta_dot_beta)\n",
        "print(torch.exp(log_theta_dot_beta_normalized))\n"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([14, 4])\n",
            "tensor([[0.3401, 0.2617, 0.1955, 0.2027],\n",
            "        [0.0803, 0.1339, 0.6496, 0.1362],\n",
            "        [0.1823, 0.2116, 0.3501, 0.2560],\n",
            "        [0.2025, 0.2731, 0.2038, 0.3205],\n",
            "        [0.2845, 0.1723, 0.3624, 0.1809],\n",
            "        [0.3429, 0.2672, 0.1781, 0.2118],\n",
            "        [0.3087, 0.0926, 0.4911, 0.1075],\n",
            "        [0.4425, 0.1420, 0.1383, 0.2771],\n",
            "        [0.0997, 0.5205, 0.0475, 0.3323],\n",
            "        [0.2193, 0.2567, 0.1770, 0.3470],\n",
            "        [0.2371, 0.2123, 0.4225, 0.1282],\n",
            "        [0.2663, 0.1078, 0.4305, 0.1954],\n",
            "        [0.2647, 0.2212, 0.3442, 0.1700],\n",
            "        [0.1429, 0.4773, 0.1642, 0.2157]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[-1.3482, -1.6479, -1.4619, -1.3612, -1.4926, -1.3389, -1.6200, -1.4568,\n",
            "         -1.1139, -1.3719, -1.4637, -1.5954, -1.4334, -1.1614],\n",
            "        [-1.5109, -1.1875, -1.2897, -1.3329, -1.3813, -1.5150, -1.3666, -1.4759,\n",
            "         -1.4383, -1.3252, -1.3899, -1.3060, -1.4082, -1.4945]],\n",
            "       grad_fn=<LogBackward>)\n",
            "tensor([[0.5406, 0.3869, 0.4571, 0.4929, 0.4722, 0.5439, 0.4370, 0.5048, 0.5804,\n",
            "         0.4883, 0.4816, 0.4282, 0.4937, 0.5825],\n",
            "        [0.4594, 0.6131, 0.5429, 0.5071, 0.5278, 0.4561, 0.5630, 0.4952, 0.4196,\n",
            "         0.5117, 0.5184, 0.5718, 0.5063, 0.4175]], grad_fn=<ExpBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}