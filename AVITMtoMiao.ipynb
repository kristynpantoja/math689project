{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AVITMtoMiao.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristynpantoja/math689project/blob/master/AVITMtoMiao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GABqU8qMLuin",
        "colab_type": "code",
        "outputId": "d9e19b7f-dfcc-4aec-ff6f-7f59b8895b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 23kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5a25a000 @  0x7f7b19b5e2a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qwuFwsAW9IN_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4_9vzhpQ9h_U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ProdLDA(nn.Module):\n",
        "\n",
        "    def __init__(self, net_arch):\n",
        "        super(ProdLDA, self).__init__()\n",
        "        ac = net_arch\n",
        "        self.net_arch = net_arch\n",
        "        # encoder\n",
        "        self.en1_fc     = nn.Linear(ac.num_input, ac.en1_units)             # 1995 -> 100\n",
        "        self.en2_fc     = nn.Linear(ac.en1_units, ac.en2_units)             # 100  -> 100\n",
        "        self.en2_drop   = nn.Dropout(0.2)\n",
        "        self.mean_fc    = nn.Linear(ac.en2_units, ac.num_topic)             # 100  -> 50\n",
        "        self.mean_bn    = nn.BatchNorm1d(ac.num_topic)                      # bn for mean\n",
        "        self.logvar_fc  = nn.Linear(ac.en2_units, ac.num_topic)             # 100  -> 50\n",
        "        self.logvar_bn  = nn.BatchNorm1d(ac.num_topic)                      # bn for logvar\n",
        "        # z\n",
        "        self.p_drop     = nn.Dropout(0.2)\n",
        "        # decoder\n",
        "        self.decoder    = nn.Linear(ac.num_topic, ac.num_input)             # 50   -> 1995\n",
        "        self.decoder_bn = nn.BatchNorm1d(ac.num_input)                      # bn for decoder\n",
        "        # prior mean and variance as constant buffers\n",
        "        prior_mean   = torch.Tensor(1, ac.num_topic).fill_(0)\n",
        "        prior_var    = torch.Tensor(1, ac.num_topic).fill_(ac.variance)\n",
        "        prior_logvar = prior_var.log()\n",
        "        self.register_buffer('prior_mean',    prior_mean)\n",
        "        self.register_buffer('prior_var',     prior_var)\n",
        "        self.register_buffer('prior_logvar',  prior_logvar)\n",
        "        # initialize decoder weight\n",
        "        if ac.init_mult != 0:\n",
        "            #std = 1. / math.sqrt( ac.init_mult * (ac.num_topic + ac.num_input))\n",
        "            self.decoder.weight.data.uniform_(0, ac.init_mult)\n",
        "        # remove BN's scale parameters\n",
        "#         self.logvar_bn .register_parameter('weight', None)\n",
        "#         self.mean_bn   .register_parameter('weight', None)\n",
        "#         self.decoder_bn.register_parameter('weight', None)\n",
        "#         self.decoder_bn.register_parameter('weight', None)\n",
        "\n",
        "\n",
        "        ###\n",
        "        self.word_embedding = nn.Embedding(ac.num_input, 50) # decoder\n",
        "        self.word_embedding_bn = nn.BatchNorm1d(50)\n",
        "        self.topic_embedding = nn.Embedding(ac.num_topic, 50) # decoder\n",
        "        self.topic_embedding_bn = nn.BatchNorm1d(50)\n",
        "        \n",
        "        self.beta = torch.zeros([ac.num_topic, ac.num_input], dtype = torch.float32) # decoder\n",
        "        ###\n",
        "\n",
        "        \n",
        "    def forward(self, input, compute_loss=False, avg_loss=True):\n",
        "        # compute posterior\n",
        "        assert input.shape[1] == doc_term_matrix_tensor.shape[1], \"input isn't batch size x vocab size\"\n",
        "        en1 = F.softplus(self.en1_fc(input))                            # en1_fc   output\n",
        "        en2 = F.softplus(self.en2_fc(en1))                              # encoder2 output\n",
        "        en2 = self.en2_drop(en2)\n",
        "        posterior_mean   = self.mean_bn  (self.mean_fc  (en2))          # posterior mean\n",
        "        posterior_logvar = self.logvar_bn(self.logvar_fc(en2))          # posterior log variance\n",
        "        posterior_var    = posterior_logvar.exp()\n",
        "        # take sample\n",
        "        eps = Variable(input.data.new().resize_as_(posterior_mean.data).normal_()) # noise\n",
        "        z = posterior_mean + posterior_var.sqrt() * eps                 # reparameterization\n",
        "        assert z.shape[1] == self.net_arch.num_topic, \"hidden variable z (from TR) isn't batch size x num_topic\"\n",
        "        p = F.softmax(z)                                                # mixture probability\n",
        "        p = self.p_drop(p)\n",
        "        assert p.shape[1] == self.net_arch.num_topic, \"p (theta) isn't same size as z\"\n",
        "        # do reconstruction\n",
        "        word_vec = self.word_embedding_bn(self.word_embedding.weight)\n",
        "        topic_vec = self.topic_embedding_bn(self.topic_embedding.weight)\n",
        "        self.beta = F.softmax(word_vec.mm(topic_vec.t()), dim = 0) # Vx100 times 100xK => beta is VxK\n",
        "        recon = p.mm(self.beta.t())         # reconstructed distribution over vocabulary\n",
        "        # p is batchxK so batchxK times KxV => batchxV\n",
        "        assert input.shape[1] == doc_term_matrix_tensor.shape[1], \"output isn't batch size x vocab size\"\n",
        "        \n",
        "        if compute_loss:\n",
        "            return recon, self.loss(input, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
        "        else:\n",
        "            return recon\n",
        "\n",
        "    def loss(self, input, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
        "        # NL\n",
        "        NL  = -(input * (recon+1e-10).log()).sum(1)\n",
        "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
        "        # https://arxiv.org/pdf/1703.01488.pdf\n",
        "        prior_mean   = Variable(self.prior_mean).expand_as(posterior_mean)\n",
        "        prior_var    = Variable(self.prior_var).expand_as(posterior_mean)\n",
        "        prior_logvar = Variable(self.prior_logvar).expand_as(posterior_mean)\n",
        "        var_division    = posterior_var  / prior_var\n",
        "        diff            = posterior_mean - prior_mean\n",
        "        diff_term       = diff * diff / prior_var\n",
        "        logvar_division = prior_logvar - posterior_logvar\n",
        "        # put KLD together\n",
        "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.net_arch.num_topic )\n",
        "        # loss\n",
        "        loss = (NL + KLD)\n",
        "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
        "        if avg:\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss\n",
        "          \n",
        "def train():\n",
        "    for epoch in range(args.num_epoch):\n",
        "        all_indices = torch.randperm(doc_term_matrix_tensor.size(0)).split(args.batch_size)\n",
        "        loss_epoch = 0.0\n",
        "        model.train()                   # switch to training mode\n",
        "        for batch_indices in all_indices:\n",
        "            if not args.nogpu: batch_indices = batch_indices.cuda()\n",
        "            input = Variable(doc_term_matrix_tensor[batch_indices])\n",
        "#             print(batch_indices.shape)\n",
        "#             print(input.shape)\n",
        "            recon, loss = model(input, compute_loss=True)\n",
        "            # optimize\n",
        "            optimizer.zero_grad()       # clear previous gradients\n",
        "            loss.backward()             # backprop\n",
        "            optimizer.step()            # update parameters\n",
        "            # report\n",
        "            loss_epoch += loss.data[0]    # add loss to loss_epoch\n",
        "        if epoch % 5 == 0:\n",
        "            print('Epoch {}, loss={}'.format(epoch, loss_epoch / len(all_indices)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l8VEfNT29xhT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jVr-x0Bk9o0E",
        "colab_type": "code",
        "outputId": "c160257a-aa75-4704-9cdb-f08fa3cf9102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# categories = ['talk.politics.guns', 'sci.space', 'soc.religion.christian',\n",
        "#               'misc.forsale', 'rec.sport.baseball', 'comp.sys.mac.hardware']\n",
        "categories = ['talk.politics.guns', 'sci.space']\n",
        "# newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YzHUuVzk1x1_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words = 'english', min_df=.01, max_df=0.9, \n",
        "                             token_pattern = u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
        "count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
        "doc_term_matrix = count_vecs.toarray()\n",
        "doc_term_matrix.shape # number of documents, number of words (in vocab)\n",
        "tokenizer = vectorizer.build_tokenizer()\n",
        "\n",
        "# note: vectorizer.get_feature_names() != vectorizer.vocabulary_\n",
        "\n",
        "doc_term_matrix_tensor = torch.from_numpy(doc_term_matrix).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TI6oVfEx91SQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from types import SimpleNamespace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mq3D13vc-GpU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_dict = {\"en1_units\" : 100, \"en2_units\" : 100, \"num_topic\" : 50, \n",
        "             \"batch_size\" : 200, \"optimizer\" : 80, \"learning_rate\" : 0.002, \n",
        "             \"momentum\" : 0.99, \"num_epoch\" : 80, \"init_mult\" : 1, \n",
        "             \"variance\" : 0.995, \"start\" : True, \"nogpu\" : True}\n",
        "args = SimpleNamespace(**args_dict)\n",
        "args.num_input = doc_term_matrix_tensor.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3LH7tOWIAGKd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = ProdLDA(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hHEE0EtRAnq6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), args.learning_rate, betas=(args.momentum, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7rfgjUfmAnua",
        "colab_type": "code",
        "outputId": "5ce8573b-3864-4f97-b718-5e8228f417bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:61: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:115: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss=624.6458129882812\n",
            "Epoch 5, loss=566.2608032226562\n",
            "Epoch 10, loss=559.2239379882812\n",
            "Epoch 15, loss=556.8187866210938\n",
            "Epoch 20, loss=558.3740844726562\n",
            "Epoch 25, loss=555.32275390625\n",
            "Epoch 30, loss=558.042236328125\n",
            "Epoch 35, loss=553.9308471679688\n",
            "Epoch 40, loss=555.9872436523438\n",
            "Epoch 45, loss=551.849365234375\n",
            "Epoch 50, loss=550.748779296875\n",
            "Epoch 55, loss=550.667236328125\n",
            "Epoch 60, loss=549.7816772460938\n",
            "Epoch 65, loss=552.714111328125\n",
            "Epoch 70, loss=549.1235961914062\n",
            "Epoch 75, loss=547.1510009765625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hdp1baXqAnx3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "associations = {\n",
        "    'jesus': ['prophet', 'jesus', 'matthew', 'christ', 'worship', 'church'],\n",
        "    'comp ': ['floppy', 'windows', 'microsoft', 'monitor', 'workstation', 'macintosh', \n",
        "              'printer', 'programmer', 'colormap', 'scsi', 'jpeg', 'compression'],\n",
        "    'car  ': ['wheel', 'tire'],\n",
        "    'polit': ['amendment', 'libert', 'regulation', 'president'],\n",
        "    'crime': ['violent', 'homicide', 'rape'],\n",
        "    'midea': ['lebanese', 'israel', 'lebanon', 'palest'],\n",
        "    'sport': ['coach', 'hitter', 'pitch'],\n",
        "    'gears': ['helmet', 'bike'],\n",
        "    'nasa ': ['orbit', 'spacecraft'],\n",
        "}\n",
        "def identify_topic_in_line(line):\n",
        "    topics = []\n",
        "    for topic, keywords in associations.items():\n",
        "        for word in keywords:\n",
        "            if word in line:\n",
        "                topics.append(topic)\n",
        "                break\n",
        "    return topics\n",
        "def print_top_words(beta, feature_names, n_top_words=10):\n",
        "    print('---------------Printing the Topics------------------')\n",
        "    for i in range(len(beta)):\n",
        "        line = \" \".join([feature_names[j] \n",
        "                            for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
        "        topics = identify_topic_in_line(line)\n",
        "        print('|'.join(topics))\n",
        "        print('     {}'.format(line))\n",
        "    print('---------------End of Topics------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0W3ZjwSL41KP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sorted(vectorizer.vocabulary_, key = lambda x: x[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TyiShF-JIyss",
        "colab_type": "code",
        "outputId": "cfdd957c-33a1-4456-8376-6c18b9202123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "emb = model.beta.detach().numpy().T\n",
        "print(\"shape of beta is \" + str(emb.shape))\n",
        "print_top_words(emb, vectorizer.get_feature_names(), n_top_words = 20)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of beta is (6, 1739)\n",
            "---------------Printing the Topics------------------\n",
            "comp \n",
            "     edu com article writes host nntp posting university max like just distribution know don reply good windows thanks think computer\n",
            "\n",
            "     edu com article writes like don just think know people university time does new use posting good way right make\n",
            "\n",
            "     people don edu god use know like just com think time say said does new right writes information make did\n",
            "\n",
            "     edu com writes max article host nntp posting university like just distribution reply know good don thanks drive new usa\n",
            "\n",
            "     edu people com don just use like know think writes time does article new god make say way right did\n",
            "\n",
            "     people edu com use don like think god know just time does way say said new make article file information\n",
            "---------------End of Topics------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nTeSabr7K3qo",
        "colab_type": "code",
        "outputId": "e295fbb1-1c88-480a-bc58-be1170c8c75e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "cell_type": "code",
      "source": [
        "beta = model.decoder.weight.detach()#.softmax(0)\n",
        "print(beta.sum(0))\n",
        "print(beta.shape)\n",
        "_, ind = torch.sort(beta, 0)\n",
        "print(ind.shape)\n",
        "# ind.numpy()[0:50, 0] - ind.numpy()[0:50, 1]\n",
        "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 0])\n",
        "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 1])\n",
        "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 2])\n",
        "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 3])\n",
        "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 4])\n",
        "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 5])\n",
        "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 6])\n",
        "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 7])\n",
        "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 8])\n",
        "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 9])\n",
        "\n",
        "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:25, 4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([691.0438, 749.8492, 650.2081, 758.7484, 738.9827, 847.6075, 663.5479,\n",
            "        851.0903, 808.6369, 837.2359, 813.2128, 679.8199, 592.9649, 769.5212,\n",
            "        881.0038, 834.2672, 849.1828, 854.6403, 863.4843, 860.0076, 687.0235,\n",
            "        715.2632, 702.8358, 867.0287, 775.7951, 790.4180, 842.8163, 840.6353,\n",
            "        823.6600, 806.1698, 751.1285, 817.8627, 761.5077, 688.9834, 677.0889,\n",
            "        675.3782, 781.6658, 829.0858, 625.1631, 790.7856, 818.1710, 876.3959,\n",
            "        644.7657, 797.1841, 760.3964, 860.3668, 661.1928, 840.3171, 735.9488,\n",
            "        763.0490])\n",
            "torch.Size([1739, 50])\n",
            "torch.Size([1739, 50])\n",
            "['normal' 'protect' 'kill' 'dave' 'function' 'cost' 'unfortunately'\n",
            " 'years' 'tell' 'normally' 'answer' 'navy' 'background' 'saw' 'room'\n",
            " 'keeping' 'uses' 'sent' 'brought' 'wait' 'readers' 'issues' 'members'\n",
            " 'order' 'steve']\n",
            "['resources' 'equivalent' 'looking' 'errors' 'notice' 'includes' 'size'\n",
            " 'lives' 'models' 'project' 'face' 'gateway' 'books' 'possibly' 'options'\n",
            " 'make' 'earth' 'mass' 'tape' 'currently' 'things' 'live' 'blue'\n",
            " 'installed' 'example']\n",
            "['notice' 'load' 'knew' 'house' 'main' 'saw' 'budget' 'hand' 'hours'\n",
            " 'effect' 'readers' 'fax' 'document' 'choice' 'technology' 'actions'\n",
            " 'kill' 'body' 'heard' 'natural' 'deleted' 'turkish' 'level' 'practice'\n",
            " 'pin']\n",
            "['cheap' 'jim' 'deal' 'released' 'watching' 'tape' 'mention' 'claims'\n",
            " 'began' 'time' 'dan' 'possibly' 'begin' 'noticed' 'star' 'account'\n",
            " 'francisco' 'sciences' 'research' 'means' 'prices' 'law' 'considering'\n",
            " 'helpful' 'messages']\n",
            "['owners' 'believe' 'unless' 'multi' 'sent' 'mention' 'andrew' 'public'\n",
            " 'death' 'possibility' 'means' 'bought' 'applications' 'mistake' 'running'\n",
            " 'united' 'international' 'different' 'bus' 'unfortunately' 'point'\n",
            " 'cheap' 'deal' 'room' 'hour']\n",
            "['welcome' 'game' 'double' 'possibly' 'summer' 'mistake' 'quite' 'various'\n",
            " 'criminal' 'great' 'test' 'richard' 'room' 'certainly' 'colorado'\n",
            " 'continue' 'light' 'loss' 'mike' 'quote' 'applications' 'deleted' 'went'\n",
            " 'newsgroup' 'saw']\n",
            "['sci' 'load' 'traffic' 'level' 'effects' 'heard' 'house' 'joseph'\n",
            " 'changing' 'create' 'switch' 'provide' 'technology' 'nyx' 'children'\n",
            " 'quote' 'wife' 'notice' 'bob' 'standards' 'billion' 'particular' 'folks'\n",
            " 'atlanta' 'ted']\n",
            "['activity' 'sexual' 'currently' 'face' 'march' 'shouldn' 'ide' 'names'\n",
            " 'includes' 'word' 'risk' 'came' 'sign' 'active' 'mark' 'activities'\n",
            " 'notice' 'mass' 'come' 'produce' 'sick' 'criminal' 'present' 'looking'\n",
            " 'words']\n",
            "['united' 'cheap' 'rutgers' 'particularly' 'armed' 'great' 'double'\n",
            " 'decision' 'engineer' 'owners' 'unless' 'public' 'deal' 'mention' 'real'\n",
            " 'various' 'enforcement' 'park' 'loss' 'needs' 'chris' 'working' 'went'\n",
            " 'million' 'freenet']\n",
            "['notice' 'includes' 'carnegie' 'history' 'led' 'fast' 'cross' 'mean'\n",
            " 'honest' 'trade' 'pre' 'wall' 'michael' 'connect' 'allowed' 'bus'\n",
            " 'resources' 'march' 'univ' 'moon' 'quoted' 'montreal' 'sexual' 'face'\n",
            " 'uses']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gry54SsunU0B",
        "colab_type": "code",
        "outputId": "56856162-5185-4907-e8be-d227024892ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "print(model.decoder.weight)\n",
        "model.decoder.weight.data.cpu()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.6879, 0.2637, 0.0701,  ..., 0.6469, 0.4965, 0.9987],\n",
            "        [0.2294, 0.0525, 0.0407,  ..., 0.2914, 0.8304, 0.1538],\n",
            "        [0.1853, 0.3867, 0.4497,  ..., 0.9726, 0.5935, 0.1058],\n",
            "        ...,\n",
            "        [0.0036, 0.3131, 0.8020,  ..., 0.0704, 0.1168, 0.8991],\n",
            "        [0.0182, 0.4061, 0.5788,  ..., 0.2914, 0.6085, 0.9265],\n",
            "        [0.7966, 0.5985, 0.6315,  ..., 0.0715, 0.7860, 0.9757]],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6879, 0.2637, 0.0701,  ..., 0.6469, 0.4965, 0.9987],\n",
              "        [0.2294, 0.0525, 0.0407,  ..., 0.2914, 0.8304, 0.1538],\n",
              "        [0.1853, 0.3867, 0.4497,  ..., 0.9726, 0.5935, 0.1058],\n",
              "        ...,\n",
              "        [0.0036, 0.3131, 0.8020,  ..., 0.0704, 0.1168, 0.8991],\n",
              "        [0.0182, 0.4061, 0.5788,  ..., 0.2914, 0.6085, 0.9265],\n",
              "        [0.7966, 0.5985, 0.6315,  ..., 0.0715, 0.7860, 0.9757]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 284
        }
      ]
    },
    {
      "metadata": {
        "id": "nLdV3bB4N0eX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def topic_coherence(beta, M, doc_term_matrix):\n",
        "  K = beta.shape[1] # beta has dim V x K\n",
        "  coherences = np.zeros(K)\n",
        "  for t in range(K):\n",
        "    index = np.argsort(-beta[:, t])[0:M]\n",
        "    cart_prod = product(list(index), list(index))\n",
        "    for ind1, ind2 in cart_prod:\n",
        "      if ind1 == ind2:\n",
        "        pass\n",
        "      else:\n",
        "        d_ind1 = (doc_term_matrix[:, ind1] > 0).sum()\n",
        "        d_ind12 = ((doc_term_matrix[:, ind1] > 0) & (doc_term_matrix[:, ind2] > 0)).sum()\n",
        "        coherences[t] += np.log1p(d_ind12) - np.log(d_ind1)\n",
        "\n",
        "  return coherences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5kRl6b_pOH_0",
        "colab_type": "code",
        "outputId": "6492d0a1-463a-4b06-b5bb-6c5d8e4f8267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "topic_coherence(emb.T, 20, doc_term_matrix)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-500.25683263, -394.54622521, -457.73329493, -499.41127946,\n",
              "       -419.44022283, -485.29432162])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "be2bpL_qOJWD",
        "colab_type": "code",
        "outputId": "ad0a3f21-f038-4a77-acec-0e6eef2d6f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sorted(vectorizer.vocabulary_, key = lambda x: x[1])[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'car'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 338
        }
      ]
    },
    {
      "metadata": {
        "id": "SbCZ3Wz-ONfd",
        "colab_type": "code",
        "outputId": "e303429c-2e2d-4964-82aa-865c6cbe5488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "vocab = {\"hi\": 13, \"bye\": 2, \"hello\": 3}\n",
        "foo = zip(*sorted(vocab.items(), key = lambda x: x[1]))\n",
        "list(foo)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bye', 'hello', 'hi'), (2, 3, 13)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 305
        }
      ]
    },
    {
      "metadata": {
        "id": "LhHX1qIm7WF_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_top_words(beta, feature_names, n_top_words=10):\n",
        "    print('---------------Printing the Topics------------------')\n",
        "    for i in range(len(beta)): # for all the rows (words in vocab) in beta,\n",
        "        line = \" \".join([feature_names[j] \n",
        "                            for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
        "#         topics = identify_topic_in_line(line)\n",
        "#         print('|'.join(topics))\n",
        "        print('     {}'.format(line))\n",
        "    print('---------------End of Topics------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7-yqzxLIBvbF",
        "colab_type": "code",
        "outputId": "ca99d547-6f85-4fd7-fae2-effdf6422503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "cell_type": "code",
      "source": [
        "print_top_words(beta.numpy().T, sorted(vectorizer.vocabulary_, key = lambda x: x[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------Printing the Topics------------------\n",
            "     view bear normal false fear waco arms mind english lot\n",
            "     bit fact choice circuit budget trouble communication technical jews leads\n",
            "     away mind happened america fear univ pick looking resources changes\n",
            "     thread equal christ bet claim mil important greatly facts player\n",
            "     carried store microsoft race helpful tax methods million england logic\n",
            "     parts suggest works poster distribution german capable circuit serial peter\n",
            "     trouble electrical bit fact choice field facts hot approach flame\n",
            "     christ jesus start images face mil purchase toronto exists mike\n",
            "     written wants newsgroups gas vice refer discussed carnegie brian faster\n",
            "     face kept christ turkish studies type bother apart start current\n",
            "     connection carried early tax blue million school large documentation background\n",
            "     state trouble past bit recognize driving people fact communication accepted\n",
            "     serial references athos circuit poster lack happen realize state columbia\n",
            "     near black driver online developed hand morning hour words louis\n",
            "     distribution college humans reality circuit explain athena software way references\n",
            "     dec open produced quite pub brand damn hopefully wings domain\n",
            "     mind away happened lot tape gone basically false stats article\n",
            "     carried million saw outside early microsoft school tax drug accepted\n",
            "     little serial face columbia board references files engine future grant\n",
            "     mind columbia references produce entire tech stats let received away\n",
            "     columbia references true marc ins files happen america poster difficult\n",
            "     columbia false received mind value stats special files modern steven\n",
            "     starts chris help looked mass currently standard essentially avoid proof\n",
            "     false leave mind direction lot fear rich away network news\n",
            "     equivalent summary graphics got think near lord language change signal\n",
            "     suggest past explain face tek purchase red bit source center\n",
            "     circuit quoted past turkish seattle state tek produce capable series\n",
            "     lot false mind rich protection waco bear view super gone\n",
            "     past circuit explain tek bit billion games pass suggest william\n",
            "     turkish references poster tek peter current athos distribution past absolutely\n",
            "     columbia future special mind files fight conditions quick america leave\n",
            "     references suggest state pass communication spent logic world reading thomas\n",
            "     face christ past start apart sgi supported copy bother purchase\n",
            "     join notice want noted spent potential win green update ins\n",
            "     lot driven rich don definition bear view unable working waco\n",
            "     computing die follows manager resolution algorithm evil opinions program europe\n",
            "     circuit bit technical past budget suggest explain wife facts store\n",
            "     references mind distribution town reality basically sure stats columbia hill\n",
            "     special future fight value evidence scale street protect news mind\n",
            "     serial state references peter produce entire mountain moved pass parts\n",
            "     false mind direction lot rich network leave fear usenet happening\n",
            "     input update freenet refer vice hard position previous argument boy\n",
            "     open drug wife accepted clock fact past flame people carried\n",
            "     upgrade blame claims color black developed kevin ontario lived surface\n",
            "     kept ride happen engine turkish face waiting especially total columbia\n",
            "     absolutely foreign state suggest grant stats ins hot single argument\n",
            "     today required circuit vice acs causes position wants really clipper\n",
            "     turkish references engine current value happen prove board street total\n",
            "     references serial poster warning distribution technical shows explain especially canada\n",
            "     america distribution view little absolutely current memory service especially appreciate\n",
            "---------------End of Topics------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eevlzc-FCAAV",
        "colab_type": "code",
        "outputId": "e0f9b3b9-c559-4dcc-9b34-88e5e811a51a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17034
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['car',\n",
              " 'park',\n",
              " 'saw',\n",
              " 'day',\n",
              " 'late',\n",
              " 'early',\n",
              " 'called',\n",
              " 'mail',\n",
              " 'washington',\n",
              " 'fair',\n",
              " 'cards',\n",
              " 'days',\n",
              " 'base',\n",
              " 'haven',\n",
              " 'mac',\n",
              " 'gave',\n",
              " 'way',\n",
              " 'market',\n",
              " 'machine',\n",
              " 'maybe',\n",
              " 'make',\n",
              " 'taking',\n",
              " 'machines',\n",
              " 'daily',\n",
              " 'dangerous',\n",
              " 'far',\n",
              " 'harvard',\n",
              " 'launch',\n",
              " 'cambridge',\n",
              " 'warning',\n",
              " 'basically',\n",
              " 'values',\n",
              " 'lawrence',\n",
              " 'makes',\n",
              " 'mass',\n",
              " 'say',\n",
              " 'hard',\n",
              " 'gas',\n",
              " 'hands',\n",
              " 'says',\n",
              " 'easily',\n",
              " 'later',\n",
              " 'magazine',\n",
              " 'faster',\n",
              " 'range',\n",
              " 'fast',\n",
              " 'data',\n",
              " 'facts',\n",
              " 'said',\n",
              " 'hardware',\n",
              " 'fault',\n",
              " 'want',\n",
              " 'david',\n",
              " 'james',\n",
              " 'main',\n",
              " 'saying',\n",
              " 'case',\n",
              " 'fall',\n",
              " 'man',\n",
              " 'wants',\n",
              " 'hand',\n",
              " 'faith',\n",
              " 'happy',\n",
              " 'water',\n",
              " 'packard',\n",
              " 'east',\n",
              " 'nasa',\n",
              " 'vax',\n",
              " 'mark',\n",
              " 'based',\n",
              " 'management',\n",
              " 'lab',\n",
              " 'language',\n",
              " 'major',\n",
              " 'sale',\n",
              " 'tape',\n",
              " 'happened',\n",
              " 'war',\n",
              " 'caused',\n",
              " 'making',\n",
              " 'save',\n",
              " 'fact',\n",
              " 'page',\n",
              " 'takes',\n",
              " 'handle',\n",
              " 'rates',\n",
              " 'cars',\n",
              " 'paying',\n",
              " 'taken',\n",
              " 'rate',\n",
              " 'pay',\n",
              " 'dan',\n",
              " 'damn',\n",
              " 'law',\n",
              " 'calls',\n",
              " 'handling',\n",
              " 'happen',\n",
              " 'navy',\n",
              " 'california',\n",
              " 'san',\n",
              " 'date',\n",
              " 'calling',\n",
              " 'maintain',\n",
              " 'careful',\n",
              " 'factor',\n",
              " 'card',\n",
              " 'background',\n",
              " 'failed',\n",
              " 'matter',\n",
              " 'caltech',\n",
              " 'pasadena',\n",
              " 'gary',\n",
              " 'wasn',\n",
              " 'mailing',\n",
              " 'waste',\n",
              " 'fax',\n",
              " 'fairly',\n",
              " 'laboratory',\n",
              " 'macintosh',\n",
              " 'care',\n",
              " 'names',\n",
              " 'wayne',\n",
              " 'cases',\n",
              " 'game',\n",
              " 'table',\n",
              " 'half',\n",
              " 'earth',\n",
              " 'paul',\n",
              " 'manner',\n",
              " 'safety',\n",
              " 'ray',\n",
              " 'radio',\n",
              " 'value',\n",
              " 'damage',\n",
              " 'face',\n",
              " 'came',\n",
              " 'pat',\n",
              " 'father',\n",
              " 'cause',\n",
              " 'past',\n",
              " 'land',\n",
              " 'wanted',\n",
              " 'easy',\n",
              " 'party',\n",
              " 'march',\n",
              " 'race',\n",
              " 'national',\n",
              " 'majority',\n",
              " 'large',\n",
              " 'hate',\n",
              " 'tax',\n",
              " 'batf',\n",
              " 'having',\n",
              " 'wait',\n",
              " 'raised',\n",
              " 'talk',\n",
              " 'gatech',\n",
              " 'carried',\n",
              " 'carry',\n",
              " 'safe',\n",
              " 'jack',\n",
              " 'parts',\n",
              " 'fans',\n",
              " 'baseball',\n",
              " 'basis',\n",
              " 'dave',\n",
              " 'bad',\n",
              " 'games',\n",
              " 'carnegie',\n",
              " 'max',\n",
              " 'nature',\n",
              " 'talking',\n",
              " 'hall',\n",
              " 'ways',\n",
              " 'manual',\n",
              " 'watching',\n",
              " 'faq',\n",
              " 'nation',\n",
              " 'various',\n",
              " 'path',\n",
              " 'pass',\n",
              " 'sample',\n",
              " 'jason',\n",
              " 'math',\n",
              " 'canada',\n",
              " 'causes',\n",
              " 'lack',\n",
              " 'false',\n",
              " 'pages',\n",
              " 'particular',\n",
              " 'wall',\n",
              " 'fail',\n",
              " 'van',\n",
              " 'earlier',\n",
              " 'carrying',\n",
              " 'matthew',\n",
              " 'watch',\n",
              " 'dale',\n",
              " 'bank',\n",
              " 'gateway',\n",
              " 'cable',\n",
              " 'manager',\n",
              " 'laws',\n",
              " 'waco',\n",
              " 'paper',\n",
              " 'larger',\n",
              " 'fan',\n",
              " 'family',\n",
              " 'daniel',\n",
              " 'ram',\n",
              " 'package',\n",
              " 'ran',\n",
              " 'particularly',\n",
              " 'waiting',\n",
              " 'magnus',\n",
              " 'patrick',\n",
              " 'latest',\n",
              " 'hasn',\n",
              " 'happens',\n",
              " 'capable',\n",
              " 'pain',\n",
              " 'happening',\n",
              " 'eat',\n",
              " 'material',\n",
              " 'paid',\n",
              " 'familiar',\n",
              " 'easier',\n",
              " 'basic',\n",
              " 'caught',\n",
              " 'passed',\n",
              " 'hardly',\n",
              " 'banks',\n",
              " 'natural',\n",
              " 'larry',\n",
              " 'random',\n",
              " 'capital',\n",
              " 'valid',\n",
              " 'vancouver',\n",
              " 'marc',\n",
              " 'ball',\n",
              " 'ibm',\n",
              " 'absolute',\n",
              " 'able',\n",
              " 'obviously',\n",
              " 'obvious',\n",
              " 'objective',\n",
              " 'bbs',\n",
              " 'fbi',\n",
              " 'object',\n",
              " 'ability',\n",
              " 'absolutely',\n",
              " 'obtain',\n",
              " 'ecn',\n",
              " 'access',\n",
              " 'active',\n",
              " 'actually',\n",
              " 'sci',\n",
              " 'scsi',\n",
              " 'actual',\n",
              " 'activities',\n",
              " 'ucs',\n",
              " 'account',\n",
              " 'cco',\n",
              " 'scott',\n",
              " 'school',\n",
              " 'lcs',\n",
              " 'science',\n",
              " 'accepted',\n",
              " 'acts',\n",
              " 'scheme',\n",
              " 'act',\n",
              " 'action',\n",
              " 'actions',\n",
              " 'occur',\n",
              " 'ice',\n",
              " 'accurate',\n",
              " 'scientific',\n",
              " 'activity',\n",
              " 'screen',\n",
              " 'scale',\n",
              " 'economic',\n",
              " 'acs',\n",
              " 'according',\n",
              " 'accept',\n",
              " 'acceptable',\n",
              " 'sciences',\n",
              " 'academic',\n",
              " 'edu',\n",
              " 'addition',\n",
              " 'add',\n",
              " 'advance',\n",
              " 'address',\n",
              " 'ideas',\n",
              " 'ide',\n",
              " 'added',\n",
              " 'additional',\n",
              " 'advice',\n",
              " 'administration',\n",
              " 'advantage',\n",
              " 'admit',\n",
              " 'idea',\n",
              " 'advanced',\n",
              " 'edge',\n",
              " 'edward',\n",
              " 'adams',\n",
              " 'education',\n",
              " 'really',\n",
              " 'separate',\n",
              " 'rest',\n",
              " 'years',\n",
              " 'reports',\n",
              " 'keywords',\n",
              " 'send',\n",
              " 'message',\n",
              " 'network',\n",
              " 'new',\n",
              " 'heard',\n",
              " 'went',\n",
              " 'recently',\n",
              " 'feel',\n",
              " 'better',\n",
              " 'people',\n",
              " 'realize',\n",
              " 'real',\n",
              " 'helpful',\n",
              " 'news',\n",
              " 'reading',\n",
              " 'newsreader',\n",
              " 'version',\n",
              " 'level',\n",
              " 'requires',\n",
              " 'person',\n",
              " 'sense',\n",
              " 'head',\n",
              " 'memory',\n",
              " 'yes',\n",
              " 'meaning',\n",
              " 'set',\n",
              " 'tell',\n",
              " 'second',\n",
              " 'weapons',\n",
              " 'need',\n",
              " 'result',\n",
              " 'bear',\n",
              " 'believe',\n",
              " 'keeping',\n",
              " 'term',\n",
              " 'defined',\n",
              " 'means',\n",
              " 'year',\n",
              " 'reasonable',\n",
              " 'mean',\n",
              " 'read',\n",
              " 'reply',\n",
              " 'request',\n",
              " 'devices',\n",
              " 'performance',\n",
              " 'versions',\n",
              " 'reach',\n",
              " 'newsgroup',\n",
              " 'report',\n",
              " 'reference',\n",
              " 'review',\n",
              " 'help',\n",
              " 'technology',\n",
              " 'mentioned',\n",
              " 'related',\n",
              " 'hey',\n",
              " 'services',\n",
              " 'jewish',\n",
              " 'jews',\n",
              " 'decide',\n",
              " 'believed',\n",
              " 'required',\n",
              " 'lead',\n",
              " 'necessarily',\n",
              " 'jesus',\n",
              " 'references',\n",
              " 'gets',\n",
              " 'religion',\n",
              " 'sea',\n",
              " 'hewlett',\n",
              " 'regards',\n",
              " 'research',\n",
              " 'center',\n",
              " 'description',\n",
              " 'deleted',\n",
              " 'design',\n",
              " 'received',\n",
              " 'hear',\n",
              " 'ken',\n",
              " 'let',\n",
              " 'removed',\n",
              " 'yesterday',\n",
              " 'reported',\n",
              " 'team',\n",
              " 'teams',\n",
              " 'key',\n",
              " 'features',\n",
              " 'developed',\n",
              " 'vehicle',\n",
              " 'return',\n",
              " 'regular',\n",
              " 'feet',\n",
              " 'lee',\n",
              " 'bet',\n",
              " 'keith',\n",
              " 'method',\n",
              " 'death',\n",
              " 'weren',\n",
              " 'generally',\n",
              " 'certainly',\n",
              " 'led',\n",
              " 'general',\n",
              " 'despite',\n",
              " 'designed',\n",
              " 'deal',\n",
              " 'kevin',\n",
              " 'tech',\n",
              " 'replies',\n",
              " 'security',\n",
              " 'helps',\n",
              " 'record',\n",
              " 'hell',\n",
              " 'decided',\n",
              " 'personal',\n",
              " 'ready',\n",
              " 'reason',\n",
              " 'reasons',\n",
              " 'dealer',\n",
              " 'felt',\n",
              " 'responsible',\n",
              " 'getting',\n",
              " 'personally',\n",
              " 'best',\n",
              " 'seriously',\n",
              " 'seen',\n",
              " 'germany',\n",
              " 'german',\n",
              " 'season',\n",
              " 'yeah',\n",
              " 'sell',\n",
              " 'service',\n",
              " 'left',\n",
              " 'definitely',\n",
              " 'needed',\n",
              " 'hello',\n",
              " 'respond',\n",
              " 'netcom',\n",
              " 'recall',\n",
              " 'health',\n",
              " 'respect',\n",
              " 'leave',\n",
              " 'serve',\n",
              " 'response',\n",
              " 'generation',\n",
              " 'men',\n",
              " 'depends',\n",
              " 'legitimate',\n",
              " 'serial',\n",
              " 'replaced',\n",
              " 'peace',\n",
              " 'define',\n",
              " 'western',\n",
              " 'near',\n",
              " 'members',\n",
              " 'tel',\n",
              " 'week',\n",
              " 'begin',\n",
              " 'remote',\n",
              " 'mention',\n",
              " 'detroit',\n",
              " 'net',\n",
              " 'sending',\n",
              " 'remember',\n",
              " 'fear',\n",
              " 'legal',\n",
              " 'federal',\n",
              " 'red',\n",
              " 'needs',\n",
              " 'rec',\n",
              " 'technical',\n",
              " 'merely',\n",
              " 'depending',\n",
              " 'require',\n",
              " 'series',\n",
              " 'self',\n",
              " 'defense',\n",
              " 'necessary',\n",
              " 'department',\n",
              " 'berkeley',\n",
              " 'ted',\n",
              " 'reserve',\n",
              " 'held',\n",
              " 'leads',\n",
              " 'kept',\n",
              " 'sent',\n",
              " 'certain',\n",
              " 'regardless',\n",
              " 'tend',\n",
              " 'mellon',\n",
              " 'dept',\n",
              " 'zero',\n",
              " 'benefit',\n",
              " 'reality',\n",
              " 'medical',\n",
              " 'medicine',\n",
              " 'refer',\n",
              " 'georgia',\n",
              " 'letter',\n",
              " 'section',\n",
              " 'leaving',\n",
              " 'des',\n",
              " 'remain',\n",
              " 'measure',\n",
              " 'welcome',\n",
              " 'determine',\n",
              " 'described',\n",
              " 'release',\n",
              " 'released',\n",
              " 'resolution',\n",
              " 'results',\n",
              " 'search',\n",
              " 'development',\n",
              " 'dec',\n",
              " 'west',\n",
              " 'terms',\n",
              " 'test',\n",
              " 'centre',\n",
              " 'texas',\n",
              " 'belief',\n",
              " 'reduce',\n",
              " 'religious',\n",
              " 'leaders',\n",
              " 'beliefs',\n",
              " 'began',\n",
              " 'relatively',\n",
              " 'text',\n",
              " 'century',\n",
              " 'nearly',\n",
              " 'heaven',\n",
              " 'weeks',\n",
              " 'regard',\n",
              " 'beginning',\n",
              " 'decision',\n",
              " 'receive',\n",
              " 'dead',\n",
              " 'methods',\n",
              " 'deep',\n",
              " 'setting',\n",
              " 'heavy',\n",
              " 'bell',\n",
              " 'recommend',\n",
              " 'newsgroups',\n",
              " 'member',\n",
              " 'seattle',\n",
              " 'peter',\n",
              " 'secret',\n",
              " 'keys',\n",
              " 'secure',\n",
              " 'responses',\n",
              " 'period',\n",
              " 'media',\n",
              " 'levels',\n",
              " 'details',\n",
              " 'decent',\n",
              " 'meet',\n",
              " 'leading',\n",
              " 'weight',\n",
              " 'remove',\n",
              " 'selling',\n",
              " 'server',\n",
              " 'develop',\n",
              " 'george',\n",
              " 'device',\n",
              " 'seeing',\n",
              " 'relevant',\n",
              " 'behavior',\n",
              " 'learned',\n",
              " 'tells',\n",
              " 'heart',\n",
              " 'messages',\n",
              " 'regarding',\n",
              " 'recent',\n",
              " 'recognize',\n",
              " 'learn',\n",
              " 'league',\n",
              " 'kent',\n",
              " 'setup',\n",
              " 'telling',\n",
              " 'keyboard',\n",
              " 'responsibility',\n",
              " 'perfect',\n",
              " 'telephone',\n",
              " 'central',\n",
              " 'sexual',\n",
              " 'jeff',\n",
              " 'replace',\n",
              " 'perfectly',\n",
              " 'readers',\n",
              " 'sex',\n",
              " 'feeling',\n",
              " 'meant',\n",
              " 'beat',\n",
              " 'resources',\n",
              " 'definition',\n",
              " 'tek',\n",
              " 'office',\n",
              " 'offer',\n",
              " 'effort',\n",
              " 'official',\n",
              " 'effect',\n",
              " 'effective',\n",
              " 'offers',\n",
              " 'afraid',\n",
              " 'effects',\n",
              " 'ignore',\n",
              " 'agree',\n",
              " 'age',\n",
              " 'ago',\n",
              " 'sgi',\n",
              " 'agency',\n",
              " 'pgp',\n",
              " 'agencies',\n",
              " 'vga',\n",
              " 'thing',\n",
              " 'thanks',\n",
              " 'thomas',\n",
              " 'chip',\n",
              " 'phone',\n",
              " 'things',\n",
              " 'checked',\n",
              " 'thousands',\n",
              " 'chicago',\n",
              " 'thank',\n",
              " 'thought',\n",
              " 'shows',\n",
              " 'think',\n",
              " 'change',\n",
              " 'thinking',\n",
              " 'christian',\n",
              " 'christ',\n",
              " 'christianity',\n",
              " 'child',\n",
              " 'children',\n",
              " 'ohio',\n",
              " 'changes',\n",
              " 'short',\n",
              " 'changed',\n",
              " 'nhl',\n",
              " 'cheap',\n",
              " 'chris',\n",
              " 'showing',\n",
              " 'check',\n",
              " 'shall',\n",
              " 'physics',\n",
              " 'shot',\n",
              " 'white',\n",
              " 'thoughts',\n",
              " 'thinks',\n",
              " 'charge',\n",
              " 'charles',\n",
              " 'thread',\n",
              " 'christians',\n",
              " 'church',\n",
              " 'throw',\n",
              " 'changing',\n",
              " 'shown',\n",
              " 'shouldn',\n",
              " 'share',\n",
              " 'chips',\n",
              " 'physical',\n",
              " 'cheaper',\n",
              " 'mhz',\n",
              " 'choice',\n",
              " 'chance',\n",
              " 'theory',\n",
              " 'character',\n",
              " 'ahead',\n",
              " 'choose',\n",
              " 'showed',\n",
              " 'cheers',\n",
              " 'phil',\n",
              " 'shipping',\n",
              " 'history',\n",
              " 'final',\n",
              " 'disk',\n",
              " 'distribution',\n",
              " 'finally',\n",
              " 'life',\n",
              " 'bit',\n",
              " 'line',\n",
              " 'like',\n",
              " 'display',\n",
              " 'size',\n",
              " 'hit',\n",
              " 'time',\n",
              " 'lies',\n",
              " 'division',\n",
              " 'tin',\n",
              " 'nice',\n",
              " 'right',\n",
              " 'fix',\n",
              " 'millions',\n",
              " 'disagree',\n",
              " 'sign',\n",
              " 'killed',\n",
              " 'given',\n",
              " 'directly',\n",
              " 'file',\n",
              " 'list',\n",
              " 'wide',\n",
              " 'digital',\n",
              " 'win',\n",
              " 'figure',\n",
              " 'uiuc',\n",
              " 'fixed',\n",
              " 'oil',\n",
              " 'bike',\n",
              " 'different',\n",
              " 'biblical',\n",
              " 'mind',\n",
              " 'live',\n",
              " 'kind',\n",
              " 'living',\n",
              " 'little',\n",
              " 'bible',\n",
              " 'simply',\n",
              " 'city',\n",
              " 'likely',\n",
              " 'times',\n",
              " 'wings',\n",
              " 'site',\n",
              " 'air',\n",
              " 'mike',\n",
              " 'single',\n",
              " 'high',\n",
              " 'citizens',\n",
              " 'died',\n",
              " 'disease',\n",
              " 'library',\n",
              " 'files',\n",
              " 'minutes',\n",
              " 'big',\n",
              " 'similar',\n",
              " 'situation',\n",
              " 'higher',\n",
              " 'light',\n",
              " 'didn',\n",
              " 'disclaimer',\n",
              " 'killing',\n",
              " 'risk',\n",
              " 'mil',\n",
              " 'view',\n",
              " 'difference',\n",
              " 'did',\n",
              " 'miles',\n",
              " 'lived',\n",
              " 'circuit',\n",
              " 'bitnet',\n",
              " 'signal',\n",
              " 'simple',\n",
              " 'mit',\n",
              " 'window',\n",
              " 'pittsburgh',\n",
              " 'lists',\n",
              " 'windows',\n",
              " 'virginia',\n",
              " 'sites',\n",
              " 'middle',\n",
              " 'bigger',\n",
              " 'hill',\n",
              " 'wish',\n",
              " 'jim',\n",
              " 'michael',\n",
              " 'kids',\n",
              " 'fighting',\n",
              " 'views',\n",
              " 'richard',\n",
              " 'wife',\n",
              " 'willing',\n",
              " 'fight',\n",
              " 'firearms',\n",
              " 'limited',\n",
              " 'pick',\n",
              " 'missing',\n",
              " 'differences',\n",
              " 'giving',\n",
              " 'william',\n",
              " 'tim',\n",
              " 'highly',\n",
              " 'minor',\n",
              " 'pitt',\n",
              " 'die',\n",
              " 'rich',\n",
              " 'night',\n",
              " 'military',\n",
              " 'million',\n",
              " 'gives',\n",
              " 'discovered',\n",
              " 'iii',\n",
              " 'field',\n",
              " 'kill',\n",
              " 'silver',\n",
              " 'civil',\n",
              " 'direction',\n",
              " 'fit',\n",
              " 'michigan',\n",
              " 'digex',\n",
              " 'listen',\n",
              " 'microsoft',\n",
              " 'video',\n",
              " 'rights',\n",
              " 'directory',\n",
              " 'fine',\n",
              " 'picture',\n",
              " 'pin',\n",
              " 'historical',\n",
              " 'cis',\n",
              " 'ride',\n",
              " 'significant',\n",
              " 'direct',\n",
              " 'disks',\n",
              " 'piece',\n",
              " 'riding',\n",
              " 'sick',\n",
              " 'discussion',\n",
              " 'diego',\n",
              " 'king',\n",
              " 'discuss',\n",
              " 'minute',\n",
              " 'discussed',\n",
              " 'difficult',\n",
              " 'vice',\n",
              " 'bits',\n",
              " 'billion',\n",
              " 'lives',\n",
              " 'mistake',\n",
              " 'winning',\n",
              " 'missed',\n",
              " 'title',\n",
              " 'limit',\n",
              " 'misc',\n",
              " 'sin',\n",
              " 'link',\n",
              " 'finding',\n",
              " 'okay',\n",
              " 'clock',\n",
              " 'floppy',\n",
              " 'plus',\n",
              " 'played',\n",
              " 'electrical',\n",
              " 'clear',\n",
              " 'allowed',\n",
              " 'class',\n",
              " 'glad',\n",
              " 'illinois',\n",
              " 'electronics',\n",
              " 'older',\n",
              " 'alternative',\n",
              " 'cleveland',\n",
              " 'old',\n",
              " 'place',\n",
              " 'black',\n",
              " 'player',\n",
              " 'allen',\n",
              " 'close',\n",
              " 'claim',\n",
              " 'play',\n",
              " 'places',\n",
              " 'clearly',\n",
              " 'playing',\n",
              " 'claimed',\n",
              " 'alan',\n",
              " 'closed',\n",
              " 'club',\n",
              " 'clipper',\n",
              " 'clinton',\n",
              " 'allow',\n",
              " 'flight',\n",
              " 'florida',\n",
              " 'plain',\n",
              " 'alive',\n",
              " 'plan',\n",
              " 'blue',\n",
              " 'flames',\n",
              " 'block',\n",
              " 'blood',\n",
              " 'plug',\n",
              " 'algorithm',\n",
              " 'alt',\n",
              " 'placed',\n",
              " 'claims',\n",
              " 'players',\n",
              " 'blame',\n",
              " 'illegal',\n",
              " 'slightly',\n",
              " 'allows',\n",
              " 'flame',\n",
              " 'slow',\n",
              " 'closer',\n",
              " 'playoffs',\n",
              " 'electronic',\n",
              " 'umd',\n",
              " 'small',\n",
              " 'email',\n",
              " 'cmu',\n",
              " 'amendment',\n",
              " 'immediately',\n",
              " 'gmt',\n",
              " 'image',\n",
              " 'vms',\n",
              " 'images',\n",
              " 'important',\n",
              " 'imho',\n",
              " 'bmw',\n",
              " 'american',\n",
              " 'imagine',\n",
              " 'smith',\n",
              " 'smaller',\n",
              " 'americans',\n",
              " 'america',\n",
              " 'impossible',\n",
              " 'employer',\n",
              " 'nntp',\n",
              " 'university',\n",
              " 'know',\n",
              " 'engine',\n",
              " 'info',\n",
              " 'knowledge',\n",
              " 'engineering',\n",
              " 'intended',\n",
              " 'answer',\n",
              " 'anybody',\n",
              " 'anymore',\n",
              " 'ones',\n",
              " 'information',\n",
              " 'known',\n",
              " 'knew',\n",
              " 'understanding',\n",
              " 'individual',\n",
              " 'individuals',\n",
              " 'analysis',\n",
              " 'instead',\n",
              " 'increase',\n",
              " 'interface',\n",
              " 'understand',\n",
              " 'uni',\n",
              " 'unless',\n",
              " 'installed',\n",
              " 'england',\n",
              " 'end',\n",
              " 'interpretation',\n",
              " 'knows',\n",
              " 'vnews',\n",
              " 'environment',\n",
              " 'international',\n",
              " 'includes',\n",
              " 'included',\n",
              " 'insurance',\n",
              " 'ins',\n",
              " 'engineer',\n",
              " 'interesting',\n",
              " 'interested',\n",
              " 'institute',\n",
              " 'univ',\n",
              " 'unix',\n",
              " 'anti',\n",
              " 'andy',\n",
              " 'include',\n",
              " 'involved',\n",
              " 'input',\n",
              " 'andrew',\n",
              " 'industry',\n",
              " 'bnr',\n",
              " 'entirely',\n",
              " 'internet',\n",
              " 'encryption',\n",
              " 'enforcement',\n",
              " 'knowing',\n",
              " 'intelligence',\n",
              " 'entire',\n",
              " 'including',\n",
              " 'unit',\n",
              " 'universe',\n",
              " 'answers',\n",
              " 'internal',\n",
              " 'innocent',\n",
              " 'announcement',\n",
              " 'ontario',\n",
              " 'unfortunately',\n",
              " 'instance',\n",
              " 'independent',\n",
              " 'online',\n",
              " 'unable',\n",
              " 'inside',\n",
              " 'united',\n",
              " 'enjoy',\n",
              " 'eng',\n",
              " 'indiana',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "7OquNhh2CQfF",
        "colab_type": "code",
        "outputId": "f9efaee2-d03d-4198-ebb4-cecde5ed7410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "np.bincount(np.array([4,6,3,6,8,2,6,78,89,5]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 1, 1, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 341
        }
      ]
    },
    {
      "metadata": {
        "id": "RoSnOvi2C0tx",
        "colab_type": "code",
        "outputId": "51356002-fb5f-48e2-9813-37a339986475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\"?\".join(\"bsc\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'b?s?c'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 316
        }
      ]
    },
    {
      "metadata": {
        "id": "awLPiUrVC-jq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc_term_matrix."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}