{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kristynpantoja/math689project/blob/master/AVITMtoLDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GABqU8qMLuin",
    "outputId": "d897ed17-fb28-4d2c-e9c5-c166f478acdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwuFwsAW9IN_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_9vzhpQ9h_U"
   },
   "outputs": [],
   "source": [
    "class ProdLDA(nn.Module):\n",
    "\n",
    "    def __init__(self, net_arch):\n",
    "        super(ProdLDA, self).__init__()\n",
    "        ac = net_arch\n",
    "        self.net_arch = net_arch\n",
    "        # encoder\n",
    "        self.en1_fc     = nn.Linear(ac.num_input, ac.en1_units)             # 1995 -> 100\n",
    "        self.en2_fc     = nn.Linear(ac.en1_units, ac.en2_units)             # 100  -> 100\n",
    "        self.en2_drop   = nn.Dropout(0.2)\n",
    "        self.mean_fc    = nn.Linear(ac.en2_units, ac.num_topic)             # 100  -> 50\n",
    "        self.mean_bn    = nn.BatchNorm1d(ac.num_topic)                      # bn for mean\n",
    "        self.logvar_fc  = nn.Linear(ac.en2_units, ac.num_topic)             # 100  -> 50\n",
    "        self.logvar_bn  = nn.BatchNorm1d(ac.num_topic)                      # bn for logvar\n",
    "        # z\n",
    "        self.p_drop     = nn.Dropout(0.2)\n",
    "        # decoder\n",
    "        self.decoder = nn.Linear(ac.num_topic, ac.num_input)      # 50   -> 1995\n",
    "        self.decoder_bn = nn.BatchNorm1d(ac.num_topic)                      # bn for decoder\n",
    "        # prior mean and variance as constant buffers\n",
    "        prior_mean   = torch.Tensor(1, ac.num_topic).fill_(0)\n",
    "        prior_var    = torch.Tensor(1, ac.num_topic).fill_(ac.variance)\n",
    "        prior_logvar = prior_var.log()\n",
    "        self.register_buffer('prior_mean',    prior_mean)\n",
    "        self.register_buffer('prior_var',     prior_var)\n",
    "        self.register_buffer('prior_logvar',  prior_logvar)\n",
    "        # initialize decoder weight\n",
    "        if ac.init_mult != 0:\n",
    "            #std = 1. / math.sqrt( ac.init_mult * (ac.num_topic + ac.num_input))\n",
    "            self.decoder.weight.data.uniform_(0, ac.init_mult)\n",
    "        # remove BN's scale parameters\n",
    "#         self.logvar_bn .register_parameter('weight', None)\n",
    "#         self.mean_bn   .register_parameter('weight', None)\n",
    "#         self.decoder_bn.register_parameter('weight', None)\n",
    "#         self.decoder_bn.register_parameter('weight', None)\n",
    "        self.beta = nn.Parameter(torch.randn([self.net_arch.num_input, self.net_arch.num_topic]))\n",
    "\n",
    "    def forward(self, input, compute_loss=False, avg_loss=True):\n",
    "        # compute posterior\n",
    "        assert input.shape[1] == doc_term_matrix_tensor.shape[1], \"input isn't batch size x vocab size\"\n",
    "        en1 = F.softplus(self.en1_fc(input))                            # en1_fc   output\n",
    "        en2 = F.softplus(self.en2_fc(en1))                              # encoder2 output\n",
    "        en2 = self.en2_drop(en2)\n",
    "        posterior_mean   = self.mean_bn  (self.mean_fc  (en2))          # posterior mean\n",
    "        posterior_logvar = self.logvar_bn(self.logvar_fc(en2))          # posterior log variance\n",
    "        posterior_var    = posterior_logvar.exp()\n",
    "        # take sample\n",
    "        eps = Variable(input.data.new().resize_as_(posterior_mean.data).normal_()) # noise\n",
    "        z = posterior_mean + posterior_var.sqrt() * eps                 # reparameterization\n",
    "        assert z.shape[1] == self.net_arch.num_topic, \"hidden variable z (from TR) isn't batch size x num_topic\"\n",
    "        # get theta\n",
    "        p = F.softmax(z)                                                # mixture probability\n",
    "        p = self.p_drop(p)\n",
    "        assert p.shape[1] == self.net_arch.num_topic, \"p (theta) isn't same size as z\"\n",
    "        # return beta times theta, i.e. VxK times batch_size x K, need to transpose theta first\n",
    "        # do reconstruction\n",
    "        recon = F.softmax(self.decoder_bn(self.beta), dim=0).mm(p.t()).t()          # reconstructed distribution over vocabulary\n",
    "        assert input.shape[1] == doc_term_matrix_tensor.shape[1], \"output isn't batch size x vocab size\"\n",
    "        \n",
    "        if compute_loss:\n",
    "            return recon, self.loss(input, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
    "        else:\n",
    "            return recon\n",
    "\n",
    "    def loss(self, input, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
    "        # NL\n",
    "        NL  = -(input * (recon+1e-10).log()).sum(1) # vector with batch-size number of elements\n",
    "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
    "        # https://arxiv.org/pdf/1703.01488.pdf\n",
    "        prior_mean   = Variable(self.prior_mean).expand_as(posterior_mean) # batch-size x num_topics\n",
    "        prior_var    = Variable(self.prior_var).expand_as(posterior_mean)\n",
    "        prior_logvar = Variable(self.prior_logvar).expand_as(posterior_mean)\n",
    "        var_division    = posterior_var  / prior_var\n",
    "        diff            = posterior_mean - prior_mean\n",
    "        diff_term       = diff * diff / prior_var\n",
    "        logvar_division = prior_logvar - posterior_logvar\n",
    "        # put KLD together\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.net_arch.num_topic )\n",
    "#         print(KLD.mean())\n",
    "        # loss\n",
    "        loss = (NL + KLD)\n",
    "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
    "        if avg:\n",
    "            return loss.mean() # averaged over all the documents in the batch (1/batch_size)*sum\n",
    "        else:\n",
    "            return loss\n",
    "          \n",
    "def train():\n",
    "    for epoch in range(args.num_epoch):\n",
    "        all_indices = torch.randperm(doc_term_matrix_tensor.size(0)).split(args.batch_size)\n",
    "        loss_epoch = 0.0\n",
    "        model.train()                   # switch to training mode\n",
    "        for batch_indices in all_indices:\n",
    "            if not args.nogpu: batch_indices = batch_indices.cuda()\n",
    "            input = Variable(doc_term_matrix_tensor[batch_indices])\n",
    "#             print(batch_indices.shape)\n",
    "#             print(input.shape)\n",
    "            recon, loss = model(input, compute_loss=True)\n",
    "            # optimize\n",
    "            optimizer.zero_grad()       # clear previous gradients\n",
    "            loss.backward()             # backprop\n",
    "            optimizer.step()            # update parameters\n",
    "            # report\n",
    "            loss_epoch += loss.data[0]    # add loss to loss_epoch, then take the average in the print statement\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch {}, loss={}'.format(epoch, loss_epoch / len(all_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8VEfNT29xhT"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVr-x0Bk9o0E"
   },
   "outputs": [],
   "source": [
    "# categories = ['talk.politics.guns', 'sci.space', 'soc.religion.christian',\n",
    "#               'misc.forsale', 'rec.sport.baseball', 'comp.sys.mac.hardware']\n",
    "categories = ['talk.politics.guns', 'sci.space']\n",
    "# newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YzHUuVzk1x1_"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = 'english', min_df=.01, max_df=0.9, \n",
    "                             token_pattern = u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
    "count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
    "doc_term_matrix = count_vecs.toarray()\n",
    "doc_term_matrix.shape # number of documents, number of words (in vocab)\n",
    "tokenizer = vectorizer.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGLE6jkn2nrO"
   },
   "outputs": [],
   "source": [
    "doc_term_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "byzpYyzh2nuZ"
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aS33KVXN2nxn"
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avmyNj3B2n3K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bB1SYkcZBvLt"
   },
   "outputs": [],
   "source": [
    "doc_term_matrix_tensor = torch.from_numpy(doc_term_matrix).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T7-j2UwCB9yy"
   },
   "outputs": [],
   "source": [
    "doc_term_matrix_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TI6oVfEx91SQ"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mq3D13vc-GpU"
   },
   "outputs": [],
   "source": [
    "args_dict = {\"en1_units\" : 100, \"en2_units\" : 100, \"num_topic\" : 50, \n",
    "             \"batch_size\" : 200, \"optimizer\" : 80, \"learning_rate\" : 0.002, \n",
    "             \"momentum\" : 0.99, \"num_epoch\" : 80, \"init_mult\" : 1, \n",
    "             \"variance\" : 0.995, \"start\" : True, \"nogpu\" : True}\n",
    "args = SimpleNamespace(**args_dict)\n",
    "args.num_input = doc_term_matrix_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3LH7tOWIAGKd"
   },
   "outputs": [],
   "source": [
    "model = ProdLDA(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDVyc_fMSCDv"
   },
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyfxWF3nCW6s"
   },
   "outputs": [],
   "source": [
    "for i in model.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHEE0EtRAnq6"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), args.learning_rate, betas=(args.momentum, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "colab_type": "code",
    "id": "7rfgjUfmAnua",
    "outputId": "66a6d0a1-6464-42ac-f8dc-c5ac18ed4b5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:104: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=611.5573120117188\n",
      "Epoch 5, loss=611.6334228515625\n",
      "Epoch 10, loss=611.9019165039062\n",
      "Epoch 15, loss=610.7617797851562\n",
      "Epoch 20, loss=613.1493530273438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-eb4765c35b4f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m#             print(batch_indices.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m#             print(input.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;31m# optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# clear previous gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-eb4765c35b4f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, compute_loss, avg_loss)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_logvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-eb4765c35b4f>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, input, recon, posterior_mean, posterior_logvar, posterior_var, avg)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_logvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# NL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mNL\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# vector with batch-size number of elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;31m# KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# https://arxiv.org/pdf/1703.01488.pdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdp1baXqAnx3"
   },
   "outputs": [],
   "source": [
    "associations = {\n",
    "    'jesus': ['prophet', 'jesus', 'matthew', 'christ', 'worship', 'church'],\n",
    "    'comp ': ['floppy', 'windows', 'microsoft', 'monitor', 'workstation', 'macintosh', \n",
    "              'printer', 'programmer', 'colormap', 'scsi', 'jpeg', 'compression'],\n",
    "    'car  ': ['wheel', 'tire'],\n",
    "    'polit': ['amendment', 'libert', 'regulation', 'president'],\n",
    "    'crime': ['violent', 'homicide', 'rape'],\n",
    "    'midea': ['lebanese', 'israel', 'lebanon', 'palest'],\n",
    "    'sport': ['coach', 'hitter', 'pitch'],\n",
    "    'gears': ['helmet', 'bike'],\n",
    "    'nasa ': ['orbit', 'spacecraft'],\n",
    "}\n",
    "def identify_topic_in_line(line):\n",
    "    topics = []\n",
    "    for topic, keywords in associations.items():\n",
    "        for word in keywords:\n",
    "            if word in line:\n",
    "                topics.append(topic)\n",
    "                break\n",
    "    return topics\n",
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print('---------------Printing the Topics------------------')\n",
    "    for i in range(len(beta)):\n",
    "        line = \" \".join([feature_names[j] \n",
    "                            for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
    "        topics = identify_topic_in_line(line)\n",
    "        print('|'.join(topics))\n",
    "        print('     {}'.format(line))\n",
    "    print('---------------End of Topics------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0W3ZjwSL41KP"
   },
   "outputs": [],
   "source": [
    "sorted(vectorizer.vocabulary_, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyiShF-JIyss"
   },
   "outputs": [],
   "source": [
    "emb = model.decoder.weight.data.cpu().numpy().T\n",
    "print(\"shape of beta is \" + str(emb.shape))\n",
    "print_top_words(emb, vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTeSabr7K3qo"
   },
   "outputs": [],
   "source": [
    "beta = model.decoder.weight.detach()#.softmax(0)\n",
    "print(beta.sum(0))\n",
    "print(beta.shape)\n",
    "_, ind = torch.sort(beta, 0)\n",
    "print(ind.shape)\n",
    "# ind.numpy()[0:50, 0] - ind.numpy()[0:50, 1]\n",
    "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 0])\n",
    "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 1])\n",
    "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 2])\n",
    "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 3])\n",
    "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 4])\n",
    "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 5])\n",
    "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 6])\n",
    "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 7])\n",
    "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 8])\n",
    "print(np.array(sorted(vectorizer.get_feature_names(), key = lambda x: x[1]))[ind.numpy()][0:25, 9])\n",
    "\n",
    "# print(np.array(vectorizer.get_feature_names())[ind.numpy()][0:25, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gry54SsunU0B"
   },
   "outputs": [],
   "source": [
    "print(model.decoder.weight)\n",
    "model.decoder.weight.data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLdV3bB4N0eX"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def topic_coherence(beta, M, doc_term_matrix):\n",
    "  K = beta.shape[1] # beta has dim V x K\n",
    "  coherences = np.zeros(K)\n",
    "  for t in range(K):\n",
    "    index = np.argsort(-beta[:, t])[0:M]\n",
    "    cart_prod = product(list(index), list(index))\n",
    "    for ind1, ind2 in cart_prod:\n",
    "      if ind1 == ind2:\n",
    "        pass\n",
    "      else:\n",
    "        d_ind1 = (doc_term_matrix[:, ind1] > 0).sum()\n",
    "        d_ind12 = ((doc_term_matrix[:, ind1] > 0) & (doc_term_matrix[:, ind2] > 0)).sum()\n",
    "        coherences[t] += np.log1p(d_ind12) - np.log(d_ind1)\n",
    "\n",
    "  return coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kRl6b_pOH_0"
   },
   "outputs": [],
   "source": [
    "topic_coherence(beta, 20, doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "be2bpL_qOJWD"
   },
   "outputs": [],
   "source": [
    "sorted(vectorizer.vocabulary_, key = lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbCZ3Wz-ONfd"
   },
   "outputs": [],
   "source": [
    "vocab = {\"hi\": 13, \"bye\": 2, \"hello\": 3}\n",
    "foo = zip(*sorted(vocab.items(), key = lambda x: x[1]))\n",
    "list(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LhHX1qIm7WF_"
   },
   "outputs": [],
   "source": [
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print('---------------Printing the Topics------------------')\n",
    "    for i in range(len(beta)): # for all the rows (words in vocab) in beta,\n",
    "        line = \" \".join([feature_names[j] \n",
    "                            for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
    "#         topics = identify_topic_in_line(line)\n",
    "#         print('|'.join(topics))\n",
    "        print('     {}'.format(line))\n",
    "    print('---------------End of Topics------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-yqzxLIBvbF"
   },
   "outputs": [],
   "source": [
    "print_top_words(beta.numpy().T, sorted(vectorizer.vocabulary_, key = lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eevlzc-FCAAV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OquNhh2CQfF"
   },
   "outputs": [],
   "source": [
    "np.bincount(np.array([4,6,3,6,8,2,6,78,89,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RoSnOvi2C0tx"
   },
   "outputs": [],
   "source": [
    "\"?\".join(\"bsc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "awLPiUrVC-jq"
   },
   "outputs": [],
   "source": [
    "doc_term_matrix."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "AVITMtoLDA.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
