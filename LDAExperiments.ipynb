{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import TopicVAE\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = 'english', min_df=.01, max_df=0.9, \n",
    "                             token_pattern = u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
    "count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
    "doc_term_matrix = count_vecs.toarray()\n",
    "doc_term_matrix.shape # number of documents, number of words (in vocab)\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "# note: vectorizer.get_feature_names() != vectorizer.vocabulary_\n",
    "\n",
    "doc_term_matrix_tensor = torch.from_numpy(doc_term_matrix).float()\n",
    "\n",
    "args_dict = {\"en1_units\" : 100, \"en2_units\" : 100, \"num_topic\" : 50, \n",
    "             \"batch_size\" : 200, \"optimizer\" : 80, \"learning_rate\" : 0.002, \n",
    "             \"momentum\" : 0.99, \"num_epoch\" : 80, \"init_mult\" : 1, \n",
    "             \"variance\" : 0.995, \"start\" : True, \"nogpu\" : True}\n",
    "args = SimpleNamespace(**args_dict)\n",
    "args.num_input = doc_term_matrix_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
     ]
    }
   ],
   "source": [
    "pretrained_language_model = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_language_model.save(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = \"fasttext-wiki-news-subwords-300\"\n",
    "try_loading_pretrained_lm = KeyedVectors.load(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cats', 0.8368596434593201),\n",
       " ('housecat', 0.767471194267273),\n",
       " ('-cat', 0.7602992057800293),\n",
       " ('dog', 0.7502298355102539),\n",
       " ('kitten', 0.7480818033218384),\n",
       " ('feline', 0.7353992462158203),\n",
       " ('super-cat', 0.7305206060409546),\n",
       " ('supercat', 0.7163283824920654),\n",
       " ('pet', 0.709028422832489),\n",
       " ('moggy', 0.7057286500930786)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_language_model.most_similar(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/math689env/lib/python3.7/site-packages/ipykernel/__main__.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.random.randn([doc_term_matrix.shape[1], 300])\n",
    "\n",
    "iterator = 0\n",
    "for word in vectorizer.get_feature_names():\n",
    "    if word in pretrained_language_model.vocab:\n",
    "        embedding_matrix[iterator] = pretrained_language_model.wv.word_vec(word)\n",
    "    else:\n",
    "        continue\n",
    "        # embedding_matrix[iterator] = pretrained_language_model.wv.most_similar(word)\n",
    "        # or something like that\n",
    "    iterator += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(embedding_matrix.sum(1) == 0) # when it was np.zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:math689env]",
   "language": "python",
   "name": "conda-env-math689env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
