{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDAClasses.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristynpantoja/math689project/blob/master/LDAClasses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ckQV7uWixyFL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class TopicVAE(nn.Module):\n",
        "    def __init__(self, net_arch):\n",
        "        super().__init__()\n",
        "        ac = net_arch\n",
        "        self.net_arch = net_arch\n",
        "        # encoder\n",
        "        self.en1_fc     = nn.Linear(ac.num_input, ac.en1_units)             # 1995 -> 100\n",
        "        self.en2_fc     = nn.Linear(ac.en1_units, ac.en2_units)             # 100  -> 100\n",
        "        self.en2_drop   = nn.Dropout(0.2)\n",
        "        self.mean_fc    = nn.Linear(ac.en2_units, ac.num_topic)             # 100  -> 50\n",
        "        self.mean_bn    = nn.BatchNorm1d(ac.num_topic)                      # bn for mean\n",
        "        self.logvar_fc  = nn.Linear(ac.en2_units, ac.num_topic)             # 100  -> 50\n",
        "        self.logvar_bn  = nn.BatchNorm1d(ac.num_topic)                      # bn for logvar\n",
        "        # z\n",
        "        self.p_drop     = nn.Dropout(0.2)\n",
        "        # decoder\n",
        "        self.decoder    = nn.Linear(ac.num_topic, ac.num_input)             # 50   -> 1995\n",
        "        self.decoder_bn = nn.BatchNorm1d(ac.num_input)                      # bn for decoder\n",
        "        # prior mean and variance as constant buffers\n",
        "        prior_mean   = torch.Tensor(1, ac.num_topic).fill_(0)\n",
        "        prior_var    = torch.Tensor(1, ac.num_topic).fill_(ac.variance)\n",
        "        prior_logvar = prior_var.log()\n",
        "        self.register_buffer('prior_mean',    prior_mean)\n",
        "        self.register_buffer('prior_var',     prior_var)\n",
        "        self.register_buffer('prior_logvar',  prior_logvar)\n",
        "        # initialize decoder weight\n",
        "        if ac.init_mult != 0:\n",
        "            self.decoder.weight.data.uniform_(0, ac.init_mult)\n",
        "            \n",
        "    def encoder(self, input):\n",
        "        assert input.shape[1] == self.net_arch.num_input, \"input isn't batch size x vocab size\"\n",
        "        en1 = F.softplus(self.en1_fc(input))                            # en1_fc   output\n",
        "        en2 = F.softplus(self.en2_fc(en1))                              # encoder2 output\n",
        "        en2 = self.en2_drop(en2)\n",
        "        posterior_mean   = self.mean_bn  (self.mean_fc  (en2))          # posterior mean\n",
        "        posterior_logvar = self.logvar_bn(self.logvar_fc(en2))          # posterior log variance\n",
        "        posterior_var    = posterior_logvar.exp()\n",
        "        return posterior_mean, posterior_logvar, posterior_var\n",
        "    \n",
        "    def reparameterize(self, input, posterior_mean, posterior_var):\n",
        "        eps = Variable(input.data.new().resize_as_(posterior_mean.data).normal_()) # noise\n",
        "        z = posterior_mean + posterior_var.sqrt() * eps                 # reparameterization\n",
        "        return z\n",
        "      \n",
        "    def generative(self, z):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def forward(self, input, compute_loss=False, avg_loss=True):\n",
        "        # compute posterior\n",
        "        posterior_mean, posterior_logvar, posterior_var = self.encoder(input)\n",
        "        z = self.reparameterize(input, posterior_mean, posterior_var)\n",
        "        recon = self.generative(z)\n",
        "        assert recon.shape[1] == self.net_arch.num_input, \"output isn't batch size x vocab size\"\n",
        "        \n",
        "        if compute_loss:\n",
        "            return recon, self.loss(input, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
        "        else:\n",
        "            return recon\n",
        "\n",
        "    def loss(self, input, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
        "        # NL\n",
        "        NL  = -(input * (recon+1e-10).log()).sum(1) # vector with batch-size number of elements\n",
        "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
        "        # https://arxiv.org/pdf/1703.01488.pdf\n",
        "        prior_mean   = Variable(self.prior_mean).expand_as(posterior_mean) # batch-size x num_topics\n",
        "        prior_var    = Variable(self.prior_var).expand_as(posterior_mean)\n",
        "        prior_logvar = Variable(self.prior_logvar).expand_as(posterior_mean)\n",
        "        var_division    = posterior_var  / prior_var\n",
        "        diff            = posterior_mean - prior_mean\n",
        "        diff_term       = diff * diff / prior_var\n",
        "        logvar_division = prior_logvar - posterior_logvar\n",
        "        # put KLD together\n",
        "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.net_arch.num_topic )\n",
        "\n",
        "        loss = (NL + KLD)\n",
        "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
        "        if avg:\n",
        "            return loss.mean() # averaged over all the documents in the batch (1/batch_size)*sum\n",
        "        else:\n",
        "            return loss\n",
        "          \n",
        "    \n",
        "          \n",
        "def train(model, args, optimizer, dataset):\n",
        "    '''\n",
        "    model - object of class TopicVAE\n",
        "    args - dict of args\n",
        "    optimizer - nn.optim\n",
        "    dataset - docs x vocab tensor document term matrix\n",
        "    '''\n",
        "    for epoch in range(args.num_epoch):\n",
        "        all_indices = torch.randperm(dataset.size(0)).split(args.batch_size)\n",
        "        loss_epoch = 0.0\n",
        "        model.train()                   # switch to training mode\n",
        "        for batch_indices in all_indices:\n",
        "            if not args.nogpu: batch_indices = batch_indices.cuda()\n",
        "            input = Variable(dataset[batch_indices])\n",
        "            recon, loss = model(input, compute_loss=True)\n",
        "            # optimize\n",
        "            optimizer.zero_grad()       # clear previous gradients\n",
        "            loss.backward()             # backprop\n",
        "            optimizer.step()            # update parameters\n",
        "            # report\n",
        "            loss_epoch += loss.data[0]    # add loss to loss_epoch\n",
        "        if epoch % 5 == 0:\n",
        "            print('Epoch {}, loss={}'.format(epoch, loss_epoch / len(all_indices)))\n",
        "\n",
        "    return model\n",
        "            \n",
        "            \n",
        "class ProdLDA(TopicVAE):\n",
        "    def __init__(self, net_arch):\n",
        "        super().__init__(net_arch)\n",
        "        \n",
        "    def generative(self, z):\n",
        "        assert z.shape[1] == self.net_arch.num_topic, \"hidden variable z (from TR) isn't batch size x num_topic\"    \n",
        "        p = F.softmax(z)                                                # mixture probability\n",
        "        p = self.p_drop(p)\n",
        "        assert p.shape[1] == self.net_arch.num_topic, \"p (theta) isn't same size as z\"\n",
        "        recon = F.softmax(self.decoder_bn(self.decoder(p)))             # reconstructed distribution over vocabulary\n",
        "        return recon\n",
        "    \n",
        "\n",
        "class LDA(TopicVAE):\n",
        "    def __init__(self, net_arch):\n",
        "        super().__init__(net_arch)\n",
        "        self.beta = nn.Parameter(torch.randn([self.net_arch.num_input, self.net_arch.num_topic]))\n",
        "        self.beta_bn = nn.BatchNorm1d(self.net_arch.num_topic)\n",
        "        \n",
        "    def generative(self, z):\n",
        "        assert z.shape[1] == self.net_arch.num_topic, \"hidden variable z (from TR) isn't batch size x num_topic\"    \n",
        "        p = F.softmax(z)                                                # mixture probability\n",
        "        p = self.p_drop(p)\n",
        "        assert p.shape[1] == self.net_arch.num_topic, \"p (theta) isn't same size as z\"\n",
        "        recon = F.softmax(self.beta_bn(self.beta), dim=0).mm(p.t()).t()\n",
        "        return recon\n",
        "\n",
        "\n",
        "class GSMLDA(TopicVAE):\n",
        "    def __init__(self, net_arch):\n",
        "        super().__init__(net_arch)\n",
        "        self.word_embedding = nn.Embedding(self.net_arch.num_input, 50) # decoder\n",
        "        self.word_embedding_bn = nn.BatchNorm1d(50)\n",
        "        self.topic_embedding = nn.Embedding(self.net_arch.num_topic, 50) # decoder\n",
        "        self.topic_embedding_bn = nn.BatchNorm1d(50)\n",
        "        self.beta = torch.zeros([self.net_arch.num_topic, self.net_arch.num_input], dtype = torch.float32) # decoder\n",
        "\n",
        "    def generative(self, z):\n",
        "        assert z.shape[1] == self.net_arch.num_topic, \"hidden variable z (from TR) isn't batch size x num_topic\"    \n",
        "        p = F.softmax(z)                                                # mixture probability\n",
        "        p = self.p_drop(p)\n",
        "        assert p.shape[1] == self.net_arch.num_topic, \"p (theta) isn't same size as z\"\n",
        "        # do reconstruction\n",
        "        word_vec = self.word_embedding_bn(self.word_embedding.weight)\n",
        "        topic_vec = self.topic_embedding_bn(self.topic_embedding.weight)\n",
        "        self.beta = F.softmax(word_vec.mm(topic_vec.t()), dim = 0) # Vx100 times 100xK => beta is VxK\n",
        "        recon = p.mm(self.beta.t())         # reconstructed distribution over vocabulary\n",
        "        # p is batchxK so batchxK times KxV => batchxV\n",
        "        return recon\n",
        "      \n",
        "      \n",
        "class GSMProdLDA(TopicVAE):\n",
        "    def __init__(self, net_arch):\n",
        "        super().__init__(net_arch)\n",
        "        self.word_embedding = nn.Embedding(self.net_arch.num_input, 50) # decoder\n",
        "        self.word_embedding_bn = nn.BatchNorm1d(50)\n",
        "        self.topic_embedding = nn.Embedding(self.net_arch.num_topic, 50) # decoder\n",
        "        self.topic_embedding_bn = nn.BatchNorm1d(50)\n",
        "        self.beta = torch.zeros([self.net_arch.num_topic, self.net_arch.num_input], dtype = torch.float32) # decoder\n",
        "\n",
        "    def generative(self, z):\n",
        "        assert z.shape[1] == self.net_arch.num_topic, \"hidden variable z (from TR) isn't batch size x num_topic\"    \n",
        "        p = F.softmax(z)                                                # mixture probability\n",
        "        p = self.p_drop(p)\n",
        "        assert p.shape[1] == self.net_arch.num_topic, \"p (theta) isn't same size as z\"\n",
        "        # do reconstruction\n",
        "        word_vec = self.word_embedding_bn(self.word_embedding.weight)\n",
        "        topic_vec = self.topic_embedding_bn(self.topic_embedding.weight)\n",
        "        self.beta = word_vec.mm(topic_vec.t()) # Vx100 times 100xK => beta is VxK\n",
        "        recon = F.softmax(p.mm(self.beta.t()), dim = 0)         # reconstructed distribution over vocabulary\n",
        "        # p is batchxK so batchxK times KxV => batchxV\n",
        "        return recon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zx7EOFg64kFk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import argparse\n",
        "from types import SimpleNamespace\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words = 'english', min_df=.01, max_df=0.9, \n",
        "                             token_pattern = u'(?ui)\\\\b[a-z]{3,}\\\\b')\n",
        "count_vecs = vectorizer.fit_transform(newsgroups_train.data)\n",
        "doc_term_matrix = count_vecs.toarray()\n",
        "doc_term_matrix.shape # number of documents, number of words (in vocab)\n",
        "tokenizer = vectorizer.build_tokenizer()\n",
        "\n",
        "# note: vectorizer.get_feature_names() != vectorizer.vocabulary_\n",
        "\n",
        "doc_term_matrix_tensor = torch.from_numpy(doc_term_matrix).float()\n",
        "\n",
        "args_dict = {\"en1_units\" : 100, \"en2_units\" : 100, \"num_topic\" : 50, \n",
        "             \"batch_size\" : 200, \"optimizer\" : 80, \"learning_rate\" : 0.002, \n",
        "             \"momentum\" : 0.99, \"num_epoch\" : 80, \"init_mult\" : 1, \n",
        "             \"variance\" : 0.995, \"start\" : True, \"nogpu\" : True}\n",
        "args = SimpleNamespace(**args_dict)\n",
        "args.num_input = doc_term_matrix_tensor.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LpXQRNGh4sZl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_ProdLDA = ProdLDA(args)\n",
        "optimizer_ProdLDA = torch.optim.Adam(model_ProdLDA.parameters(), args.learning_rate, betas=(args.momentum, 0.999))\n",
        "model_ProdLDA = train(model_ProdLDA, args, optimizer_ProdLDA, doc_term_matrix_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xF92AeEw6WWN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_LDA = LDA(args)\n",
        "optimizer_LDA = torch.optim.Adam(model_LDA.parameters(), args.learning_rate, betas=(args.momentum, 0.999))\n",
        "model_LDA = train(model_LDA, args, optimizer_LDA, doc_term_matrix_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W-_oBTlHELiV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_GSMLDA = GSMLDA(args)\n",
        "optimizer_GSMLDA = torch.optim.Adam(model_GSMLDA.parameters(), args.learning_rate, betas=(args.momentum, 0.999))\n",
        "model_GSMLDA = train(model_GSMLDA, args, optimizer_GSMLDA, doc_term_matrix_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xt1Ul076ESIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1034
        },
        "outputId": "a89b33c8-8315-4e33-97c7-9676859e5539"
      },
      "cell_type": "code",
      "source": [
        "model_GSMProdLDA = GSMProdLDA(args)\n",
        "optimizer_GSMProdLDA = torch.optim.Adam(model_GSMProdLDA.parameters(), args.learning_rate, betas=(args.momentum, 0.999))\n",
        "model_GSMProdLDA = train(model_GSMProdLDA, args, optimizer_GSMProdLDA, doc_term_matrix_tensor)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:180: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:111: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss=460.3024597167969\n",
            "Epoch 5, loss=396.4909973144531\n",
            "Epoch 10, loss=389.53173828125\n",
            "Epoch 15, loss=387.416015625\n",
            "Epoch 20, loss=385.3667297363281\n",
            "Epoch 25, loss=385.24200439453125\n",
            "Epoch 30, loss=384.0899353027344\n",
            "Epoch 35, loss=382.36944580078125\n",
            "Epoch 40, loss=381.57354736328125\n",
            "Epoch 45, loss=379.7637023925781\n",
            "Epoch 50, loss=379.6176452636719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bc10af4a468b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_GSMProdLDA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGSMProdLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer_GSMProdLDA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_GSMProdLDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_GSMProdLDA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_GSMProdLDA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_GSMProdLDA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_term_matrix_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-59d436c4d0a1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, args, optimizer, dataset)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnogpu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0;31m# optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# clear previous gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-59d436c4d0a1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, compute_loss, avg_loss)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mposterior_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_logvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_arch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output isn't batch size x vocab size\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-59d436c4d0a1>\u001b[0m in \u001b[0;36mgenerative\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mtopic_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_embedding_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Vx100 times 100xK => beta is VxK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# reconstructed distribution over vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# p is batchxK so batchxK times KxV => batchxV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}